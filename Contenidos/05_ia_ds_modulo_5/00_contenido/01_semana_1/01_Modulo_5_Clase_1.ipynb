{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d825b043",
   "metadata": {},
   "source": [
    "# **Módulo 5 - Clase 1: Procesamiento Básico de Texto con Python**\n",
    "\n",
    "**Duración total:** 3 horas\n",
    "\n",
    "**Asignatura:** Ciencia de Datos e Inteligencia Artificial\n",
    "\n",
    "**Unidad:** Procesamiento Básico de Texto: Limpieza, Tokenización y Análisis Exploratorio\n",
    "\n",
    "**Herramientas:** Python, NLTK, pandas, matplotlib, WordCloud, TextBlob (para el ejercicio de análisis de sentimientos)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d524cf5",
   "metadata": {},
   "source": [
    "### 1. Introducción al Procesamiento de Lenguaje Natural (PLN)\n",
    "\n",
    "**Objetivo:** Brindar el contexto y la motivación del procesamiento de texto en proyectos de Ciencia de Datos e Inteligencia Artificial.\n",
    "\n",
    "### Teoría:\n",
    "\n",
    "El Procesamiento de Lenguaje Natural (PLN) es una rama de la IA que **se encarga de la interacción entre las computadoras y el lenguaje humano. Su objetivo es permitir que las máquinas entiendan, interpreten y generen lenguaje humano.**\n",
    "\n",
    "**Aplicaciones del PLN:**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Análisis de sentimientos**\n",
    "\n",
    "**Definición:**\n",
    "El análisis de sentimientos (también llamado minería de opiniones) es una técnica del PLN que consiste en identificar, extraer y clasificar automáticamente emociones expresadas en un texto. Su objetivo principal es determinar si el sentimiento detrás de un texto es **positivo**, **negativo** o **neutro**.\n",
    "\n",
    "**Aplicaciones comunes:**\n",
    "\n",
    "* Opiniones de clientes en productos y servicios (por ejemplo, reseñas de Amazon o TripAdvisor).\n",
    "* Análisis de reputación en redes sociales (tweets, comentarios).\n",
    "* Estudios de mercado y marketing.\n",
    "\n",
    "**En la práctica:**\n",
    "Se basa en técnicas de aprendizaje supervisado (entrenar modelos con textos etiquetados) o en enfoques léxicos (uso de diccionarios de palabras positivas/negativas). Herramientas comunes: `TextBlob`, `VADER`, modelos BERT.\n",
    "\n",
    "\n",
    "| Característica             | Descripción                                                                     |\n",
    "| -------------------------- | ------------------------------------------------------------------------------- |\n",
    "| **Objetivo**               | Determinar si un texto expresa un sentimiento positivo, negativo o neutro.      |\n",
    "| **Entrada**                | Texto libre (comentarios, reseñas, publicaciones en redes, etc.).               |\n",
    "| **Salida**                 | Etiqueta de sentimiento o puntuación de polaridad (por ejemplo, de -1 a 1).     |\n",
    "| **Técnicas comunes**       | Léxicas (diccionarios), aprendizaje automático supervisado, redes neuronales.   |\n",
    "| **Bibliotecas frecuentes** | `TextBlob`, `VADER`, `NLTK`, `transformers (BERT, RoBERTa)`                     |\n",
    "| **Aplicaciones prácticas** | Marketing, análisis reputacional, política, e-commerce, experiencia de usuario. |\n",
    "| **Ventaja principal**      | Permite obtener insights emocionales masivos sin análisis manual.               |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Clasificación de textos**\n",
    "\n",
    "**Definición:**\n",
    "La clasificación de textos es la tarea de asignar una o varias categorías predefinidas a un fragmento de texto. Es una de las tareas más fundamentales en PLN.\n",
    "\n",
    "**Tipos comunes:**\n",
    "\n",
    "* **Binaria**: texto spam o no spam.\n",
    "* **Multiclase**: clasificar noticias por secciones (deportes, política, salud).\n",
    "* **Multietiqueta**: un texto puede pertenecer a varias categorías.\n",
    "\n",
    "**Aplicaciones comunes:**\n",
    "\n",
    "* Filtros de spam en correo electrónico.\n",
    "* Moderación de contenido en redes sociales.\n",
    "* Clasificación automática de documentos legales o médicos.\n",
    "\n",
    "**En la práctica:**\n",
    "Se puede implementar con algoritmos como Naïve Bayes, SVM, redes neuronales o transformers, utilizando TF-IDF o representaciones vectoriales modernas como Word2Vec, BERT.\n",
    "\n",
    "\n",
    "| Característica             | Descripción                                                             |\n",
    "| -------------------------- | ----------------------------------------------------------------------- |\n",
    "| **Objetivo**               | Asignar categorías predefinidas a un texto.                             |\n",
    "| **Tipos de clasificación** | Binaria (spam/no spam), multiclase, multietiqueta.                      |\n",
    "| **Entrada**                | Texto sin estructura (mensajes, artículos, correos).                    |\n",
    "| **Salida**                 | Categoría o grupo al que pertenece el texto.                            |\n",
    "| **Técnicas comunes**       | Naïve Bayes, SVM, Random Forest, redes neuronales, transformers.        |\n",
    "| **Bibliotecas frecuentes** | `scikit-learn`, `spaCy`, `fastText`, `transformers`                     |\n",
    "| **Aplicaciones prácticas** | Moderación de contenido, filtros de spam, categorización de documentos. |\n",
    "| **Ventaja principal**      | Automatiza la organización de grandes volúmenes de texto.               |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Traducción automática**\n",
    "\n",
    "**Definición:**\n",
    "La traducción automática (Machine Translation, MT) es el proceso mediante el cual un sistema convierte texto de un idioma a otro sin intervención humana.\n",
    "\n",
    "**Aplicaciones comunes:**\n",
    "\n",
    "* Traductores en línea (Google Translate, DeepL).\n",
    "* Plataformas educativas multilingües.\n",
    "* Comunicación en aplicaciones y servicios internacionales.\n",
    "\n",
    "**Técnicas:**\n",
    "\n",
    "* Sistemas basados en reglas (en desuso).\n",
    "* Sistemas estadísticos (SMT, como el antiguo Google Translate).\n",
    "* **Traducción automática neuronal** (NMT): modelos de deep learning como seq2seq, transformers (Ej: Google Neural Machine Translation, OpenNMT).\n",
    "\n",
    "**En la práctica:**\n",
    "Se utilizan redes neuronales profundas entrenadas con grandes corpus paralelos (texto equivalente en dos idiomas). Requiere procesamiento y alineación gramatical compleja.\n",
    "\n",
    "\n",
    "| Característica             | Descripción                                                                  |\n",
    "| -------------------------- | ---------------------------------------------------------------------------- |\n",
    "| **Objetivo**               | Convertir texto de un idioma origen a un idioma destino de forma automática. |\n",
    "| **Entrada**                | Texto en idioma A.                                                           |\n",
    "| **Salida**                 | Traducción equivalente en idioma B.                                          |\n",
    "| **Técnicas principales**   | SMT (traducción estadística), NMT (traducción automática neuronal).          |\n",
    "| **Modelos populares**      | seq2seq con LSTM, Transformer, MarianMT, GPT, mBART.                         |\n",
    "| **Bibliotecas frecuentes** | `OpenNMT`, `Hugging Face Transformers`, `MarianNMT`, `fairseq`               |\n",
    "| **Aplicaciones prácticas** | Traducción web, contenido multilingüe, e-learning, comercio internacional.   |\n",
    "| **Ventaja principal**      | Ahorra tiempo y recursos en contextos multilingües.                          |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Sistemas de recomendación basados en texto**\n",
    "\n",
    "**Definición:**\n",
    "Son sistemas que sugieren contenido a los usuarios a partir del análisis semántico y contextual del texto (descripciones, títulos, reseñas, etc.).\n",
    "\n",
    "**Aplicaciones comunes:**\n",
    "\n",
    "* Recomendación de artículos o noticias según intereses.\n",
    "* Sugerencia de productos basada en opiniones escritas.\n",
    "* Plataformas educativas que recomiendan recursos personalizados.\n",
    "\n",
    "**En la práctica:**\n",
    "\n",
    "* **Sistemas content-based:** usan características textuales del ítem (título, descripción) y las comparan con los intereses del usuario.\n",
    "* **Técnicas:** TF-IDF, embeddings, análisis semántico latente (LSA), Word2Vec, BERT.\n",
    "\n",
    "**Ejemplo:** Un usuario que lee muchas reseñas de libros de ciencia ficción recibirá sugerencias de libros con textos similares.\n",
    "\n",
    "\n",
    "| Característica             | Descripción                                                                |\n",
    "| -------------------------- | -------------------------------------------------------------------------- |\n",
    "| **Objetivo**               | Sugerir contenidos, productos o servicios en función del análisis textual. |\n",
    "| **Entrada**                | Texto relacionado al producto/usuario (reseñas, descripciones, títulos).   |\n",
    "| **Salida**                 | Lista personalizada de recomendaciones.                                    |\n",
    "| **Técnicas comunes**       | TF-IDF, Word Embeddings (Word2Vec, GloVe), modelos semánticos (BERT).      |\n",
    "| **Modelos frecuentes**     | Sistemas content-based o híbridos (con filtrado colaborativo).             |\n",
    "| **Bibliotecas útiles**     | `scikit-learn`, `gensim`, `spaCy`, `transformers`, `LightFM`.              |\n",
    "| **Aplicaciones prácticas** | Recomendaciones de libros, películas, cursos, productos.                   |\n",
    "| **Ventaja principal**      | Mejora la experiencia del usuario con contenido relevante.                 |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Chatbots y asistentes virtuales**\n",
    "\n",
    "**Definición:**\n",
    "Son sistemas automáticos que interactúan con humanos en lenguaje natural, comprendiendo, procesando y generando respuestas adecuadas a entradas textuales o de voz.\n",
    "\n",
    "**Tipos:**\n",
    "\n",
    "* **Basados en reglas:** flujos predefinidos y scripts condicionales.\n",
    "* **Basados en IA:** utilizan PLN y modelos de aprendizaje profundo.\n",
    "\n",
    "**Aplicaciones comunes:**\n",
    "\n",
    "* Servicio al cliente automatizado (bancos, comercios).\n",
    "* Asistentes personales (Siri, Alexa, Google Assistant).\n",
    "* Plataformas de educación, salud y e-commerce.\n",
    "\n",
    "**En la práctica:**\n",
    "Involucran tareas complejas del PLN como:\n",
    "\n",
    "* Reconocimiento de intenciones (intent classification)\n",
    "* Extracción de entidades (NER)\n",
    "* Generación de lenguaje natural (NLG)\n",
    "\n",
    "**Frameworks populares:** Rasa, Dialogflow, Microsoft Bot Framework.\n",
    "\n",
    "\n",
    "| Característica             | Descripción                                                                   |\n",
    "| -------------------------- | ----------------------------------------------------------------------------- |\n",
    "| **Objetivo**               | Simular conversaciones humanas para brindar asistencia automatizada.          |\n",
    "| **Entrada**                | Pregunta o instrucción escrita o hablada.                                     |\n",
    "| **Salida**                 | Respuesta en lenguaje natural o ejecución de una acción.                      |\n",
    "| **Componentes clave**      | Detección de intención, extracción de entidades, generación de respuestas.    |\n",
    "| **Técnicas comunes**       | Reglas + PLN clásico o modelos de deep learning (BERT, GPT).                  |\n",
    "| **Frameworks populares**   | `Rasa`, `Dialogflow`, `Botpress`, `Microsoft Bot Framework`, `ChatGPT API`    |\n",
    "| **Aplicaciones prácticas** | Soporte al cliente, asistentes personales, automatización de tareas.          |\n",
    "| **Ventaja principal**      | Escalabilidad y disponibilidad continua de servicios sin intervención humana. |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e2483",
   "metadata": {},
   "source": [
    "### **Flujo General del Procesamiento Textual**\n",
    "\n",
    "El **procesamiento textual** es una secuencia estructurada de etapas que transforman texto sin procesar (ruidoso y complejo) en datos útiles para modelos de inteligencia artificial. Este flujo es fundamental en cualquier aplicación de PLN (Procesamiento de Lenguaje Natural) y permite extraer valor semántico, sintáctico y estadístico del lenguaje humano.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Recolección de datos**\n",
    "\n",
    "**¿Qué es?**\n",
    "Es la fase en la que se obtienen los datos textuales que se van a procesar y analizar.\n",
    "\n",
    "**Fuentes comunes:**\n",
    "\n",
    "* **Scraping web:** automatización para extraer texto de páginas web (por ejemplo, reseñas de productos, noticias, redes sociales).\n",
    "* **APIs:** como las de Twitter, Reddit o IMDb, que permiten consultar textos en tiempo real de forma estructurada.\n",
    "* **Archivos:** documentos planos como `.txt`, `.csv`, `.json`, PDFs, entre otros.\n",
    "\n",
    "**Consideraciones:**\n",
    "\n",
    "* Ética y legalidad del acceso a datos (respetar políticas de privacidad).\n",
    "* Formato y codificación (UTF-8 es el más común).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478810f4",
   "metadata": {},
   "source": [
    "#### **Ejercicio**\n",
    "\n",
    "Construir una **capa de extracción de texto** desde:\n",
    "\n",
    "1. Sitios web mediante scraping (`requests + BeautifulSoup`)\n",
    "2. APIs públicas (ejemplo: Reddit con `praw`)\n",
    "3. Archivos locales (`.txt`, `.csv`, `.json`)\n",
    "\n",
    "Todo dentro de una arquitectura extensible como una capa que puedes integrar en una aplicación mayor (por ejemplo, usando FastAPI o Streamlit).\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejercicio**\n",
    "\n",
    "#### Requisitos\n",
    "\n",
    "Instala los paquetes necesarios:\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4 praw pandas praw\n",
    "\n",
    "```\n",
    "\n",
    "1. `requests`\n",
    "2. `beautifulsoup4`\n",
    "3. Crear una cuenta gratuita en [https://newsdata.io/](https://newsdata.io/) y obtener tu `API_KEY`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **Script Python - `data_collector.py`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collector.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Scraping Web\n",
    "# -------------------------------\n",
    "\n",
    "def extraer_texto_web(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el texto visible de los <p> de una página web.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        parrafos = soup.find_all('p')\n",
    "        texto = ' '.join([p.get_text(strip=True) for p in parrafos if p.get_text(strip=True)])\n",
    "        return texto if texto else \"No se encontró texto en la página.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error en scraping: {e}\"\n",
    "\n",
    "# -------------------------------\n",
    "# API de Noticias (NewsData.io)\n",
    "# -------------------------------\n",
    "\n",
    "def extraer_noticias_newsdata(api_key: str, categoria: str = \"technology\", idioma: str = \"es\", limite=5) -> list:\n",
    "    \"\"\"\n",
    "    Extrae titulares recientes de noticias utilizando NewsData.io API gratuita.\n",
    "    \"\"\"\n",
    "    url = f\"https://newsdata.io/api/1/news?apikey={api_key}&language={idioma}&category={categoria}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        articulos = data.get(\"results\", [])[:limite]\n",
    "        return [art[\"title\"] for art in articulos if \"title\" in art]\n",
    "    except Exception as e:\n",
    "        return [f\"Error en API de noticias: {e}\"]\n",
    "\n",
    "# -------------------------------\n",
    "# Lectura de archivos locales\n",
    "# -------------------------------\n",
    "\n",
    "def leer_archivo(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee archivos .txt, .csv o .json y devuelve su contenido como string.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return f\"Archivo no encontrado: {path}\"\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    try:\n",
    "        if ext == '.txt':\n",
    "            with open(path, encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        elif ext == '.csv':\n",
    "            df = pd.read_csv(path)\n",
    "            return df.to_string(index=False)\n",
    "        elif ext == '.json':\n",
    "            with open(path, encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return json.dumps(data, indent=2, ensure_ascii=False)\n",
    "        else:\n",
    "            return f\"Formato no soportado: {ext}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error leyendo archivo: {e}\"\n",
    "\n",
    "# -------------------------------\n",
    "# Ejemplo de uso local\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- SCRAPING WEB ---\")\n",
    "    url = \"https://www.bbc.com/mundo\"\n",
    "    texto_web = extraer_texto_web(url)\n",
    "    print(texto_web[:500], \"...\\n\")\n",
    "\n",
    "    print(\"--- API DE NOTICIAS ---\")\n",
    "    api_key = os.getenv(\"NEWSDATA_API_KEY\", \"pub_2ac1dbe13deb4121913ac355f9da375d\")\n",
    "    noticias = extraer_noticias_newsdata(api_key)\n",
    "    for title in noticias:\n",
    "        print(\"-\", title)\n",
    "\n",
    "    print(\"\\n--- ARCHIVOS LOCALES ---\")\n",
    "    ruta_archivo = \"ejemplo.txt\"\n",
    "\n",
    "    # Crear archivo si no existe (para prueba)\n",
    "    if not os.path.exists(ruta_archivo):\n",
    "        with open(ruta_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"Este es un archivo de prueba para análisis de texto. Contiene varias frases útiles.\")\n",
    "\n",
    "    print(leer_archivo(ruta_archivo)[:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e36b3",
   "metadata": {},
   "source": [
    "### 2. **Limpieza de texto**\n",
    "\n",
    "**¿Qué es?**\n",
    "Consiste en eliminar elementos innecesarios o ruidosos del texto para facilitar su análisis y representación.\n",
    "\n",
    "**Operaciones comunes:**\n",
    "\n",
    "* Pasar todo a minúsculas\n",
    "* Eliminar signos de puntuación y caracteres especiales\n",
    "* Eliminar números irrelevantes\n",
    "* Quitar palabras vacías (stopwords) como “el”, “la”, “de”\n",
    "* Corregir errores ortográficos (opcional)\n",
    "* Eliminar URLs, etiquetas HTML, menciones o hashtags\n",
    "\n",
    "**Objetivo:**\n",
    "Obtener un texto limpio y homogéneo que facilite el procesamiento posterior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Script Python - Limpieza de texto básica**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fed895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpieza_texto.py\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar stopwords si aún no están\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def limpiar_texto(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia texto eliminando HTML, menciones, hashtags, emojis,\n",
    "    URLs, correos electrónicos, puntuación, números y stopwords.\n",
    "    \"\"\"\n",
    "    if not isinstance(texto, str):\n",
    "        raise ValueError(\"La entrada debe ser una cadena de texto.\")\n",
    "\n",
    "    # Si contiene tags HTML, procesamos con BeautifulSoup\n",
    "    if '<' in texto and '>' in texto:\n",
    "        texto = BeautifulSoup(texto, features=\"html.parser\").get_text()\n",
    "\n",
    "    # Eliminar correos electrónicos\n",
    "    texto = re.sub(r'\\S+@\\S+', '', texto)\n",
    "\n",
    "    # Eliminar URLs\n",
    "    texto = re.sub(r'http\\S+|www\\.\\S+', '', texto)\n",
    "\n",
    "    # Eliminar menciones y hashtags\n",
    "    texto = re.sub(r'[@#]\\w+', '', texto)\n",
    "\n",
    "    # Eliminar emojis y caracteres no alfanuméricos (excepto espacios)\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto, flags=re.UNICODE)\n",
    "\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Eliminar números\n",
    "    texto = re.sub(r'\\d+', '', texto)\n",
    "\n",
    "    # Eliminar signos de puntuación restantes\n",
    "    texto = texto.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Eliminar espacios múltiples\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    palabras = [p for p in texto.split() if p not in stop_words]\n",
    "\n",
    "    return ' '.join(palabras)\n",
    "\n",
    "# -------------------------------\n",
    "# Ejemplo de uso\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    texto_original = \"\"\"\n",
    "    ¡Hola! Me llamo Luis y este es mi correo: luis@email.com.\n",
    "    #NLP es increíble. Visita https://example.com para más información.\n",
    "    \"\"\"\n",
    "    texto_limpio = limpiar_texto(texto_original)\n",
    "    print(\"Texto limpio:\\n\", texto_limpio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d1c4e",
   "metadata": {},
   "source": [
    "### 3. **Tokenización y normalización**\n",
    "\n",
    "**¿Qué es?**\n",
    "Es la fragmentación del texto y la reducción de sus palabras a formas base para unificar variaciones gramaticales.\n",
    "\n",
    "**Tokenización:**\n",
    "\n",
    "* Divide el texto en unidades mínimas como palabras o frases (tokens).\n",
    "  Ejemplo: “Estoy feliz” → \\[“Estoy”, “feliz”]\n",
    "\n",
    "**Normalización:**\n",
    "\n",
    "* **Stemming:** Reduce palabras a su raíz (ej: “jugando” → “jug”)\n",
    "* **Lematización:** Reduce palabras a su forma base (ej: “jugando” → “jugar”)\n",
    "\n",
    "**Objetivo:**\n",
    "Reducir la complejidad del lenguaje natural sin perder significado.\n",
    "\n",
    "---\n",
    "\n",
    "### Instalar `spaCy`\n",
    "\n",
    "Ejecuta esto en tu terminal o consola (no en Jupyter):\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download es_core_news_sm\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Script Python – Tokenización y normalización**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizacion_spacy.py\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Cargar modelo en español\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"El modelo 'es_core_news_sm' no está instalado. Ejecuta esto en consola:\")\n",
    "    print(\"    python -m spacy download es_core_news_sm\")\n",
    "    exit()\n",
    "\n",
    "def tokenizar_y_lematizar_spacy(texto: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokeniza y lematiza un texto en español usando spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "    return [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "\n",
    "# -------------------------------\n",
    "# Ejemplo de uso\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    texto = \"Los estudiantes estaban estudiando intensamente y luego descansaron un poco.\"\n",
    "\n",
    "    print(\"Texto original:\")\n",
    "    print(texto)\n",
    "\n",
    "    print(\"\\nTokenización + Lematización (con spaCy):\")\n",
    "    print(tokenizar_y_lematizar_spacy(texto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c57eb",
   "metadata": {},
   "source": [
    "### **Nota técnica importante:**\n",
    "\n",
    "Para **lematización real en español**, \n",
    "[`spaCy`](https://spacy.io/) con el modelo `es_core_news_sm`, ya que `nltk` no ofrece lematización precisa en este idioma.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6af67",
   "metadata": {},
   "source": [
    "### 4. **Análisis exploratorio de texto (EDA textual)**\n",
    "\n",
    "**¿Qué es?**\n",
    "Es una revisión visual y estadística del contenido textual para entender su estructura, frecuencia y distribución.\n",
    "\n",
    "**Técnicas comunes:**\n",
    "\n",
    "* Conteo de palabras y n-gramas frecuentes\n",
    "* Nube de palabras (WordCloud)\n",
    "* Distribución de longitud de textos\n",
    "* Frecuencia de aparición de ciertos términos\n",
    "* Análisis de coocurrencia\n",
    "\n",
    "**Objetivo:**\n",
    "Descubrir patrones, outliers y relaciones antes de aplicar modelos de aprendizaje automático.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Ejercicio**\n",
    "\n",
    "Realizar un análisis exploratorio básico de un corpus textual usando:\n",
    "\n",
    "* Conteo de palabras\n",
    "* Frecuencia de términos\n",
    "* Longitud de textos\n",
    "* Nube de palabras\n",
    "* (Opcional) Coocurrencia simple\n",
    "\n",
    "---\n",
    "\n",
    "### **Instala primero estas bibliotecas si aún no las tienes:**\n",
    "\n",
    "```bash\n",
    "pip install pandas matplotlib wordcloud nltk seaborn spacy\n",
    "python -m spacy download es_core_news_sm\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Script Python – `eda_textual.py`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_textual.py\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# -------------------------------\n",
    "# Cargar modelo de spaCy en español\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocesamiento y tokenización\n",
    "# -------------------------------\n",
    "\n",
    "def limpiar_y_tokenizar(texto: str) -> list:\n",
    "    \"\"\"\n",
    "    Limpia, lematiza y tokeniza un texto en español usando spaCy.\n",
    "    Filtra stopwords, puntuación y símbolos.\n",
    "    \"\"\"\n",
    "    doc = nlp(texto.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# -------------------------------\n",
    "# Conteo de palabras más frecuentes\n",
    "# -------------------------------\n",
    "\n",
    "def contar_palabras(lista_textos: list) -> Counter:\n",
    "    \"\"\"\n",
    "    Cuenta palabras más frecuentes en una lista de textos.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for texto in lista_textos:\n",
    "        tokens.extend(limpiar_y_tokenizar(texto))\n",
    "    return Counter(tokens)\n",
    "\n",
    "# -------------------------------\n",
    "# Nube de palabras\n",
    "# -------------------------------\n",
    "\n",
    "def generar_nube_palabras(textos: list):\n",
    "    \"\"\"\n",
    "    Genera una nube de palabras a partir de una lista de textos.\n",
    "    \"\"\"\n",
    "    texto_unido = \" \".join(textos)\n",
    "    tokens_limpios = limpiar_y_tokenizar(texto_unido)\n",
    "    texto_limpio = \" \".join(tokens_limpios)\n",
    "\n",
    "    nube = WordCloud(width=800, height=400, background_color='white').generate(texto_limpio)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(nube, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Nube de palabras\")\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Distribución de longitud de textos\n",
    "# -------------------------------\n",
    "\n",
    "def graficar_longitud_textos(textos: list):\n",
    "    \"\"\"\n",
    "    Grafica la distribución de longitud de textos en número de palabras.\n",
    "    \"\"\"\n",
    "    longitudes = [len(limpiar_y_tokenizar(t)) for t in textos]\n",
    "    sns.histplot(longitudes, bins=10, kde=True)\n",
    "    plt.title(\"Distribución de longitud de textos\")\n",
    "    plt.xlabel(\"Cantidad de palabras\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Ejecución principal\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulación de dataset\n",
    "    datos = {\n",
    "        'id': [1, 2, 3],\n",
    "        'texto': [\n",
    "            \"¡Me encantó este producto! Lo recomiendo totalmente.\",\n",
    "            \"La calidad no era la esperada. Muy decepcionado.\",\n",
    "            \"Buen precio, entrega rápida, volveré a comprar.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(datos)\n",
    "\n",
    "    print(\"Análisis exploratorio básico del texto:\\n\")\n",
    "\n",
    "    # Conteo de palabras\n",
    "    conteo = contar_palabras(df['texto'].tolist())\n",
    "    print(\"Top 10 palabras más comunes:\")\n",
    "    for palabra, frecuencia in conteo.most_common(10):\n",
    "        print(f\"  {palabra}: {frecuencia}\")\n",
    "\n",
    "    # Nube de palabras\n",
    "    generar_nube_palabras(df['texto'].tolist())\n",
    "\n",
    "    # Distribución de longitud\n",
    "    graficar_longitud_textos(df['texto'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12955bb7",
   "metadata": {},
   "source": [
    "### 5. **Representación vectorial del texto**\n",
    "\n",
    "**¿Qué es?**\n",
    "Es la conversión de texto a una estructura numérica (vectores) que los algoritmos de IA pueden procesar.\n",
    "\n",
    "**Métodos principales:**\n",
    "\n",
    "* **Bag of Words (BoW):** Representa la frecuencia de palabras.\n",
    "* **TF-IDF:** Ajusta la importancia de palabras según su frecuencia en documentos.\n",
    "* **Embeddings:** Capturan significado semántico y contexto (Word2Vec, GloVe, FastText, BERT).\n",
    "\n",
    "**Objetivo:**\n",
    "Transformar lenguaje natural en datos numéricos útiles para el modelado.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejercicio con los tres enfoques de **representación vectorial de texto**:\n",
    "\n",
    "* **Bag of Words (BoW)**\n",
    "* **TF-IDF**\n",
    "* **Word Embeddings** (con `spaCy` para simplificar).\n",
    "\n",
    "### `representacion_vectorial.py`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6351c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# representacion_vectorial.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# ---------------------------------------\n",
    "# Cargar modelo spaCy si está disponible\n",
    "# ---------------------------------------\n",
    "def cargar_spacy():\n",
    "    try:\n",
    "        return spacy.load(\"es_core_news_md\")  # Modelo con vectores preentrenados\n",
    "    except OSError:\n",
    "        print(\"Modelo 'es_core_news_md' no encontrado.\")\n",
    "        print(\"Para usar Word Embeddings, instala con:\")\n",
    "        print(\"   python -m spacy download es_core_news_md\")\n",
    "        return None\n",
    "\n",
    "nlp = cargar_spacy()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Corpus de ejemplo\n",
    "# ---------------------------------------\n",
    "corpus = [\n",
    "    \"Me encanta el aprendizaje automático\",\n",
    "    \"El aprendizaje profundo es una subárea del aprendizaje automático\",\n",
    "    \"Las redes neuronales son fundamentales en el aprendizaje profundo\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Bag of Words (BoW)\n",
    "# ---------------------------------------\n",
    "print(\"Representación: Bag of Words (BoW)\\n\")\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(corpus)\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "print(df_bow)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. TF-IDF\n",
    "# ---------------------------------------\n",
    "print(\"\\nRepresentación: TF-IDF\\n\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(df_tfidf.round(2))\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Word Embeddings con spaCy (si está disponible)\n",
    "# ---------------------------------------\n",
    "if nlp:\n",
    "    print(\"\\nRepresentación: Word Embeddings (spaCy)\\n\")\n",
    "\n",
    "    def vector_promedio(texto):\n",
    "        doc = nlp(texto)\n",
    "        return doc.vector\n",
    "\n",
    "    embedding_vectors = [vector_promedio(texto) for texto in corpus]\n",
    "\n",
    "    print(f\"Vector de dimensión: {embedding_vectors[0].shape}\")\n",
    "    print(\"Primeros 10 valores del vector 1:\", embedding_vectors[0][:10])\n",
    "else:\n",
    "    print(\"\\nWord Embeddings desactivado. spaCy 'es_core_news_md' no está disponible.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c8707",
   "metadata": {},
   "source": [
    "### 6. **Modelado (aprendizaje supervisado o no supervisado)**\n",
    "\n",
    "**¿Qué es?**\n",
    "Aplicación de algoritmos de aprendizaje automático para resolver tareas específicas usando la representación vectorial del texto.\n",
    "\n",
    "**Tipos de aprendizaje:**\n",
    "\n",
    "| Tipo               | Descripción                                                                      |\n",
    "| ------------------ | -------------------------------------------------------------------------------- |\n",
    "| **Supervisado**    | Requiere datos etiquetados. Ej: clasificación de sentimientos, spam/no spam      |\n",
    "| **No supervisado** | No requiere etiquetas. Ej: agrupación de documentos (clustering), topic modeling |\n",
    "\n",
    "**Modelos comunes:**\n",
    "\n",
    "* Naïve Bayes, SVM, Logistic Regression\n",
    "* Random Forest, redes neuronales, transformers\n",
    "\n",
    "**Objetivo:**\n",
    "Extraer conocimiento útil del texto procesado, como predicciones, clasificaciones o agrupaciones.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "\n",
    "# ---------------------------------------\n",
    "# Configuraciones iniciales\n",
    "# ---------------------------------------\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Datos simulados\n",
    "# ---------------------------------------\n",
    "corpus = [\n",
    "    \"Me encanta este producto, es excelente\",\n",
    "    \"Es una porquería, lo odio totalmente\",\n",
    "    \"Muy satisfecho con la compra, lo recomiendo\",\n",
    "    \"No me gustó, es una pérdida de dinero\",\n",
    "    \"Funciona bien, pero esperaba más\",\n",
    "    \"Terrible calidad, no lo recomiendo\",\n",
    "    \"Es justo lo que necesitaba, me encantó\",\n",
    "    \"Mala experiencia, no volveré a comprar\"\n",
    "]\n",
    "etiquetas = [1, 0, 1, 0, 1, 0, 1, 0]  # 1: Positivo, 0: Negativo\n",
    "\n",
    "# ---------------------------------------\n",
    "# Vectorización\n",
    "# ---------------------------------------\n",
    "vectorizador = TfidfVectorizer()\n",
    "X = vectorizador.fit_transform(corpus)\n",
    "y = etiquetas\n",
    "\n",
    "# ---------------------------------------\n",
    "# División de datos\n",
    "# ---------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Modelado Supervisado\n",
    "# ---------------------------------------\n",
    "def evaluar_modelo(nombre, modelo):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    pred = modelo.predict(X_test)\n",
    "\n",
    "    print(f\"\\nModelo: {nombre}\")\n",
    "    print(classification_report(y_test, pred, target_names=[\"Negativo\", \"Positivo\"], zero_division=0))\n",
    "\n",
    "    matriz = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(matriz, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Neg\", \"Pos\"], yticklabels=[\"Neg\", \"Pos\"])\n",
    "    plt.title(f\"Matriz de confusión - {nombre}\")\n",
    "    plt.xlabel(\"Predicción\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n",
    "\n",
    "modelos = {\n",
    "    \"Naïve Bayes\": MultinomialNB(),\n",
    "    \"SVM\": LinearSVC(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "print(\"Clasificación Supervisada\\n\")\n",
    "for nombre, modelo in modelos.items():\n",
    "    evaluar_modelo(nombre, modelo)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Clustering No Supervisado (KMeans)\n",
    "# ---------------------------------------\n",
    "print(\"\\nClustering (No Supervisado): KMeans\\n\")\n",
    "\n",
    "num_clusters = 2\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "etiquetas_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "df_resultado = pd.DataFrame({\n",
    "    \"Texto\": corpus,\n",
    "    \"Cluster\": etiquetas_clusters\n",
    "})\n",
    "print(df_resultado)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Visualización 2D de Clusters con PCA\n",
    "# ---------------------------------------\n",
    "try:\n",
    "    pca = PCA(n_components=2)\n",
    "    X_2D = pca.fit_transform(X.toarray())\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    palette = sns.color_palette(\"Set2\", num_clusters)\n",
    "    for i in range(num_clusters):\n",
    "        puntos = X_2D[df_resultado[\"Cluster\"] == i]\n",
    "        plt.scatter(puntos[:, 0], puntos[:, 1], s=100, label=f\"Cluster {i}\", color=palette[i])\n",
    "\n",
    "    for i, txt in enumerate(df_resultado[\"Texto\"]):\n",
    "        plt.annotate(i, (X_2D[i, 0] + 0.02, X_2D[i, 1] + 0.02), fontsize=8)\n",
    "\n",
    "    plt.title(\"Agrupación de textos con KMeans + PCA\")\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error en visualización con PCA: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7044d",
   "metadata": {},
   "source": [
    "## 5. Ejercicio Práctico: Análisis de Sentimientos \n",
    "**Objetivo:** Integrar las técnicas aprendidas para analizar sentimientos en un conjunto de reseñas reales. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d8635",
   "metadata": {},
   "source": [
    "## Estructura del proyecto\n",
    "\n",
    "```\n",
    "analisis_sentimientos/\n",
    "│\n",
    "├── main.py                        # Punto de entrada principal\n",
    "│\n",
    "├── datos/\n",
    "│   └── recoleccion.py            # Capa 1: Recolección de datos\n",
    "│\n",
    "├── procesamiento/\n",
    "│   ├── limpieza.py               # Capa 2: Limpieza de texto\n",
    "│   ├── exploracion.py            # Capa 3: Análisis exploratorio\n",
    "│   └── vectorizacion.py          # Capa 4: Representación vectorial\n",
    "│\n",
    "├── modelo/\n",
    "│   └── entrenamiento.py          # Capa 5: Entrenamiento y evaluación\n",
    "│\n",
    "└── utils/\n",
    "    └── __init__.py               # (opcional) Utilidades comunes o configuraciones\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e64a1a",
   "metadata": {},
   "source": [
    "### 1. `datos/recoleccion.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 1: Recolección de datos\n",
    "\n",
    "# Importamos la biblioteca pandas, utilizada para trabajar con estructuras de datos como DataFrames.\n",
    "import pandas as pd\n",
    "\n",
    "# Definimos una función llamada 'cargar_datos' que simula la recolección de reseñas.\n",
    "def cargar_datos():\n",
    "    \"\"\"\n",
    "    Simula la carga de reseñas de productos.\n",
    "    Retorna un DataFrame con texto y su respectiva etiqueta de sentimiento.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista de cadenas de texto que representan reseñas de usuarios sobre productos.\n",
    "    reseñas = [\n",
    "        \"Me encantó el producto, lo recomiendo totalmente\",            # Positiva\n",
    "        \"Es una pérdida de dinero, muy malo\",                          # Negativa\n",
    "        \"Excelente calidad y entrega rápida\",                          # Positiva\n",
    "        \"Horrible, llegó dañado y sin caja\",                           # Negativa\n",
    "        \"Producto decente, aunque esperaba más\",                       # Positiva\n",
    "        \"Muy contento con la compra, funciona perfecto\",               # Positiva\n",
    "        \"No sirve, se rompió el primer día\",                           # Negativa\n",
    "        \"Satisfecho con la calidad, lo usaré nuevamente\"              # Positiva\n",
    "    ]\n",
    "\n",
    "    # Lista de etiquetas correspondientes a cada reseña.\n",
    "    # 1 representa sentimiento positivo y 0 representa sentimiento negativo.\n",
    "    etiquetas = [1, 0, 1, 0, 1, 1, 0, 1]\n",
    "\n",
    "    # Creamos un DataFrame de pandas a partir de las reseñas y etiquetas.\n",
    "    # El DataFrame tendrá dos columnas: 'texto' y 'sentimiento'.\n",
    "    return pd.DataFrame({'texto': reseñas, 'sentimiento': etiquetas})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bdc91",
   "metadata": {},
   "source": [
    "### 2. `procesamiento/limpieza.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 2: Limpieza de texto\n",
    "\n",
    "# Importamos el módulo 'string' que nos da acceso a caracteres especiales como puntuación.\n",
    "import string\n",
    "\n",
    "# Importamos el módulo 're' para expresiones regulares (no usado en esta versión, pero útil si se amplía).\n",
    "import re\n",
    "\n",
    "# Definimos una función que recibe como entrada una cadena de texto.\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    Convierte texto a minúsculas, elimina puntuación y palabras cortas.\n",
    "    Retorna el texto limpio como una sola cadena.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validación: si la entrada no es una cadena de texto, devolvemos una cadena vacía.\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Convertimos todo el texto a minúsculas para unificar comparaciones.\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # 2. Eliminamos los signos de puntuación utilizando 'translate' y 'string.punctuation'.\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Separamos el texto en palabras usando espacios como separador (tokenización básica).\n",
    "    tokens = texto.split()\n",
    "    \n",
    "    # 4. Eliminamos palabras muy cortas (menores o iguales a un carácter, como 'a', 'y', etc.).\n",
    "    tokens_limpios = [t for t in tokens if len(t) > 1]\n",
    "    \n",
    "    # 5. Unimos los tokens limpios de nuevo en una sola cadena separada por espacios.\n",
    "    return ' '.join(tokens_limpios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95905c",
   "metadata": {},
   "source": [
    "### 3. `procesamiento/exploracion.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29654229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 3: Análisis exploratorio\n",
    "\n",
    "# Importamos matplotlib para graficar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importamos seaborn, una librería que facilita gráficos estadísticos con estilo\n",
    "import seaborn as sns\n",
    "\n",
    "# Función que recibe un DataFrame con una columna llamada 'sentimiento'\n",
    "def graficar_distribucion(df):\n",
    "    \"\"\"\n",
    "    Muestra la distribución de clases de sentimiento (0 = Negativo, 1 = Positivo).\n",
    "    \"\"\"\n",
    "\n",
    "    # Creamos un gráfico de barras con conteo por clase usando seaborn.\n",
    "    # 'x' es la columna del DataFrame que contiene las etiquetas.\n",
    "    # 'hue' es igual a 'x' para mostrar diferente color por clase.\n",
    "    # 'palette' define los colores del gráfico.\n",
    "    # 'legend=False' oculta la leyenda adicional.\n",
    "    sns.countplot(data=df, x='sentimiento', hue='sentimiento', palette='Set2', legend=False)\n",
    "\n",
    "    # Título del gráfico\n",
    "    plt.title(\"Distribución de sentimientos\")\n",
    "\n",
    "    # Etiqueta del eje X\n",
    "    plt.xlabel(\"Sentimiento\")\n",
    "\n",
    "    # Etiqueta del eje Y\n",
    "    plt.ylabel(\"Cantidad\")\n",
    "\n",
    "    # Cambiamos los valores del eje X: 0 → \"Negativo\", 1 → \"Positivo\"\n",
    "    plt.xticks([0, 1], ['Negativo', 'Positivo'])\n",
    "\n",
    "    # Mostramos el gráfico en pantalla\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5bd1d1",
   "metadata": {},
   "source": [
    "### 4. `procesamiento/vectorizacion.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63db9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 4: Representación vectorial\n",
    "\n",
    "# Importamos el vectorizador TF-IDF desde sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Definimos una función que recibe un corpus de texto como lista de frases\n",
    "def vectorizar_texto(corpus):\n",
    "    \"\"\"\n",
    "    Convierte texto en vectores TF-IDF.\n",
    "    Retorna: \n",
    "      - la matriz resultante (X),\n",
    "      - el objeto vectorizador para futuras transformaciones.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creamos una instancia del vectorizador TF-IDF, que convierte texto en vectores numéricos\n",
    "    vectorizador = TfidfVectorizer()\n",
    "\n",
    "    # Ajustamos el vectorizador al corpus y transformamos el texto en una matriz dispersa\n",
    "    X = vectorizador.fit_transform(corpus)\n",
    "\n",
    "    # Retornamos la matriz TF-IDF y el vectorizador por separado\n",
    "    return X, vectorizador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47cdb0f",
   "metadata": {},
   "source": [
    "### 5. `modelo/entrenamiento.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca003a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 5: Modelado y evaluación\n",
    "\n",
    "# Importamos el modelo de regresión logística para clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Importamos herramienta para dividir el dataset en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importamos métricas de evaluación: reporte de clasificación y matriz de confusión\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Importamos bibliotecas de visualización para graficar la matriz de confusión\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definimos una función que entrena y evalúa un modelo usando los datos vectorizados y las etiquetas\n",
    "def entrenar_y_evaluar(X, y):\n",
    "    \"\"\"\n",
    "    Entrena un modelo y evalúa su desempeño usando Regresión Logística.\n",
    "    \"\"\"\n",
    "\n",
    "    # Se divide el dataset en 75% para entrenamiento y 25% para prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    # Se crea una instancia del modelo de Regresión Logística con hasta 1000 iteraciones\n",
    "    modelo = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Se entrena el modelo con los datos de entrenamiento\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    # Se hacen predicciones sobre los datos de prueba\n",
    "    pred = modelo.predict(X_test)\n",
    "\n",
    "    # Se imprime el reporte de clasificación: precisión, recall y f1-score\n",
    "    print(\"\\nReporte de clasificación:\")\n",
    "    print(classification_report(\n",
    "        y_test, pred,\n",
    "        target_names=[\"Negativo\", \"Positivo\"],  # Nombres para las clases\n",
    "        zero_division=0  # Evita errores si hay división por cero\n",
    "    ))\n",
    "\n",
    "    # Se genera la matriz de confusión para evaluar el rendimiento del modelo\n",
    "    matriz = confusion_matrix(y_test, pred)\n",
    "\n",
    "    # Visualiza la matriz de confusión como un mapa de calor\n",
    "    sns.heatmap(\n",
    "        matriz, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=[\"Neg\", \"Pos\"], yticklabels=[\"Neg\", \"Pos\"]\n",
    "    )\n",
    "\n",
    "    # Se agregan etiquetas y título al gráfico\n",
    "    plt.title(\"Matriz de Confusión\")\n",
    "    plt.xlabel(\"Predicción\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3c334",
   "metadata": {},
   "source": [
    "### 6. `main.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e98db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de entrada: ensamblaje de las capas\n",
    "\n",
    "# Importamos la función para cargar los datos (Capa 1: Recolección)\n",
    "from datos.recoleccion import cargar_datos\n",
    "\n",
    "# Importamos la función para limpiar el texto (Capa 2: Limpieza)\n",
    "from procesamiento.limpieza import limpiar_texto\n",
    "\n",
    "# Importamos la función para graficar la distribución de clases (Capa 3: Exploración)\n",
    "from procesamiento.exploracion import graficar_distribucion\n",
    "\n",
    "# Importamos la función para vectorizar el texto (Capa 4: Representación vectorial)\n",
    "from procesamiento.vectorizacion import vectorizar_texto\n",
    "\n",
    "# Importamos la función para entrenar y evaluar el modelo (Capa 5: Modelado)\n",
    "from modelo.entrenamiento import entrenar_y_evaluar\n",
    "\n",
    "# Verificamos que este script se esté ejecutando directamente (no importado)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1. Recolección de datos: carga un conjunto de reseñas simuladas con sus etiquetas\n",
    "    df = cargar_datos()\n",
    "\n",
    "    # 2. Limpieza del texto: se aplica la limpieza a cada reseña\n",
    "    df['texto_limpio'] = df['texto'].apply(limpiar_texto)\n",
    "\n",
    "    # 3. Análisis exploratorio: muestra la cantidad de reseñas positivas vs negativas\n",
    "    graficar_distribucion(df)\n",
    "\n",
    "    # 4. Vectorización del texto: convierte las reseñas limpias en vectores numéricos (TF-IDF)\n",
    "    X, _ = vectorizar_texto(df['texto_limpio'])\n",
    "\n",
    "    # 5. Modelado: se entrena el modelo con los vectores y las etiquetas, y se evalúa\n",
    "    y = df['sentimiento']\n",
    "    entrenar_y_evaluar(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a8fc9",
   "metadata": {},
   "source": [
    "### 6. `requirements.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0aad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencias para análisis de sentimientos\n",
    "pandas>=1.5.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.1.0\n",
    "matplotlib>=3.5.0\n",
    "seaborn>=0.11.0\n",
    "datasets\n",
    "snscrape\n",
    "\n",
    "# Para procesamiento de texto\n",
    "regex>=2022.0.0\n",
    "\n",
    "# Para visualizaciones\n",
    "plotly>=5.0.0\n",
    "\n",
    "# Para manejo de datos\n",
    "openpyxl>=3.0.0\n",
    "xlrd>=2.0.0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95af7c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Cambio de la capa de recolección\n",
    "\n",
    "#### 1. Reemplazar el contenido de `recoleccion.py` por esto:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capa 1: Recolección de datos desde Hugging Face\n",
    "\n",
    "# Importamos la librería pandas para manipular estructuras de datos tipo DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Importamos la función 'load_dataset' desde la librería 'datasets' de Hugging Face\n",
    "# Esta función permite acceder a miles de datasets públicos para tareas de NLP y ML\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Definimos la función principal para cargar los datos\n",
    "def cargar_datos(n=100):\n",
    "    \"\"\"\n",
    "    Carga reseñas reales de productos desde el dataset 'amazon_polarity'.\n",
    "    Devuelve un DataFrame con columnas 'texto' y 'sentimiento'.\n",
    "    1 = positivo, 0 = negativo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Usamos 'load_dataset' para cargar el conjunto de datos llamado 'amazon_polarity'\n",
    "    # 'split=\"train\"' indica que queremos la partición de entrenamiento del dataset\n",
    "    dataset = load_dataset(\"amazon_polarity\", split=\"train\")\n",
    "\n",
    "    # Extraemos las primeras 'n' reseñas de la columna 'content' (el texto de la reseña)\n",
    "    textos = dataset[:n][\"content\"]\n",
    "\n",
    "    # Extraemos las primeras 'n' etiquetas de la columna 'label' (0 = negativo, 1 = positivo)\n",
    "    etiquetas = dataset[:n][\"label\"]\n",
    "\n",
    "    # Creamos un DataFrame de pandas con dos columnas: 'texto' y 'sentimiento'\n",
    "    df = pd.DataFrame({\n",
    "        \"texto\": textos,\n",
    "        \"sentimiento\": etiquetas\n",
    "    })\n",
    "\n",
    "    # Retornamos el DataFrame para ser usado por las siguientes capas del sistema\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdfd9a",
   "metadata": {},
   "source": [
    "### Para ejecutarlo\n",
    "\n",
    "Desde consola:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "cd analisis_sentimientos\n",
    "python main.py\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d5c38",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusión\n",
    "\n",
    "Cada una de estas etapas forma parte de una **cadena crítica** en el procesamiento textual. Omitir una etapa o realizarla de forma incorrecta puede llevar a malos resultados o interpretaciones erróneas. Por eso, la calidad y seguridad del código en cada paso es tan importante como la técnica misma.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
