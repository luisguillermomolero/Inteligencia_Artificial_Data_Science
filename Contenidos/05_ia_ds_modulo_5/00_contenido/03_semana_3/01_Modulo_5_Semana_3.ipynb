{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a1bbb4",
   "metadata": {},
   "source": [
    "# Clase 3 – Desarrollo de aplicaciones interactivas con servicios web generativos\n",
    "\n",
    "## **¿Por qué envolver a los modelos generativos en servicios web?**\n",
    "\n",
    "Un **servicio web** es una forma de poner un modelo “detrás de una puerta” a la que cualquiera puede acceder usando internet o una red interna.\n",
    "En lugar de que cada persona tenga que instalar y entender el modelo, el servicio web hace el trabajo difícil y solo entrega el resultado.\n",
    "\n",
    "### **1. ¿Qué problema resuelve?**\n",
    "\n",
    "* **Facilita el acceso:** cualquiera puede usar el modelo desde un navegador o aplicación sin instalar nada.\n",
    "* **Centraliza la gestión:** si mejoras o corriges el modelo, solo actualizas el servicio, no a cada usuario.\n",
    "* **Escala para muchos usuarios:** puedes atender a decenas, cientos o miles de peticiones al mismo tiempo.\n",
    "* **Seguridad y control:** decides quién puede usarlo, cómo y con qué límites (por ejemplo, cuántas peticiones por minuto).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Casos de uso prácticos**\n",
    "\n",
    "1. **Chatbots internos para empresas**\n",
    "\n",
    "   * Ejemplo: un bot que responda preguntas sobre políticas de la empresa.\n",
    "   * Beneficio: no hay que instalar nada en cada computador; todos acceden por una URL segura.\n",
    "\n",
    "2. **Asistentes de redacción**\n",
    "\n",
    "   * Ejemplo: una API que ayude a escribir correos o informes con estilo profesional.\n",
    "   * Beneficio: se integra fácilmente en aplicaciones como Word o Gmail mediante un botón.\n",
    "\n",
    "3. **Generación de imágenes para marketing**\n",
    "\n",
    "   * Ejemplo: un servicio que crea banners o publicaciones automáticas para redes sociales.\n",
    "   * Beneficio: el equipo de diseño no necesita aprender IA, solo sube texto y recibe la imagen.\n",
    "\n",
    "4. **Análisis de datos o informes automáticos**\n",
    "\n",
    "   * Ejemplo: un modelo que resume reportes o hace predicciones sobre ventas.\n",
    "   * Beneficio: se ejecuta desde un panel web y no desde la computadora del analista.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1226b",
   "metadata": {},
   "source": [
    "## **1. Patrón: client → API → orquestador → proveedor de modelo**\n",
    "\n",
    "![Diagrama explicativo](img/img1.png)\n",
    "\n",
    "\n",
    "* **Client (cliente)** → es la aplicación que usa la IA.\n",
    "  *Ejemplo:* Una app de chat en tu empresa.\n",
    "\n",
    "* **API** → es la puerta de entrada. El cliente no habla directo con la IA, sino que envía peticiones a una API (como un “mesero” que lleva tu orden).\n",
    "\n",
    "* **Orquestador** → es el “cerebro intermedio” que decide *qué modelo usar*, cómo procesar la petición y qué hacer si algo falla.\n",
    "  *Ejemplo:* si la empresa usa varios modelos (uno barato para pruebas, otro potente para tareas críticas), el orquestador decide cuál usar.\n",
    "\n",
    "* **Proveedor de modelo** → es quien tiene el modelo generativo: puede ser OpenAI (ChatGPT), HuggingFace o un servidor interno (on-prem).\n",
    "\n",
    "**¿Por qué se hace así?**\n",
    "Porque **no conviene conectar la app directo al modelo**:\n",
    "\n",
    "* Mejor control de seguridad\n",
    "* Mejor manejo de costos\n",
    "* Más flexibilidad si cambias de proveedor\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Contratos: qué significa eso**\n",
    "\n",
    "Un *contrato* es como un **acuerdo escrito entre la API y quien la usa**. Incluye:\n",
    "\n",
    "* **Esquemas** → cómo deben lucir las peticiones y respuestas (qué campos hay, qué datos se esperan).\n",
    "* **Versionamiento (`/v1/...`)** → si la API cambia, no rompe lo que ya existe.\n",
    "* **Idempotencia** → si mandas la misma orden dos veces, el resultado debe ser el mismo (para evitar cobros duplicados o errores).\n",
    "* **Observabilidad** → poder monitorear qué está pasando (tiempos de respuesta, fallos).\n",
    "* **FinOps** → controlar **cuánto cuesta cada petición**, para no llevarte sorpresas con la factura.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Riesgos y cómo mitigarlos**\n",
    "\n",
    "Usar modelos generativos no es gratis ni 100% seguro. Algunos problemas típicos:\n",
    "\n",
    "1. **Inyección de prompt** → alguien mete texto malicioso para engañar al modelo.\n",
    "   *Solución:* filtrar y validar lo que entra.\n",
    "\n",
    "2. **Fuga de datos** → la IA podría revelar datos sensibles.\n",
    "   *Solución:* nunca enviar información privada sin protección.\n",
    "\n",
    "3. **Overreliance** → confiar ciegamente en la IA y dejar de revisar su trabajo.\n",
    "   *Solución:* siempre validar los resultados.\n",
    "\n",
    "4. **DoS (Denial of Service)** → demasiadas peticiones a la API pueden tumbarla.\n",
    "   *Solución:* poner límites de uso y balanceadores.\n",
    "\n",
    "5. **Supply chain** → depender de software externo que podría ser vulnerable.\n",
    "   *Solución:* auditar y actualizar dependencias.\n",
    "\n",
    "6. **Output inseguro** → el modelo podría generar algo tóxico o peligroso.\n",
    "   *Solución:* filtros de contenido y revisiones humanas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117890f0",
   "metadata": {},
   "source": [
    "## Construir **una API de generación de texto** siguiendo la estructura modular:\n",
    "\n",
    "\n",
    "```\n",
    "app/\n",
    "├── api/    \n",
    "│   └── app.py              # Servidor FastAPI principal\n",
    "├── client/\n",
    "│   └── index.html          # Cliente web\n",
    "├── orchestrator/\n",
    "│   └── orchestrator.py     # Lógica de negocio\n",
    "├── provider/\n",
    "│   └── provider_local.py   # Generador de texto local\n",
    "└── requirements.txt        # Solo dependencias necesarias\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5c5fe",
   "metadata": {},
   "source": [
    "\n",
    "## 1) ¿Qué es una **API REST**?\n",
    "\n",
    "**Definición simple:**\n",
    "Una API (Interfaz de Programación de Aplicaciones) es un conjunto de reglas que permite que dos programas se comuniquen. **REST** es un estilo para diseñar APIs que usa **HTTP** (el mismo protocolo de la web).\n",
    "\n",
    "**Analogía para la clase:**\n",
    "Piensa en un restaurante:\n",
    "\n",
    "* El **cliente** (navegador o app) hace un pedido (request).\n",
    "* El **mesero** (API) lleva la orden a la cocina y devuelve la comida (response).\n",
    "* El **menu** son los recursos que puedes pedir (usuarios, productos, mensajes).\n",
    "\n",
    "**Propiedades clave de REST:**\n",
    "\n",
    "* **Stateless (sin estado):** cada petición contiene la información necesaria; el servidor no guarda estado entre peticiones.\n",
    "* **Recursos identificables:** todo se modela como recursos (p. ej. `/products/123`).\n",
    "* **Representaciones:** los recursos se devuelven en formatos como JSON.\n",
    "* **Interfaz uniforme:** uso consistente de métodos HTTP (GET/POST/PUT/DELETE).\n",
    "* **Caché, capa, código bajo demanda** (mencionar brevemente).\n",
    "\n",
    "**Verbos (métodos - ejemplo):**\n",
    "\n",
    "* `GET /products` → obtener lista de productos.\n",
    "* `GET /products/1` → obtener producto id=1.\n",
    "* `POST /products` → crear un producto.\n",
    "* `PUT /products/1` → actualizar producto.\n",
    "* `DELETE /products/1` → borrar producto.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced6634",
   "metadata": {},
   "source": [
    "## 2) ¿Qué es **FastAPI**?\n",
    "\n",
    "FastAPI es un **framework moderno para construir APIs web** en Python. Su principal objetivo es que puedas desarrollar **servicios rápidos, seguros y fáciles de mantener** sin tener que escribir código complicado.\n",
    "\n",
    "### Características principales:\n",
    "\n",
    "1. **Rápido (de verdad)**\n",
    "\n",
    "   * Su nombre no es casualidad: está construido sobre **Starlette** (para la parte web) y **Pydantic** (para la validación de datos).\n",
    "   * Gracias a esto, aprovecha Python **asíncrono (async/await)**, logrando un rendimiento comparable a Node.js o Go.\n",
    "\n",
    "2. **Simple y productivo**\n",
    "\n",
    "   * Permite escribir menos código y lograr más.\n",
    "   * La sintaxis es clara y directa: con solo unas líneas puedes crear endpoints funcionales.\n",
    "   * Ideal tanto para proyectos pequeños como grandes aplicaciones empresariales.\n",
    "\n",
    "3. **Validación automática de datos**\n",
    "\n",
    "   * Al usar **Pydantic**, valida automáticamente que la información enviada por el cliente tenga el formato correcto (por ejemplo, que un email sea email o que un número no sea texto).\n",
    "\n",
    "4. **Documentación automática**\n",
    "\n",
    "   * Sin escribir nada adicional, FastAPI genera una **interfaz interactiva de documentación** en la ruta `/docs` usando **Swagger UI**.\n",
    "   * También ofrece otra documentación alternativa en `/redoc`.\n",
    "   * Esto facilita probar los endpoints desde el navegador sin necesidad de usar herramientas externas como Postman.\n",
    "\n",
    "5. **Compatible con estándares modernos**\n",
    "\n",
    "   * Sigue las especificaciones **OpenAPI** (antes Swagger), lo que hace fácil integrar tu API con otras aplicaciones y servicios.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37d3c0",
   "metadata": {},
   "source": [
    "## **2. Paso a paso del código**\n",
    "\n",
    "### **requirements.txt**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi\n",
    "uvicorn\n",
    "pydantic\n",
    "python-multipart\n",
    "httpx\n",
    "markovify\n",
    "openai\n",
    "python-dotenv\n",
    "google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf1e15",
   "metadata": {},
   "source": [
    "**Ejecutar la siguiente instrucción**\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5e1b5",
   "metadata": {},
   "source": [
    "### **provider/provider_local.py**\n",
    "\n",
    "Módulo que conecta con el modelo. Inicialmente usaremos un modelo *simulado* para que no dependas de credenciales. Luego puedes cambiarlo por OpenAI o HF real.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provider/provider_local.py\n",
    "import random\n",
    "import re\n",
    "\n",
    "async def generate_text_local(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera texto usando reglas locales y plantillas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir prompt a minúsculas para análisis\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Detectar el tipo de prompt\n",
    "        if any(word in prompt_lower for word in ['historia', 'cuento', 'story', 'narrar']):\n",
    "            return generate_story(prompt)\n",
    "        elif any(word in prompt_lower for word in ['poema', 'poem', 'verso', 'rima']):\n",
    "            return generate_poem(prompt)\n",
    "        elif any(word in prompt_lower for word in ['explica', 'explain', 'qué es', 'what is']):\n",
    "            return generate_explanation(prompt)\n",
    "        elif any(word in prompt_lower for word in ['describe', 'describe', 'cómo es', 'how is']):\n",
    "            return generate_description(prompt)\n",
    "        else:\n",
    "            return generate_general_response(prompt)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error al generar texto: {str(e)}\"\n",
    "\n",
    "def generate_story(prompt: str) -> str:\n",
    "    \"\"\"Genera una historia corta basada en el prompt.\"\"\"\n",
    "    stories = [\n",
    "        f\"Érase una vez, en un lugar muy especial, donde {prompt.lower()} se convirtió en el protagonista de una aventura increíble. La historia comenzó cuando...\",\n",
    "        f\"En un mundo lleno de posibilidades, {prompt.lower()} marcó el inicio de algo extraordinario. Los personajes se encontraron y...\",\n",
    "        f\"La magia de {prompt.lower()} se desplegó ante los ojos de todos. Era como si el universo entero conspirara para crear algo hermoso...\"\n",
    "    ]\n",
    "    return random.choice(stories)\n",
    "\n",
    "def generate_poem(prompt: str) -> str:\n",
    "    \"\"\"Genera un poema corto basado en el prompt.\"\"\"\n",
    "    poems = [\n",
    "        f\"En el silencio de la noche,\\n{prompt.lower()} brilla con luz propia,\\ncomo una estrella fugaz\\nque ilumina el camino.\",\n",
    "        f\"Entre versos y rimas,\\n{prompt.lower()} se convierte en arte,\\nun suspiro del alma\\nque toca el corazón.\",\n",
    "        f\"La poesía de {prompt.lower()}\\nfluye como un río,\\nlleno de emociones\\nque nunca se agotan.\"\n",
    "    ]\n",
    "    return random.choice(poems)\n",
    "\n",
    "def generate_explanation(prompt: str) -> str:\n",
    "    \"\"\"Genera una explicación basada en el prompt.\"\"\"\n",
    "    explanations = [\n",
    "        f\"{prompt} es un concepto fascinante que representa la creatividad y la innovación en su máxima expresión. Es algo que trasciende los límites convencionales.\",\n",
    "        f\"Para entender {prompt}, debemos explorar sus múltiples dimensiones. Es como un diamante con muchas facetas, cada una revelando algo nuevo y sorprendente.\",\n",
    "        f\"{prompt} es la manifestación de ideas que transforman la realidad. Es el puente entre lo que imaginamos y lo que podemos crear.\"\n",
    "    ]\n",
    "    return random.choice(explanations)\n",
    "\n",
    "def generate_description(prompt: str) -> str:\n",
    "    \"\"\"Genera una descripción basada en el prompt.\"\"\"\n",
    "    descriptions = [\n",
    "        f\"{prompt} se presenta como una experiencia única y memorable. Sus características lo hacen especial y digno de ser recordado.\",\n",
    "        f\"Al observar {prompt}, uno puede apreciar la belleza en los detalles. Es como un cuadro que revela nuevos matices con cada mirada.\",\n",
    "        f\"{prompt} tiene una presencia que llena el espacio con su energía. Es imposible ignorarlo, y una vez que lo conoces, nunca lo olvidas.\"\n",
    "    ]\n",
    "    return random.choice(descriptions)\n",
    "\n",
    "def generate_general_response(prompt: str) -> str:\n",
    "    \"\"\"Genera una respuesta general basada en el prompt.\"\"\"\n",
    "    responses = [\n",
    "        f\"El prompt '{prompt}' ha inspirado una respuesta creativa que combina imaginación y lógica. Es fascinante cómo las palabras pueden abrir puertas a nuevos mundos.\",\n",
    "        f\"Cuando pienso en '{prompt}', mi mente se llena de posibilidades infinitas. Es como tener una llave que abre múltiples puertas de la creatividad.\",\n",
    "        f\"'{prompt}' es más que solo palabras; es un portal a la imaginación. Cada vez que lo considero, descubro algo nuevo y emocionante.\"\n",
    "    ]\n",
    "    return random.choice(responses) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333e105",
   "metadata": {},
   "source": [
    "### **orchestrator/orchestrator.py**\n",
    "\n",
    "Aquí controlamos validaciones, límites y lógica antes de llamar al proveedor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d75481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchestrator/orchestrator.py\n",
    "from provider.provider_local import generate_text_local\n",
    "\n",
    "MAX_PROMPT_LENGTH = 200  # Seguridad: limitar entrada\n",
    "\n",
    "async def handle_generation(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Lógica de negocio para generar texto usando el generador local.\n",
    "    \"\"\"\n",
    "    if len(prompt) == 0:\n",
    "        raise ValueError(\"El prompt no puede estar vacío.\")\n",
    "    if len(prompt) > MAX_PROMPT_LENGTH:\n",
    "        raise ValueError(\"El prompt excede el tamaño máximo permitido.\")\n",
    "    \n",
    "    # Usar el generador local para texto creativo\n",
    "    return await generate_text_local(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761141e6",
   "metadata": {},
   "source": [
    "### **api/app.py**\n",
    "\n",
    "El servidor FastAPI que expone el endpoint `/generate`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dbbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api/app.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from orchestrator.orchestrator import handle_generation\n",
    "\n",
    "app = FastAPI(title=\"API de generación de texto\", version=\"1.0\")\n",
    "\n",
    "# Configurar CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # En producción, especifica los dominios permitidos\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Definir el esquema de entrada usando Pydantic\n",
    "class PromptRequest(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"\n",
    "    Endpoint de prueba para verificar que el servidor funciona.\n",
    "    \"\"\"\n",
    "    return {\"message\": \"Servidor funcionando correctamente\"}\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: PromptRequest):\n",
    "    \"\"\"\n",
    "    Endpoint para generar texto a partir de un prompt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = await handle_generation(request.prompt)\n",
    "        return {\"result\": result}\n",
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "    except Exception:\n",
    "        raise HTTPException(status_code=500, detail=\"Error interno del servidor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4a9f6",
   "metadata": {},
   "source": [
    "Ejecutar servidor:\n",
    "\n",
    "```bash\n",
    "uvicorn api.app:app --reload\n",
    "```\n",
    "\n",
    "Visitar: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f68340",
   "metadata": {},
   "source": [
    "### **client/index.html**\n",
    "\n",
    "Un cliente sencillo que llama a la API usando `httpx`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <title>Cliente Web</title>\n",
    "    <style>\n",
    "      body {\n",
    "        font-family: Arial, sans-serif;\n",
    "        margin: 40px;\n",
    "        background-color: #f9f9f9;\n",
    "      }\n",
    "      .container {\n",
    "        max-width: 400px;\n",
    "        margin: auto;\n",
    "        padding: 20px;\n",
    "        background: white;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "      }\n",
    "      input,\n",
    "      button {\n",
    "        width: 100%;\n",
    "        padding: 10px;\n",
    "        margin: 10px 0;\n",
    "        font-size: 1rem;\n",
    "      }\n",
    "      #respuesta {\n",
    "        margin-top: 20px;\n",
    "        padding: 10px;\n",
    "        background: #eef;\n",
    "        border-radius: 5px;\n",
    "        min-height: 50px;\n",
    "      }\n",
    "    </style>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"container\">\n",
    "      <h2>Enviar prompt al servidor</h2>\n",
    "      <input type=\"text\" id=\"prompt\" placeholder=\"Escribe un prompt...\" />\n",
    "      <button onclick=\"enviarPrompt()\">Enviar</button>\n",
    "      <div id=\"respuesta\">La respuesta aparecerá aquí...</div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "      async function enviarPrompt() {\n",
    "        const prompt = document.getElementById(\"prompt\").value;\n",
    "        const respuestaDiv = document.getElementById(\"respuesta\");\n",
    "        respuestaDiv.innerHTML = \"Enviando...\";\n",
    "\n",
    "        try {\n",
    "          const response = await fetch(\"http://127.0.0.1:8000/generate\", {\n",
    "            method: \"POST\",\n",
    "            headers: { \"Content-Type\": \"application/json\" },\n",
    "            body: JSON.stringify({ prompt }),\n",
    "          });\n",
    "          const data = await response.json();\n",
    "          respuestaDiv.innerHTML = data.result || JSON.stringify(data);\n",
    "        } catch (error) {\n",
    "          respuestaDiv.innerHTML = \"Error al conectar con el servidor.\";\n",
    "          console.error(error);\n",
    "        }\n",
    "      }\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf184f3",
   "metadata": {},
   "source": [
    "Ejecutar:\n",
    "\n",
    "```bash\n",
    "python client/main.py\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a132ccb",
   "metadata": {},
   "source": [
    "### **Proyecto de API de generación de texto** incorporando elementos de IA.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "app/\n",
    "├── api/    \n",
    "│   └── app.py              # Servidor FastAPI principal\n",
    "├── client/\n",
    "│   └── index.html          # Cliente web\n",
    "├── orchestrator/\n",
    "│   └── orchestrator.py     # Lógica de negocio\n",
    "├── provider/\n",
    "│   └── provider_local.py   # Generador de texto local\n",
    "└── requirements.txt        # Solo dependencias necesarias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba514d7",
   "metadata": {},
   "source": [
    "#### **Paso 1:** Obtener una API key Gratuita\n",
    "\n",
    "1. Ir al https://aistudio.google.com/app/apikey\n",
    "\n",
    "2. Crear una cuenta o inicia sesión\n",
    "\n",
    "3. Crear una nueva API key\n",
    "\n",
    "4. IMPORTANTE: Guarda esta key en un lugar seguro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794f52b",
   "metadata": {},
   "source": [
    "#### **Paso 2:** Crear el archivo .env para las claves\n",
    "\n",
    "Ahora creamos el archivo donde guardarás tu API key:\n",
    "\n",
    "\n",
    "```\n",
    "app/.env\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY=AIzaSyBV5W0ay18d-W0yvSAkLaFvUESklBGmqng\n",
    "OPENAI_MODEL=gpt-3.5-turbo\n",
    "MAX_PROMPT_LENGTH=200\n",
    "MAX_TOKENS=150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe728721",
   "metadata": {},
   "source": [
    "#### **Paso 3:** Crear un archivo de configuración para las claves\n",
    "\n",
    "Primero, vamos a crear un archivo para manejar las configuraciones de manera segura:\n",
    "\n",
    "```\n",
    "app/config.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b39366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno desde archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuración de Gemini\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configuración de la aplicación\n",
    "MAX_PROMPT_LENGTH = int(os.getenv(\"MAX_PROMPT_LENGTH\", \"200\"))\n",
    "MAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", \"150\"))\n",
    "\n",
    "# Verificar que la API key esté configurada\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"ADVERTENCIA: GEMINI_API_KEY no está configurada\")\n",
    "    print(\"Crea un archivo .env con tu API key de Gemini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129aa266",
   "metadata": {},
   "source": [
    "#### **Paso 4:** Crear el nuevo proveedor \n",
    "\n",
    "```\n",
    "app/provider/provider_gemini.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70858d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provider/provider_gemini.py\n",
    "import google.generativeai as genai\n",
    "from config import GEMINI_API_KEY, MAX_TOKENS\n",
    "\n",
    "# Configurar Gemini\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "async def generate_text_gemini(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera texto usando la API de Google Gemini.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar que la API key esté configurada\n",
    "        if not GEMINI_API_KEY:\n",
    "            return \"Error: API key de Gemini no configurada. Revisa tu archivo .env\"\n",
    "        \n",
    "        # Configurar el modelo Gemini - Usando un modelo más básico\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        \n",
    "        # Crear el prompt para Gemini\n",
    "        full_prompt = f\"\"\"\n",
    "        Eres un asistente creativo que genera texto en español. \n",
    "        Responde de manera creativa, amigable y útil.\n",
    "        \n",
    "        Prompt del usuario: {prompt}\n",
    "        \n",
    "        Genera una respuesta creativa y útil:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generar texto con Gemini\n",
    "        response = model.generate_content(full_prompt)\n",
    "        \n",
    "        # Extraer y retornar la respuesta\n",
    "        if response.text:\n",
    "            return response.text.strip()\n",
    "        else:\n",
    "            return f\"Respuesta generada para: {prompt}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error al generar texto: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a8724",
   "metadata": {},
   "source": [
    "#### **Paso 5:** Actualizar requirements.txt\n",
    "\n",
    "\n",
    "```\n",
    "app/requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi\n",
    "uvicorn\n",
    "pydantic\n",
    "python-multipart\n",
    "httpx\n",
    "markovify\n",
    "openai\n",
    "python-dotenv\n",
    "google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b91b61",
   "metadata": {},
   "source": [
    "**Ejecutar la siguiente instrucción**\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fab961",
   "metadata": {},
   "source": [
    "#### **Paso 6:** Actualizar el orquestador\n",
    "\n",
    "Ahora modificamos el orquestador para usar el nuevo provider de GENINI:\n",
    "\n",
    "```\n",
    "app/orchestrator/orchestrator.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe98e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchestrator/orchestrator.py\n",
    "from provider.provider_gemini import generate_text_gemini\n",
    "from config import MAX_PROMPT_LENGTH\n",
    "\n",
    "async def handle_generation(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Lógica de negocio para generar texto usando Google Gemini.\n",
    "    \"\"\"\n",
    "    if len(prompt) == 0:\n",
    "        raise ValueError(\"El prompt no puede estar vacío.\")\n",
    "    if len(prompt) > MAX_PROMPT_LENGTH:\n",
    "        raise ValueError(\"El prompt excede el tamaño máximo permitido.\")\n",
    "    \n",
    "    # Usar Gemini para generar texto creativo\n",
    "    return await generate_text_gemini(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f548f",
   "metadata": {},
   "source": [
    "#### **Paso 8:** Crear un archivo .gitignore\n",
    "\n",
    "Es importante que no subas tu API key a Git:\n",
    "\n",
    "```\n",
    "app/.gitignore\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4352dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivos de configuración con claves\n",
    ".env\n",
    ".env.local\n",
    ".env.production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01719a",
   "metadata": {},
   "source": [
    "Ejecutar servidor:\n",
    "\n",
    "```bash\n",
    "uvicorn api.app:app --reload\n",
    "```\n",
    "\n",
    "Visitar: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e9f15",
   "metadata": {},
   "source": [
    "# Ahora, vamos a mejorar *el proyecto de API de generación de texto* incorporando **elementos de IA**. Vamos a **modificar los archivos que ya tienes**:\n",
    "\n",
    "`provider/provider.py`\n",
    "\n",
    "`orchestrator/orchestrator.py`\n",
    "\n",
    "`api/app.py`\n",
    "\n",
    "`client/main.py` para añadir:\n",
    "\n",
    "\n",
    "* Integración opcional con **OpenAI** (si hay clave),\n",
    "* **Fallback local** ligero con **Markovify** (funciona sin clave y sin descargar modelos pesados),\n",
    "* Streaming simulado,\n",
    "* Plantilla (prompt engineering) y pre/post-procesado,\n",
    "* Ejemplos y corpus de demostración,\n",
    "* Instrucciones claras dónde pegar cada fragmento de código y cómo ejecutar.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f305e",
   "metadata": {},
   "source": [
    "## librerías mínimas\n",
    "pip install fastapi uvicorn httpx pydantic\n",
    "\n",
    "## librerías opcionales para IA (fallback local y OpenAI)\n",
    "pip install markovify     # generador local liviano\n",
    "pip install openai        # si usarás OpenAI (opcional)\n",
    "```\n",
    "\n",
    "> Nota: `markovify` es una librería pequeña que genera texto a partir de un corpus —ideal para demo si nadie tiene clave OpenAI o hay problemas de cuota. `openai` es opcional y requiere clave.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45427332",
   "metadata": {},
   "source": [
    "## 1) Qué se va a añadir y dónde \n",
    "\n",
    "* `provider/provider.py` → **sustituir/extender**: intentar usar OpenAI si existe clave; si no, usar Markovify; exponer funciones `generate_text(...)` y `stream_text(...)`.\n",
    "* `orchestrator/orchestrator.py` → **sustituir/editar**: añadir `prepare_prompt()` (plantilla) y `postprocess_text()` (limpieza).\n",
    "* `api/app.py` → **editar**: usar `orchestrator.prepare_prompt` y `provider.generate_text` / streaming; exponer `POST /generate` (compatible con body JSON).\n",
    "* `client/main.py` → **editar**: añadir ejemplo con header X-API-Key y ejemplo `stream=True`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09aed7",
   "metadata": {},
   "source": [
    "## 2) Código: provider/provider.py (reemplazarprovider/provider.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provider/provider.py\n",
    "import os\n",
    "import asyncio\n",
    "from typing import AsyncIterator\n",
    "\n",
    "# Intentaremos usar OpenAI si está configurado, si no, usaremos markovify como fallback.\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")  # si no existe, queda vacío\n",
    "\n",
    "# Lazy imports: evitamos fallos si openai/markovify no están instalados\n",
    "openai = None\n",
    "markov_model = None\n",
    "\n",
    "if OPENAI_KEY:\n",
    "    try:\n",
    "        import openai as _openai\n",
    "        openai = _openai\n",
    "        openai.api_key = OPENAI_KEY\n",
    "    except Exception as e:\n",
    "        openai = None\n",
    "\n",
    "# Cargar corpus para markovify si existe\n",
    "try:\n",
    "    import markovify\n",
    "    CORPUS_PATH = os.path.join(os.path.dirname(__file__), \"corpus.txt\")\n",
    "    if os.path.exists(CORPUS_PATH):\n",
    "        with open(CORPUS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            if text.strip():\n",
    "                markov_model = markovify.Text(text)\n",
    "except Exception:\n",
    "    markov_model = None\n",
    "\n",
    "\n",
    "# ---- Generación (OpenAI sync wrapped to async) ----\n",
    "def _openai_generate_sync(prompt: str, max_tokens: int, temperature: float) -> str:\n",
    "    # Usamos Completion (estandar) para compatibilidad; cambiar engine/model si se desea.\n",
    "    resp = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "    text = resp.choices[0].text\n",
    "    return text.strip()\n",
    "\n",
    "async def openai_generate(prompt: str, max_tokens: int, temperature: float) -> str:\n",
    "    # Ejecutar la llamada de OpenAI en hilo (porque la SDK es sincrona)\n",
    "    return await asyncio.to_thread(_openai_generate_sync, prompt, max_tokens, temperature)\n",
    "\n",
    "\n",
    "# ---- Markov fallback (local, muy simple) ----\n",
    "def markov_generate_sync(prompt: str, max_tokens: int, temperature: float) -> str:\n",
    "    # Si hay modelo Markov, generamos una o más frases; si no, devolvemos un eco o mock\n",
    "    if markov_model:\n",
    "        # El prompt no se utiliza directamente por markovify, así que preparamos respuesta basada en corpús\n",
    "        sentence = markov_model.make_sentence(tries=100)\n",
    "        if sentence:\n",
    "            return f\"{prompt}\\n\\n{sentence}\"\n",
    "    # fallback sencillo:\n",
    "    return f\"[LOCAL MOCK] {prompt}\"\n",
    "\n",
    "async def markov_generate(prompt: str, max_tokens: int, temperature: float) -> str:\n",
    "    return await asyncio.to_thread(markov_generate_sync, prompt, max_tokens, temperature)\n",
    "\n",
    "\n",
    "# ---- API pública: generate_text (escoge OpenAI si disponible, sino Markov/fallback) ----\n",
    "async def generate_text(prompt: str, max_tokens: int = 150, temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Genera texto. Si OPENAI_KEY está presente y la librería cargó correctamente, usa OpenAI.\n",
    "    En caso contrario, usa Markovify (si corpus disponible) o un mock simple.\n",
    "    \"\"\"\n",
    "    if openai:\n",
    "        try:\n",
    "            return await openai_generate(prompt, max_tokens, temperature)\n",
    "        except Exception:\n",
    "            # si falla OpenAI por cualquier motivo, caemos al fallback local\n",
    "            return await markov_generate(prompt, max_tokens, temperature)\n",
    "    else:\n",
    "        return await markov_generate(prompt, max_tokens, temperature)\n",
    "\n",
    "\n",
    "# ---- Streaming simulado: devuelve AsyncIterator[bytes] ----\n",
    "async def stream_text(prompt: str, max_tokens: int = 150, temperature: float = 0.7) -> AsyncIterator[bytes]:\n",
    "    \"\"\"\n",
    "    Genera texto y lo devuelve en fragmentos (bytes) para simular streaming token a token.\n",
    "    - Si se usa OpenAI con streaming real, podrías adaptar aquí.\n",
    "    - Para clase, simulamos cortando el texto en fragmentos.\n",
    "    \"\"\"\n",
    "    text = await generate_text(prompt, max_tokens=max_tokens, temperature=temperature)\n",
    "    chunk_size = 40\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        await asyncio.sleep(0.03)  # simula latencia/tokens\n",
    "        yield text[i : i + chunk_size].encode(\"utf-8\")\n",
    "    # señal de final (opcional)\n",
    "    await asyncio.sleep(0.01)\n",
    "    yield b\"[DONE]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb551008",
   "metadata": {},
   "source": [
    "**Nota:** Existe diferencia entre *OpenAI (modelo neuronal)* y *Markov (modelo estadístico)*. Este pipeline usa OpenAI si esta disponible (KEY), sino, hace usop de fallback sin necesidad de descargar modelos pesados.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1712f",
   "metadata": {},
   "source": [
    "## 3) Código: orchestrator/orchestrator.py (reemplaza o pega)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orchestrator/orchestrator.py\n",
    "import re\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Prompt engineering simple: pre y post procesamiento\n",
    "# --------------------------------------------------\n",
    "\n",
    "def prepare_prompt(user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Añade contexto y formato de sistema básico (plantilla).\n",
    "    Mostrar esto en clase como 'prompt template'.\n",
    "    \"\"\"\n",
    "    base_instructions = (\n",
    "        \"Eres un asistente conciso y educativo. Responde en lenguaje claro y breve.\\n\"\n",
    "        \"Si el usuario pide listas, devuelve bullets. Si pide código, usa bloques de código.\"\n",
    "    )\n",
    "    # Sanitizar entrada: elimina saltos duplicados y espacios innecesarios\n",
    "    p = user_prompt.strip()\n",
    "    p = re.sub(r\"\\s{2,}\", \" \", p)\n",
    "    # Construye prompt final\n",
    "    final = f\"{base_instructions}\\n\\nUsuario: {p}\\n\\nRespuesta:\"\n",
    "    return final\n",
    "\n",
    "\n",
    "def postprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia la salida (por ejemplo, recortar espacios, eliminar tokens especiales).\n",
    "    Explicar que en producción se harían validaciones más estrictas y filtros de seguridad.\n",
    "    \"\"\"\n",
    "    t = text.strip()\n",
    "    # elimina posibles tokens [DONE] si aparecen\n",
    "    t = t.replace(\"[DONE]\", \"\").strip()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594ae74",
   "metadata": {},
   "source": [
    "**Nota:** Investigar qué es *prompt engineering*? (plantilla + instrucciones de sistema). Ayuda a obtener respuestas mejores y más seguras.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e6807",
   "metadata": {},
   "source": [
    "## 4) Código: api/app.py (edita el endpoint `/generate`)\n",
    "\n",
    "Sustituir o adaptar `api/app.py` para que use `orchestrator` y `provider`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api/app.py\n",
    "import os\n",
    "import time\n",
    "from typing import Optional\n",
    "from fastapi import FastAPI, HTTPException, Header, Depends, Request\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from orchestrator import orchestrator\n",
    "from provider import provider\n",
    "\n",
    "# Config simple (puedes usar variables de entorno)\n",
    "API_KEY = os.getenv(\"SERVICE_API_KEY\", \"changeme\")\n",
    "\n",
    "app = FastAPI(title=\"API Generador de Texto (IA demo)\")\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    prompt: str = Field(..., min_length=1)\n",
    "    max_tokens: int = Field(150, ge=1, le=1024)\n",
    "    temperature: float = Field(0.7, ge=0.0, le=1.0)\n",
    "    stream: bool = Field(False)\n",
    "\n",
    "# simple auth header\n",
    "def require_api_key(x_api_key: Optional[str] = Header(None)):\n",
    "    if x_api_key != API_KEY:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "    return x_api_key\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate_endpoint(body: TextRequest, api_key: str = Depends(require_api_key), req: Request = None):\n",
    "    \"\"\"\n",
    "    Flujo:\n",
    "    1) preparar prompt (orchestrator)\n",
    "    2) llamar provider (OpenAI o fallback local)\n",
    "    3) postprocess y devolver (o streaming)\n",
    "    \"\"\"\n",
    "    prompt = orchestrator.prepare_prompt(body.prompt)\n",
    "\n",
    "    # pequeño logging\n",
    "    client_ip = req.client.host if req.client else \"unknown\"\n",
    "    print(f\"[REQUEST] from {client_ip} prompt_len={len(body.prompt)} stream={body.stream}\")\n",
    "\n",
    "    # streaming\n",
    "    if body.stream:\n",
    "        async def streamer():\n",
    "            async for chunk in provider.stream_text(prompt, max_tokens=body.max_tokens, temperature=body.temperature):\n",
    "                yield chunk\n",
    "        return StreamingResponse(streamer(), media_type=\"text/plain; charset=utf-8\")\n",
    "    # no streaming\n",
    "    raw = await provider.generate_text(prompt, max_tokens=body.max_tokens, temperature=body.temperature)\n",
    "    text = orchestrator.postprocess_text(raw)\n",
    "    return JSONResponse({\"id\": \"demo_gen_1\", \"text\": text})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa996e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) Código: client/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9821cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Cliente API de generación de texto</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Consumir API de generación de texto</h1>\n",
    "    <textarea id=\"prompt\" rows=\"4\" cols=\"50\" placeholder=\"Escribe tu prompt aquí...\"></textarea><br><br>\n",
    "    <button onclick=\"sendRequest()\">Enviar Prompt</button>\n",
    "\n",
    "    <h2>Respuesta:</h2>\n",
    "    <pre id=\"response\"></pre>\n",
    "\n",
    "    <script>\n",
    "        const API_URL = \"http://127.0.0.1:8000/generate\";\n",
    "        const API_KEY = \"changeme\";  // mejor usar variable de entorno si es producción\n",
    "\n",
    "        async function sendRequest() {\n",
    "            const promptText = document.getElementById(\"prompt\").value;\n",
    "\n",
    "            const payload = { prompt: promptText, stream: false };\n",
    "            const headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"X-API-Key\": API_KEY\n",
    "            };\n",
    "\n",
    "            try {\n",
    "                const resp = await fetch(API_URL, {\n",
    "                    method: \"POST\",\n",
    "                    headers: headers,\n",
    "                    body: JSON.stringify(payload)\n",
    "                });\n",
    "\n",
    "                if (!resp.ok) {\n",
    "                    document.getElementById(\"response\").textContent = \n",
    "                        \"Error: \" + resp.status + \" \" + resp.statusText;\n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                const data = await resp.json();\n",
    "                document.getElementById(\"response\").textContent = JSON.stringify(data, null, 2);\n",
    "            } catch (err) {\n",
    "                document.getElementById(\"response\").textContent = \"Error de conexión: \" + err;\n",
    "            }\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550008b",
   "metadata": {},
   "source": [
    "## 6) Archivo auxiliar: provider/corpus.txt\n",
    "\n",
    "Para que markovify funcione, crea un archivo ligero con textos de demostración. Crea `provider/corpus.txt` con algo así (pegar literal):\n",
    "\n",
    "```\n",
    "La inteligencia artificial está transformando la forma en que trabajamos. En este texto de ejemplo mostramos oraciones sobre IA.\n",
    "La IA permite automatizar tareas repetitivas, generar contenido y ayudar en la toma de decisiones.\n",
    "Los modelos generativos pueden producir textos en varios estilos: técnicos, persuasivos o científicos.\n",
    "Este corpus es solo para demo. En un entorno real, utilice datos relevantes y limpios.\n",
    "```\n",
    "\n",
    "**Dónde ponerlo:** `provider/corpus.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a3d1f",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Ejecutar la app (instrucciones paso a paso en clase)\n",
    "\n",
    "\n",
    "```bash\n",
    "uvicorn api.app:app --reload --port 8000\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
