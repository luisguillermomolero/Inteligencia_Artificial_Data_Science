{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c804dd46",
   "metadata": {},
   "source": [
    "# **Clase 1**: Preprocesamiento de Datos en el contexto de **Ciencia de Datos e Inteligencia Artificial**\n",
    "- **Normas internacionales** (PEP 8, ISO/IEC 25010 para calidad del software, FAIR data principles),\n",
    "- **Buenas prÃ¡cticas** de programaciÃ³n (DRY, KISS, modularidad),\n",
    "- **Cuerpos de conocimiento** relevantes (como el **DMM â€“ Data Management Maturity** y el **DAS â€“ Data Analytics Body of Knowledge**),\n",
    "- **CÃ³digo limpio** y comentado,\n",
    "- Uso de **Scikit-learn Pipelines**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c409b7",
   "metadata": {},
   "source": [
    "## ðŸ§  Clase 1: Preprocesamiento de Datos (DuraciÃ³n: 3 horas)\n",
    "\n",
    "### ðŸŽ¯ Objetivo general:\n",
    "El estudiante entenderÃ¡ y aplicarÃ¡ tÃ©cnicas fundamentales de preprocesamiento de datos para proyectos de Ciencia de Datos e IA, integrando prÃ¡cticas de calidad de cÃ³digo y uso de librerÃ­as estÃ¡ndar de Python.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805d3f8",
   "metadata": {},
   "source": [
    "## ðŸ§© Estructura de la Clase\n",
    "\n",
    "| SecciÃ³n | DuraciÃ³n | Contenido |\n",
    "|--------|----------|-----------|\n",
    "| 1 | 30 min | TeorÃ­a del preprocesamiento y su rol en la calidad de los modelos |\n",
    "| 2 | 40 min | Limpieza de datos: valores nulos, duplicados y outliers |\n",
    "| 3 | 30 min | TransformaciÃ³n: normalizaciÃ³n, estandarizaciÃ³n y codificaciÃ³n |\n",
    "| 4 | 40 min | Pipelines en Scikit-learn: construcciÃ³n paso a paso |\n",
    "| 5 | 40 min | Taller prÃ¡ctico completo con dataset real |\n",
    "| 6 | 10 min | Cierre, discusiÃ³n y evaluaciÃ³n rÃ¡pida |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f2edf",
   "metadata": {},
   "source": [
    "## ðŸ§¾ 1. IntroducciÃ³n TeÃ³rica (30 min)\n",
    "\n",
    "### ðŸ” Â¿QuÃ© es el preprocesamiento de datos?\n",
    "- Es una **etapa clave** antes de entrenar modelos de IA.\n",
    "- Mejora la **calidad del input**, garantizando que el modelo **aprenda correctamente**.\n",
    "\n",
    "### ðŸ“š Principios y EstÃ¡ndares:\n",
    "- **PEP 8**: guÃ­a de estilo para escribir cÃ³digo legible.\n",
    "- **DRY/KISS**: mantener cÃ³digo simple y evitar redundancias.\n",
    "- **ISO/IEC 25010**: calidad del software â†’ mantenibilidad, portabilidad, eficiencia.\n",
    "- **FAIR Principles**: datos deben ser encontrables, accesibles, interoperables y reutilizables.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b92bf",
   "metadata": {},
   "source": [
    "## ðŸ§¼ 2. Limpieza de Datos (40 min)\n",
    "\n",
    "### ðŸ“Œ a. Manejo de valores nulos\n",
    "\n",
    "#### ðŸ§  TeorÃ­a:\n",
    "- TÃ©cnicas:\n",
    "  - EliminaciÃ³n\n",
    "  - ImputaciÃ³n (media, mediana, moda, KNN)\n",
    "\n",
    "#### ðŸ§ª Ejemplo prÃ¡ctico:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b090341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dataset simulado\n",
    "df = pd.DataFrame({\n",
    "    'edad': [25, 30, np.nan, 40, 35],\n",
    "    'ingresos': [50000, 60000, 55000, np.nan, 58000]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b728b4f",
   "metadata": {},
   "source": [
    "# 1. Detectar nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233eec41",
   "metadata": {},
   "source": [
    "# 2. Imputar media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edad'].fillna(df['edad'].mean(), inplace=True)\n",
    "df['ingresos'].fillna(df['ingresos'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874f90e",
   "metadata": {},
   "source": [
    "# Validar cambios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbfa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a3fc2a",
   "metadata": {},
   "source": [
    "### ðŸ“Œ b. Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset con duplicado\n",
    "df_dup = pd.DataFrame({\n",
    "    'nombre': ['Ana', 'Luis', 'Luis'],\n",
    "    'edad': [28, 35, 35]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac28b2",
   "metadata": {},
   "source": [
    "# Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup_clean = df_dup.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16410ffe",
   "metadata": {},
   "source": [
    "### ðŸ“Œ c. Outliers (con IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_outliers(df, columna):\n",
    "    Q1 = df[columna].quantile(0.25)\n",
    "    Q3 = df[columna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return df[(df[columna] >= Q1 - 1.5*IQR) & (df[columna] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "df_sin_outliers = eliminar_outliers(df, 'ingresos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672695fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ 3. TransformaciÃ³n de Datos (30 min)\n",
    "\n",
    "### ðŸ“Œ a. NormalizaciÃ³n vs. EstandarizaciÃ³n\n",
    "\n",
    "#### NormalizaciÃ³n (MinMaxScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db210c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[['edad_norm']] = scaler.fit_transform(df[['edad']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c1727",
   "metadata": {},
   "source": [
    "#### EstandarizaciÃ³n (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['ingresos_std']] = scaler.fit_transform(df[['ingresos']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8e2ca",
   "metadata": {},
   "source": [
    "### ðŸ“Œ b. CodificaciÃ³n de variables categÃ³ricas\n",
    "\n",
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categ = pd.DataFrame({'color': ['rojo', 'azul', 'verde', 'azul']})\n",
    "df_onehot = pd.get_dummies(df_categ, columns=['color'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d15611",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_categ['color_encoded'] = le.fit_transform(df_categ['color'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e038ed4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— 4. Scikit-learn Pipelines (40 min)\n",
    "\n",
    "### ðŸŽ¯ Ventajas:\n",
    "- Reproducibilidad\n",
    "- Modularidad\n",
    "- IntegraciÃ³n con GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88228b7d",
   "metadata": {},
   "source": [
    "### ðŸ§ª Ejemplo prÃ¡ctico completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Dataset simulado\n",
    "df = pd.DataFrame({\n",
    "    'edad': [25, 30, np.nan, 40, 35],\n",
    "    'ingresos': [50000, 60000, 55000, np.nan, 58000],\n",
    "    'ciudad': ['BogotÃ¡', 'Cali', 'BogotÃ¡', 'MedellÃ­n', 'Cali']\n",
    "})\n",
    "\n",
    "# Separar variables\n",
    "num_features = ['edad', 'ingresos']\n",
    "cat_features = ['ciudad']\n",
    "\n",
    "# Pipelines por tipo de dato\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# ComposiciÃ³n\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "# Aplicar al dataset\n",
    "X_transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Validar resultado\n",
    "print(X_transformed.toarray() if hasattr(X_transformed, \"toarray\") else X_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3fcf1",
   "metadata": {},
   "source": [
    "## ðŸ§ª 5. Taller prÃ¡ctico completo (40 min)\n",
    "\n",
    "### ðŸ—‚ Dataset: [Titanic Dataset](https://www.kaggle.com/c/titanic/data)\n",
    "- Objetivo: preparar datos para modelo de clasificaciÃ³n de supervivencia.\n",
    "\n",
    "#### Actividades:\n",
    "1. Cargar dataset\n",
    "2. Limpiar valores nulos\n",
    "3. Codificar 'Sex', 'Embarked'\n",
    "4. Eliminar columnas innecesarias ('Name', 'Ticket')\n",
    "5. Normalizar edad y tarifa\n",
    "6. Construir pipeline completa\n",
    "7. Guardar dataset preprocesado\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
