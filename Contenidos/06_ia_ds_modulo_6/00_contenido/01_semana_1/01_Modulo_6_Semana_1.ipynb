{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAampK_0VLtL"
   },
   "source": [
    "# Clase 1: Introducci√≥n a Big Data y PySpark\n",
    "\n",
    "**Rol del docente:** Profesor de Big Data.\n",
    "\n",
    "**Objetivo general:** Al finalizar la sesi√≥n, el estudiante comprende los fundamentos de Big Data, la arquitectura b√°sica de Apache Spark y es capaz de ejecutar un an√°lisis inicial de un *dataset* grande con PySpark aplicando buenas pr√°cticas de ingenier√≠a, calidad de datos y est√°ndares.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8akMMcebVQII"
   },
   "source": [
    "## **1) Fundamentos de Big Data**\n",
    "\n",
    "### **1.1 ¬øQu√© es Big Data?**\n",
    "\n",
    "Big Data **no se trata √∫nicamente de tener ‚Äúmuchos datos‚Äù**, sino de la **capacidad de procesarlos, analizarlos y generar valor a partir de ellos** usando tecnolog√≠as avanzadas y equipos humanos especializados. Veamos sus caracter√≠sticas fundamentales y por qu√© es un concepto mucho m√°s profundo que simplemente ‚Äúdatos grandes‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### **Las 5V (y extensiones 7V)**\n",
    "\n",
    "1. **Volumen**\n",
    "\n",
    "   * Se refiere a la **cantidad masiva de datos** que generan las empresas, dispositivos IoT, redes sociales, sensores, transacciones, etc.\n",
    "   * Ejemplo: Facebook procesa m√°s de *4 petabytes* de datos diarios.\n",
    "\n",
    "**Caso real:** Lectura del dataset de ‚ÄúYellow Taxi Trip Records‚Äù de Nueva York (disponible p√∫blicamente en Amazon S3), que contiene millones de viajes de taxi por mes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalar libreria\n",
    "\n",
    "```\n",
    "pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soluci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEH6Ev8qYE_p"
   },
   "outputs": [],
   "source": [
    "# PySpark: Framework de procesamiento distribuido para grandes vol√∫menes de datos\n",
    "# (Este script descarga y analiza un parquet p√∫blico de NYC Taxi de forma local)\n",
    "\n",
    "# Importar SparkSession: punto de entrada para crear DataFrames y ejecutar operaciones Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar requests para descargar archivos v√≠a HTTP(S)\n",
    "import requests\n",
    "\n",
    "# Importar warnings para suprimir advertencias no relevantes en la salida\n",
    "import warnings\n",
    "\n",
    "# Suprimir advertencias de la librer√≠a warnings para mantener la salida limpia\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------\n",
    "# 1. Crear la sesi√≥n de Spark\n",
    "# -------------------------\n",
    "# Iniciamos el \"builder\" para configurar la SparkSession\n",
    "# .appName() define el nombre que ver√° en la UI de Spark\n",
    "# .config(...) a√±ade configuraciones espec√≠ficas (aqu√≠ ejemplos de ajustes)\n",
    "# .getOrCreate() crea la sesi√≥n si no existe o devuelve la existente\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigData_Volumen\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ajustar el nivel de logs del SparkContext para evitar ver mucho detalle (INFO/DEBUG)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 2. Definir URL y ruta local para descargar dataset\n",
    "# URL p√∫blica hacia el archivo Parquet (CloudFront p√∫blico que sirve datos de NYC taxi)\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "# Nombre del archivo local donde lo guardaremos\n",
    "local_path = \"yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "# 2b. Descargar dataset (manejo de errores b√°sico)\n",
    "try:\n",
    "    # Mensaje informativo para el usuario/alumno\n",
    "    print(\"Descargando dataset...\")\n",
    "    # Realiza la petici√≥n HTTP con un timeout para no colgar indefinidamente\n",
    "    response = requests.get(url, timeout=30)\n",
    "    # Abrir el archivo local en modo binario y escribir el contenido descargado\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # Mensaje de √©xito\n",
    "    print(\"Descarga completada\")\n",
    "except Exception as e:\n",
    "    # Si ocurre cualquier error de red o IO, lo mostramos (podr√≠as manejarlo m√°s finamente)\n",
    "    print(f\"Error en descarga: {e}\")\n",
    "\n",
    "# 3. Leer el archivo Parquet con Spark\n",
    "# read.parquet() es la forma nativa y m√°s eficiente de leer Parquet en Spark\n",
    "df = spark.read.parquet(local_path)\n",
    "\n",
    "# 4. Inspecci√≥n b√°sica del dataset\n",
    "print(\"INFORMACI√ìN DEL DATASET NYC TAXI\")\n",
    "\n",
    "# Contar el n√∫mero total de registros (count() es una acci√≥n que dispara ejecuci√≥n)\n",
    "total_records = df.count()\n",
    "# Imprimir el n√∫mero total con formato de miles\n",
    "print(f\"\\nTOTAL DE REGISTROS: {total_records:,}\")\n",
    "\n",
    "# Mostrar s√≥lo las primeras 3 filas con algunas columnas relevantes, sin truncamiento\n",
    "print(\"\\nPRIMERAS 3 FILAS:\")\n",
    "df.select(\"VendorID\", \"tpep_pickup_datetime\", \"fare_amount\", \"tip_amount\", \"total_amount\") \\\n",
    "  .show(3, truncate=False)\n",
    "\n",
    "# 4b. Mostrar esquema de forma ordenada (campo ‚Üí tipo)\n",
    "print(\"\\nESQUEMA DEL DATASET:\")\n",
    "# Creamos un diccionario para ordenar/mostrar el esquema m√°s legible\n",
    "schema_dict = {}\n",
    "for field in df.schema.fields:\n",
    "    # field.name es el nombre de la columna; field.dataType es su tipo (p. ej., TimestampType, DoubleType)\n",
    "    schema_dict[field.name] = str(field.dataType)\n",
    "\n",
    "# Imprimir el esquema enumerado y con alineaci√≥n para que sea f√°cil de leer en clase\n",
    "for i, (field_name, field_type) in enumerate(schema_dict.items(), 1):\n",
    "    print(f\"  {i:2d}. {field_name:<25} ‚Üí {field_type}\")\n",
    "\n",
    "# 5. An√°lisis estad√≠stico b√°sico (tarifas y propinas)\n",
    "print(\"\\nAN√ÅLISIS ESTAD√çSTICO B√ÅSICO\")\n",
    "\n",
    "# Columnas num√©ricas de inter√©s para el ejemplo\n",
    "numeric_cols = ['fare_amount', 'tip_amount', 'total_amount', 'trip_distance']\n",
    "\n",
    "# Iteramos sobre las columnas num√©ricas y mostramos estad√≠sticas resumidas\n",
    "for col in numeric_cols:\n",
    "    # Comprobar que la columna exista en el DataFrame (por compatibilidad entre versiones de dataset)\n",
    "    if col in df.columns:\n",
    "        # summary() produce m√©tricas: count, mean, stddev, min, max (entre otras si se pide)\n",
    "        # collect() trae las filas resultantes al driver como lista de Rows\n",
    "        stats = df.select(col).summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\").collect()\n",
    "        # Si stats no est√° vac√≠o procedemos a extraer los valores\n",
    "        if stats:\n",
    "            # Cada entrada de stats es una fila; accedemos por √≠ndice y luego por nombre de columna\n",
    "            count = stats[0][col]    # 'count' viene como string (por eso mostramos tal cual)\n",
    "            mean = stats[1][col]     # 'mean' normalmente es string convertible a float\n",
    "            std = stats[2][col]      # 'stddev' como string\n",
    "            min_val = stats[3][col]  # 'min'\n",
    "            max_val = stats[4][col]  # 'max'\n",
    "            \n",
    "            # Imprimir las estad√≠sticas con formato legible (conversi√≥n a float para formateo)\n",
    "            print(f\"\\n  {col.upper()}:\")\n",
    "            print(f\"    ‚Ä¢ Total: {count}\")\n",
    "            # Guardamos un manejo seguro al convertir a float (siempre que no sea 'null' o cadena vac√≠a)\n",
    "            try:\n",
    "                print(f\"    ‚Ä¢ Promedio: ${float(mean):.2f}\")\n",
    "            except Exception:\n",
    "                print(f\"    ‚Ä¢ Promedio: {mean} (no convertible a float)\")\n",
    "            try:\n",
    "                print(f\"    ‚Ä¢ Desviaci√≥n: ${float(std):.2f}\")\n",
    "            except Exception:\n",
    "                print(f\"    ‚Ä¢ Desviaci√≥n: {std} (no convertible a float)\")\n",
    "            # Min / Max tambi√©n formateados si son convertibles\n",
    "            try:\n",
    "                print(f\"    ‚Ä¢ Rango: ${float(min_val):.2f} - ${float(max_val):.2f}\")\n",
    "            except Exception:\n",
    "                print(f\"    ‚Ä¢ Rango: {min_val} - {max_val}\")\n",
    "\n",
    "# 6. Cerrar la sesi√≥n de Spark (liberar recursos)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4Il5PxoYFsZ"
   },
   "source": [
    "2. **Velocidad**\n",
    "\n",
    "   * Describe la **rapidez con la que los datos se generan y deben procesarse**.\n",
    "   * Ejemplo: Transacciones de tarjetas de cr√©dito que deben validarse en milisegundos para evitar fraude.\n",
    "\n",
    "\n",
    "**Caso empresarial:** detecci√≥n de fraude en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do6TqFs4ZaRP"
   },
   "source": [
    "3. **Variedad**\n",
    "\n",
    "   * Son los **diferentes formatos y fuentes**: datos estructurados (bases de datos), semiestructurados (JSON, XML) y no estructurados (im√°genes, videos, texto libre).\n",
    "   * Ejemplo: En salud, los datos provienen de historiales cl√≠nicos, estudios de imagen, sensores port√°tiles, etc.\n",
    "\n",
    "**Caso empresarial:** combinar ventas (estructuradas) con clics web (semiestructurados) para an√°lisis omnicanal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR7VqKfxZapY"
   },
   "outputs": [],
   "source": [
    "# Importar SparkSession: punto de entrada para crear DataFrames y usar Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar requests: librer√≠a para realizar peticiones HTTP (descargar archivos desde internet)\n",
    "import requests\n",
    "\n",
    "# Importar os: utilidades del sistema de archivos (rutas, creaci√≥n de carpetas, comprobaciones)\n",
    "import os\n",
    "\n",
    "# Crear sesi√≥n de Spark\n",
    "# builder() arma la configuraci√≥n; appName() nombra la aplicaci√≥n (aparece en la UI de Spark);\n",
    "# getOrCreate() crea la sesi√≥n si no existe o devuelve la existente.\n",
    "spark = SparkSession.builder.appName(\"Streaming_Clase\").getOrCreate()\n",
    "\n",
    "print(\"=== DESCARGANDO DATASETS DESDE INTERNET ===\")\n",
    "\n",
    "# Definici√≥n de la URL del dataset 1 (aeropuertos). Es un CSV p√∫blico alojado en GitHub.\n",
    "airports_url = \"https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv\"\n",
    "\n",
    "# Ruta local donde se pretende guardar el CSV de aeropuertos\n",
    "airports_file = \"airports_from_internet.csv\"\n",
    "\n",
    "# Mostrar en consola de d√≥nde se va a descargar el archivo (√∫til para depuraci√≥n y explicaci√≥n)\n",
    "print(f\"Descargando aeropuertos desde: {airports_url}\")\n",
    "\n",
    "# En producci√≥n conviene crear la carpeta con os.makedirs(output_dir, exist_ok=True) antes de escribir.\n",
    "try:\n",
    "    # Realizar la petici√≥n HTTP GET hacia la URL del CSV (puede tardar seg√∫n la red)\n",
    "    response = requests.get(airports_url, timeout=30)\n",
    "    # Abrir (o crear) el archivo local en modo texto con codificaci√≥n UTF-8 y escribir el contenido\n",
    "    with open(airports_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    # Indicar que la descarga fue exitosa\n",
    "    print(\"Dataset de aeropuertos descargado exitosamente\")\n",
    "except Exception as e:\n",
    "    # Si ocurre cualquier error (red, permiso, path inv√°lido), lo imprimimos para diagn√≥stico\n",
    "    print(f\"Error descargando aeropuertos: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset 2: Pa√≠ses (CSV real)\n",
    "# -------------------------\n",
    "\n",
    "# URL p√∫blica del dataset de c√≥digos de pa√≠s (CSV en GitHub)\n",
    "countries_url = \"https://raw.githubusercontent.com/datasets/country-codes/master/data/country-codes.csv\"\n",
    "\n",
    "# Ruta local donde se guardar√° el CSV de pa√≠ses\n",
    "countries_file = \"countries_from_internet.csv\"\n",
    "\n",
    "# Mensaje en consola indicando la URL de descarga del segundo dataset\n",
    "print(f\"Descargando pa√≠ses desde: {countries_url}\")\n",
    "\n",
    "try:\n",
    "    # Petici√≥n HTTP GET para descargar el CSV de pa√≠ses\n",
    "    response = requests.get(countries_url, timeout=30)\n",
    "    # Escribir el contenido descargado en el archivo local con codificaci√≥n UTF-8\n",
    "    with open(countries_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    # Mensaje de √©xito\n",
    "    print(\"Dataset de pa√≠ses descargado exitosamente\")\n",
    "except Exception as e:\n",
    "    # Mensaje de error para depuraci√≥n si la descarga falla\n",
    "    print(f\"Error descargando pa√≠ses: {e}\")\n",
    "\n",
    "# L√≠nea en blanco para separar secciones en la salida por consola\n",
    "print(\"\\n=== LEYENDO DATASETS DESCARGADOS ===\")\n",
    "\n",
    "# Leer el CSV de aeropuertos con Spark:\n",
    "# - .read crea un lector de DataFrame\n",
    "# - .option(\"header\", True) indica que la primera fila contiene nombres de columnas\n",
    "# - .csv(path) lee el CSV desde la ruta local proporcionada\n",
    "df_airports = spark.read.option(\"header\", True).csv(airports_file)\n",
    "\n",
    "# Leer el CSV de pa√≠ses con Spark (mismas opciones)\n",
    "df_countries = spark.read.option(\"header\", True).csv(countries_file)\n",
    "\n",
    "# Mensaje indicando que vamos a mostrar el esquema (estructura de columnas) del dataset de aeropuertos\n",
    "print(\"=== SCHEMA DE AEROPUERTOS (DESCARGADO) ===\")\n",
    "\n",
    "# printSchema() imprime en consola los nombres de columnas y sus tipos inferidos por Spark\n",
    "df_airports.printSchema()\n",
    "\n",
    "# Mensaje indicando que ahora mostraremos el esquema del dataset de pa√≠ses\n",
    "print(\"\\n=== SCHEMA DE PA√çSES (DESCARGADO) ===\")\n",
    "\n",
    "# Imprimir el esquema de pa√≠ses\n",
    "df_countries.printSchema()\n",
    "\n",
    "# Mensaje que indica que vamos a mostrar una muestra de los datos de aeropuertos\n",
    "print(\"\\n=== MUESTRA DE AEROPUERTOS ===\")\n",
    "\n",
    "# Seleccionar columnas relevantes del DataFrame de aeropuertos y mostrar 5 filas sin truncar\n",
    "# - select(...) elige columnas\n",
    "# - show(5, truncate=False) muestra 5 registros y no trunca texto largo\n",
    "df_airports.select(\"ident\", \"name\", \"iso_country\", \"type\").show(5, truncate=False)\n",
    "\n",
    "# Mensaje que indica que vamos a mostrar una muestra del dataset de pa√≠ses\n",
    "print(\"\\n=== MUESTRA DE PA√çSES ===\")\n",
    "\n",
    "# Seleccionar columnas clave del DataFrame de pa√≠ses y mostrar 5 filas sin truncar\n",
    "# Observa que el nombre de la columna \"ISO3166-1-Alpha-2\" incluye guiones; Spark lo acepta tal cual\n",
    "df_countries.select(\"ISO3166-1-Alpha-2\", \"official_name_en\", \"Continent\").show(5, truncate=False)\n",
    "\n",
    "# Finalmente, cerrar la sesi√≥n de Spark para liberar recursos del cluster/local\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSWuPgj9ZbqP"
   },
   "source": [
    "4. **Veracidad**\n",
    "\n",
    "   * Garantizar que los datos sean **fiables y precisos**, evitando errores o duplicidades.\n",
    "   * Ejemplo: Si un sensor de temperatura falla y env√≠a lecturas incorrectas, podr√≠a afectar un sistema industrial automatizado.\n",
    "\n",
    "**Caso empresarial:** asegurarse de que no entren datos corruptos al pipeline financiero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3vnNlwXZjXw"
   },
   "outputs": [],
   "source": [
    "# Importar SparkSession: punto de entrada para trabajar con DataFrames en PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar funciones de conveniencia de pyspark.sql.functions\n",
    "# - col: referencia columnas en expresiones\n",
    "# - when, count, length, regexp_replace, upper, trim: funciones √∫tiles para limpieza y validaci√≥n\n",
    "from pyspark.sql.functions import col, when, count, length, regexp_replace, upper, trim\n",
    "\n",
    "# Importar tipos (no usado expl√≠citamente en el script, pero disponible si necesitas esquemas)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Importar requests para descargar archivos desde una URL\n",
    "import requests\n",
    "\n",
    "# -------------------------\n",
    "# Crear sesi√≥n de Spark\n",
    "# -------------------------\n",
    "# .builder: inicia la configuraci√≥n de la SparkSession\n",
    "# .appName(): nombre de la aplicaci√≥n (aparece en UI)\n",
    "# .getOrCreate(): crea la sesi√≥n o devuelve la existente\n",
    "spark = SparkSession.builder.appName(\"Veracidad_Datos\").getOrCreate()\n",
    "\n",
    "# Mensaje informativo para la consola / presentaci√≥n\n",
    "print(\"=== SISTEMA DE VERACIDAD DE DATOS ===\")\n",
    "print(\"Garantizando datos fiables, precisos y sin errores\")\n",
    "\n",
    "# -------------------------\n",
    "# Descargar dataset desde internet\n",
    "# -------------------------\n",
    "print(\"\\n=== DESCARGANDO DATASET DESDE INTERNET ===\")\n",
    "\n",
    "# URL p√∫blica del dataset de aeropuertos (CSV alojado en GitHub)\n",
    "url = \"https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv\"\n",
    "\n",
    "# Nombre de archivo local donde guardaremos el CSV descargado\n",
    "archivo = \"aeropuertos.csv\"\n",
    "\n",
    "# Mensaje indicando la URL que se va a descargar (√∫til para depuraci√≥n)\n",
    "print(f\"Descargando dataset desde: {url}\")\n",
    "\n",
    "# Intento de descarga con manejo b√°sico de excepciones\n",
    "try:\n",
    "    # Realiza la petici√≥n HTTP GET (sin timeout en este ejemplo; en producci√≥n a√±ade timeout)\n",
    "    response = requests.get(url, timeout=30)\n",
    "    # Escribe el contenido de la respuesta en un archivo local (modo texto con encoding UTF-8)\n",
    "    with open(archivo, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    # Mensaje de √©xito\n",
    "    print(\"Dataset descargado exitosamente\")\n",
    "except Exception as e:\n",
    "    # Si ocurre cualquier problema (red, permisos, path), lo mostramos en consola\n",
    "    print(f\"Error descargando: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# 1. DETECCI√ìN DE DATOS CORRUPTOS\n",
    "# -------------------------\n",
    "print(\"\\n=== 1. DETECCI√ìN DE DATOS CORRUPTOS ===\")\n",
    "\n",
    "# Leer el CSV con Spark indicando que la primera fila es cabecera\n",
    "df = spark.read.option(\"header\", True).csv(archivo)\n",
    "\n",
    "# Mostrar estado inicial: n√∫mero de registros (count() fuerza ejecuci√≥n distribuida)\n",
    "print(\"ESTADO INICIAL DEL DATASET:\")\n",
    "print(f\"Total de registros: {df.count()}\")\n",
    "print(f\"Total de columnas: {len(df.columns)}\")\n",
    "\n",
    "# Detectar valores faltantes por columna (isNull detecta solo NULL, no strings vac√≠os)\n",
    "print(\"\\nVALORES FALTANTES POR COLUMNA:\")\n",
    "for col_name in df.columns:\n",
    "    # Cada filter.count() ejecuta una acci√≥n; en datasets grandes puede ser costoso\n",
    "    missing_count = df.filter(col(col_name).isNull()).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"  {col_name}: {missing_count} valores faltantes\")\n",
    "\n",
    "# Detectar duplicados comparando todas las columnas (groupBy(df.columns).count())\n",
    "# Nota: agrupar por todas las columnas puede ser muy costoso si el dataset es grande\n",
    "duplicates = df.groupBy(df.columns).count().filter(col(\"count\") > 1)\n",
    "duplicate_count = duplicates.count()\n",
    "print(f\"\\nDUPLICADOS ENCONTRADOS: {duplicate_count}\")\n",
    "\n",
    "# Si hay duplicados, mostramos ejemplos\n",
    "if duplicate_count > 0:\n",
    "    print(\"Ejemplos de duplicados:\")\n",
    "    duplicates.show(5, truncate=False)\n",
    "\n",
    "# -------------------------\n",
    "# 2. LIMPIEZA Y VALIDACI√ìN DE DATOS\n",
    "# -------------------------\n",
    "print(\"\\n=== 2. LIMPIEZA Y VALIDACI√ìN DE DATOS ===\")\n",
    "\n",
    "# Eliminar filas que no tienen 'ident' o 'name' (clave m√≠nima para identificar aeropuerto)\n",
    "# .na.drop() elimina filas con NULLs en las columnas indicadas\n",
    "df_clean = df.na.drop(subset=[\"ident\", \"name\"])\n",
    "\n",
    "# Validar formato de 'iso_country': filtramos solo c√≥digos de longitud 2 (ej: 'US', 'CO')\n",
    "df_clean = df_clean.filter(length(col(\"iso_country\")) == 2)\n",
    "\n",
    "# Limpiar el nombre: quitar espacios exteriores y convertir a may√∫sculas => columna 'name_clean'\n",
    "df_clean = df_clean.withColumn(\"name_clean\", trim(upper(col(\"name\"))))\n",
    "\n",
    "# Validar coordenadas: mantener solo filas cuya columna 'coordinates' contiene una coma (lon,lat)\n",
    "# Esto evita valores sin formato esperado\n",
    "df_clean = df_clean.filter(col(\"coordinates\").contains(\",\"))\n",
    "\n",
    "# Mostrar n√∫mero de registros tras la limpieza\n",
    "# (recalcular count -> acci√≥n costosa; en produccion considere cache() antes)\n",
    "print(f\"Registros despu√©s de limpieza: {df_clean.count()}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. CONTROL DE CALIDAD (reglas)\n",
    "# -------------------------\n",
    "print(\"\\n=== 3. CONTROL DE CALIDAD ===\")\n",
    "\n",
    "# Preparar lista para almacenar resultados de cada regla de validaci√≥n\n",
    "validation_results = []\n",
    "\n",
    "# Regla 1: Todos los registros limpios deben tener 'ident' no nulo\n",
    "ident_count = df_clean.filter(col(\"ident\").isNotNull()).count()\n",
    "total_count = df_clean.count()\n",
    "validation_results.append((\"Ident √∫nico\", ident_count == total_count, f\"{ident_count}/{total_count}\"))\n",
    "\n",
    "# Regla 2: C√≥digos de pa√≠s v√°lidos (longitud == 2)\n",
    "valid_countries = df_clean.filter(length(col(\"iso_country\")) == 2).count()\n",
    "validation_results.append((\"C√≥digos pa√≠s v√°lidos\", valid_countries == total_count, f\"{valid_countries}/{total_count}\"))\n",
    "\n",
    "# Regla 3: 'type' debe estar dentro de tipos esperados\n",
    "valid_types = [\"large_airport\", \"medium_airport\", \"small_airport\", \"heliport\", \"seaplane_base\"]\n",
    "valid_type_count = df_clean.filter(col(\"type\").isin(valid_types)).count()\n",
    "validation_results.append((\"Tipos v√°lidos\", valid_type_count == total_count, f\"{valid_type_count}/{total_count}\"))\n",
    "\n",
    "# Imprimir los resultados de las validaciones con estado claro\n",
    "print(\"üìã RESULTADOS DE VALIDACI√ìN:\")\n",
    "for rule, passed, details in validation_results:\n",
    "    status = \"PAS√ì\" if passed else \"FALL√ì\"\n",
    "    print(f\"  {rule}: {status} ({details})\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. DETECCI√ìN DE ANOMAL√çAS\n",
    "# -------------------------\n",
    "print(\"\\n=== 4. DETECCI√ìN DE ANOMAL√çAS ===\")\n",
    "\n",
    "# Anomal√≠a 1: contar aeropuertos sin coordenadas (NULL o cadena vac√≠a)\n",
    "sin_coordenadas = df_clean.filter(col(\"coordinates\").isNull() | (col(\"coordinates\") == \"\")).count()\n",
    "print(f\"Aeropuertos sin coordenadas: {sin_coordenadas}\")\n",
    "\n",
    "# Anomal√≠a 2: nombres muy cortos (menos de 3 caracteres) ‚Äî posible dato corrupto o abreviatura\n",
    "nombres_cortos = df_clean.filter(length(col(\"name\")) < 3).count()\n",
    "print(f\"Nombres muy cortos (<3 chars): {nombres_cortos}\")\n",
    "\n",
    "# Anomal√≠a 3: tipos desconocidos (no est√°n en la lista 'valid_types')\n",
    "tipos_raros = df_clean.filter(~col(\"type\").isin(valid_types)).count()\n",
    "print(f\"Tipos de aeropuerto desconocidos: {tipos_raros}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. REPORTE FINAL DE CALIDAD\n",
    "# -------------------------\n",
    "print(\"\\n=== 5. REPORTE FINAL DE CALIDAD ===\")\n",
    "\n",
    "# Calcular puntaje simple de calidad: porcentaje de reglas que pasaron\n",
    "quality_score = sum(1 for _, passed, _ in validation_results if passed) / len(validation_results) * 100\n",
    "print(f\"PUNTAJE DE CALIDAD: {quality_score:.1f}%\")\n",
    "\n",
    "# Interpretaci√≥n textual del puntaje\n",
    "if quality_score >= 90:\n",
    "    print(\"EXCELENTE: Los datos son altamente confiables\")\n",
    "elif quality_score >= 70:\n",
    "    print(\"BUENO: Los datos son confiables con algunas excepciones\")\n",
    "else:\n",
    "    print(\"ADVERTENCIA: Los datos requieren revisi√≥n manual\")\n",
    "\n",
    "# Resumen con n√∫meros absolutos\n",
    "print(f\"\\nRESUMEN FINAL:\")\n",
    "print(f\"  - Registros originales: {df.count()}\")\n",
    "print(f\"  - Registros despu√©s de limpieza: {df_clean.count()}\")\n",
    "print(f\"  - Registros perdidos: {df.count() - df_clean.count()}\")\n",
    "print(f\"  - Porcentaje de retenci√≥n: {(df_clean.count() / df.count() * 100):.1f}%\")\n",
    "\n",
    "# Mostrar una muestra de 5 registros limpios y validados para inspecci√≥n\n",
    "print(\"\\n=== MUESTRA DE DATOS VALIDADOS ===\")\n",
    "df_clean.select(\"ident\", \"name_clean\", \"iso_country\", \"type\", \"coordinates\").show(5, truncate=False)\n",
    "\n",
    "# Cerrar la sesi√≥n de Spark para liberar recursos\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jRSqNriZjwv"
   },
   "source": [
    "5. **Valor**\n",
    "\n",
    "   * No basta con almacenar datos: deben **traducirse en conocimiento √∫til** o en ventajas competitivas.\n",
    "   * Ejemplo: Amazon usa datos de compras para recomendar productos relevantes y aumentar ventas.\n",
    "\n",
    "**Caso empresarial:** usar los datos para priorizar productos m√°s rentables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWTuj7TWaH84"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, when\n",
    "import requests\n",
    "\n",
    "# === 1. INICIO DE SESI√ìN SPARK ===\n",
    "spark = SparkSession.builder.appName(\"Valor_Datos\").getOrCreate()\n",
    "\n",
    "print(\"=== EXTRACCI√ìN DE VALOR DE DATOS ===\")\n",
    "print(\"Transformando datos en conocimiento √∫til\")\n",
    "\n",
    "# === 2. DESCARGA DEL DATASET REAL ===\n",
    "url = \"https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv\"\n",
    "archivo = \"aeropuertos.csv\"\n",
    "\n",
    "print(f\"Descargando desde: {url}\")\n",
    "response = requests.get(url)\n",
    "with open(archivo, 'w', encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "print(\"Dataset descargado\")\n",
    "\n",
    "# === 3. LECTURA DEL DATASET CON SPARK ===\n",
    "df = spark.read.option(\"header\", True).csv(archivo)\n",
    "print(f\"Total aeropuertos: {df.count()}\")   # Contar filas totales\n",
    "\n",
    "# === 4. AN√ÅLISIS ESTRAT√âGICO ===\n",
    "print(\"\\n=== 1. TOP 5 PA√çSES CON M√ÅS AEROPUERTOS ===\")\n",
    "top_countries = (\n",
    "    df.groupBy(\"iso_country\")                # Agrupar por pa√≠s\n",
    "      .agg(count(\"*\").alias(\"total\"))        # Contar aeropuertos por pa√≠s\n",
    "      .orderBy(desc(\"total\"))                # Ordenar de mayor a menor\n",
    "      .limit(5)                              # Quedarse con top 5\n",
    ")\n",
    "top_countries.show(truncate=False)           # Mostrar resultado sin truncar\n",
    "\n",
    "# === 5. SEGMENTACI√ìN DE MERCADO ===\n",
    "print(\"\\n=== 2. DISTRIBUCI√ìN POR TIPO ===\")\n",
    "market_segments = (\n",
    "    df.groupBy(\"type\")                       # Agrupar por tipo de aeropuerto\n",
    "      .agg(count(\"*\").alias(\"cantidad\"))     # Contar por tipo\n",
    "      .orderBy(desc(\"cantidad\"))             # Ordenar descendente\n",
    ")\n",
    "market_segments.show(truncate=False)\n",
    "\n",
    "# === 6. OPORTUNIDADES DE INVERSI√ìN ===\n",
    "print(\"\\n=== 3. PA√çSES CON SOLO AEROPUERTOS PEQUE√ëOS ===\")\n",
    "opportunities = (\n",
    "    df.groupBy(\"iso_country\")                                         # Agrupar por pa√≠s\n",
    "      .agg(\n",
    "          count(\"*\").alias(\"total\"),                                  # Total de aeropuertos\n",
    "          count(when(col(\"type\") == \"small_airport\", True)).alias(\"peque√±os\"),  # Contar solo peque√±os\n",
    "          count(when(col(\"type\") == \"large_airport\", True)).alias(\"grandes\")   # Contar grandes\n",
    "      )\n",
    "      .filter((col(\"grandes\") == 0) & (col(\"total\") > 10))            # Pa√≠ses sin aeropuertos grandes pero con >10 aeropuertos\n",
    "      .orderBy(desc(\"total\"))                                         # Ordenar descendente\n",
    "      .limit(5)                                                       # Limitar a 5\n",
    ")\n",
    "opportunities.show(truncate=False)\n",
    "\n",
    "# === 7. REPORTE FINAL ===\n",
    "print(\"\\n=== REPORTE FINAL ===\")\n",
    "total_airports = df.count()\n",
    "large_airports = df.filter(col(\"type\") == \"large_airport\").count()\n",
    "small_airports = df.filter(col(\"type\") == \"small_airport\").count()\n",
    "\n",
    "print(\"RESUMEN:\")\n",
    "print(f\"  - Aeropuertos grandes: {large_airports} ({large_airports/total_airports*100:.1f}%)\")\n",
    "print(f\"  - Aeropuertos peque√±os: {small_airports} ({small_airports/total_airports*100:.1f}%)\")\n",
    "\n",
    "# C√°lculo simple de valor estrat√©gico ponderado\n",
    "strategic_value = (large_airports * 0.4 + small_airports * 0.1) / total_airports * 100\n",
    "print(f\"VALOR ESTRAT√âGICO: {strategic_value:.1f}%\")\n",
    "\n",
    "# Evaluaci√≥n de resultados\n",
    "if strategic_value >= 20:\n",
    "    print(\"BUENO: Oportunidades claras identificadas\")\n",
    "else:\n",
    "    print(\"MODERADO: Requiere an√°lisis adicional\")\n",
    "\n",
    "print(\"\\nAPLICACIONES:\")\n",
    "print(\"  - Identificar mercados objetivo\")\n",
    "print(\"  - Oportunidades de inversi√≥n\")\n",
    "print(\"  - Estrategias de expansi√≥n\")\n",
    "\n",
    "# === 8. CIERRE DE SESI√ìN ===\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZvbdDM_aIU3"
   },
   "source": [
    "---\n",
    "\n",
    "### **Extensiones frecuentes: 7V**\n",
    "\n",
    "* **Variabilidad:** los datos no siempre tienen un comportamiento constante, pueden cambiar su estructura o significado con el tiempo.\n",
    "* **Visibilidad:** la capacidad de entender qu√© datos son importantes y cu√°les no.\n",
    "\n",
    "---\n",
    "\n",
    "### **Big Data ‚â† Datos grandes**\n",
    "\n",
    "**Big Data implica tres elementos clave:**\n",
    "\n",
    "1. **Procesos:** c√≥mo se capturan, almacenan, procesan y analizan los datos.\n",
    "2. **Tecnolog√≠a:** infraestructura necesaria (Hadoop, Spark, NoSQL, cloud computing).\n",
    "3. **Personas:** especialistas capaces de transformar datos en decisiones (cient√≠ficos de datos, ingenieros de datos, analistas).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtLSlV31VwxT"
   },
   "source": [
    "### 1.2 Casos de uso (ejemplos reales)\n",
    "\n",
    "* **Retail**: recomendaciones personalizadas y *forecasting* de demanda.\n",
    "* **Salud**: anal√≠tica de cohortes y detecci√≥n temprana de eventos adversos.\n",
    "* **Servicios financieros**: *fraud detection* en tiempo casi real.\n",
    "* **Transporte urbano**: an√°lisis de flujos, tiempos de viaje, congesti√≥n y optimizaci√≥n de rutas.\n",
    "* **Industria/IoT**: mantenimiento predictivo (sensores, *streams* de eventos).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I8_gB7jVzcq"
   },
   "source": [
    "### 1.3 Patrones de procesamiento\n",
    "\n",
    "* **Batch** (lotes): grandes vol√∫menes, latencia alta; *data lakes*, ETL/ELT.\n",
    "* **Streaming** (tiempo real o casi real): baja latencia; Kafka + Spark Structured Streaming.\n",
    "* **H√≠bridos**: Lambda/Kappa architectures.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gakub1osW9yp"
   },
   "source": [
    "## 2) **Ecosistema y Arquitectura de Spark**\n",
    "\n",
    "### 2.1 **Piezas del ecosistema (visi√≥n r√°pida)**\n",
    "\n",
    "* **Almacenamiento**:\n",
    "\n",
    "  * **HDFS, S3, Azure Blob, Google Cloud Storage, lagos de datos**: repositorios donde residen datos estructurados y no estructurados.\n",
    "  * **Formatos Parquet/ORC (columnar, comprimido)**: (del ingl√©s Apache Parquet) son un formato de archivo de almacenamiento de datos de c√≥digo abierto, optimizado para el almacenamiento eficiente y el an√°lisis de grandes vol√∫menes de datos en el ecosistema de big data.\n",
    "\n",
    "\n",
    "    **Ejemplo:** una aseguradora almacena historiales de p√≥lizas y reclamaciones en Parquet sobre S3 para reducir costos de almacenamiento y acelerar consultas que analizan el riesgo de clientes.\n",
    "\n",
    "\n",
    "* **Capa de tablas**:\n",
    "\n",
    "  * **Delta Lake, Apache Iceberg, Apache Hudi**: permiten transacciones ACID (Atomicidad, Consistencia, Aislamiento y Durabilidad), control de versiones (*time travel*) y evoluci√≥n de esquemas.\n",
    "\n",
    "\n",
    "    **Ejemplo:** un banco usa Delta Lake para auditar cambios en transacciones hist√≥ricas: si hay una correcci√≥n contable, pueden ‚Äúviajar en el tiempo‚Äù y recuperar el estado de la tabla antes del ajuste.\n",
    "\n",
    "\n",
    "* **C√≥mputo**:\n",
    "\n",
    "  * **Apache Spark (Batch/SQL/ML/Streaming)**: procesamiento masivo en lotes o en tiempo real.\n",
    "  * **Flink (event-driven)**: orientado a flujos continuos y baja latencia.\n",
    "  * **Trino/Presto (consulta interactiva)**: consultas ad-hoc r√°pidas sobre diferentes fuentes.\n",
    "\n",
    "\n",
    "    **Ejemplo:** un retailer global usa Spark Streaming para detectar picos de demanda en tiempo real y ajustar inventario; mientras su equipo de BI usa Trino para consultas r√°pidas sin tener que mover datos.\n",
    "\n",
    "\n",
    "* **Orquestaci√≥n**:\n",
    "\n",
    "  * **Airflow, Dagster**: coordinan y programan pipelines complejos.\n",
    "\n",
    "  \n",
    "    **Ejemplo:** una empresa de log√≠stica usa Airflow para automatizar procesos diarios: extraer datos de sensores de camiones, procesarlos con Spark y generar dashboards de eficiencia de rutas antes de las 7 am.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llNOmCCpXNJK"
   },
   "source": [
    "### 2.2 **Arquitectura de Spark**\n",
    "\n",
    "* **Driver y Executors**:\n",
    "\n",
    "  * El **Driver** coordina, planifica y crea el DAG de ejecuci√≥n ((Gr√°fico Ac√≠clico Dirigido) es un gr√°fico que representa un flujo de trabajo o pipeline de datos de manera visual); los **Executors** procesan los datos.\n",
    "\n",
    "    **Ejemplo:** en una empresa de e-commerce, el driver env√≠a tareas de limpieza y agregaci√≥n de datos de usuarios hacia m√∫ltiples executors que corren en un cl√∫ster Kubernetes para calcular m√©tricas de conversi√≥n.\n",
    "\n",
    "\n",
    "* **Lazy evaluation**:\n",
    "\n",
    "  * Las transformaciones no ejecutan de inmediato; Spark construye un DAG y solo realiza el c√≥mputo cuando ocurre una **acci√≥n**.\n",
    "\n",
    "    **Ejemplo:** una fintech aplica transformaciones (`filter`, `join`) sobre millones de transacciones fraudulentas potenciales, pero el c√°lculo real solo ocurre cuando llaman `count()` para saber cu√°ntos casos sospechosos hay.\n",
    "\n",
    "\n",
    "* **DataFrame API (Catalyst + Tungsten)**:\n",
    "\n",
    "  * Preferida sobre RDD (conjunto de datos distribuidos resilientes (RDD): Colecci√≥n inmutable y tolerante a fallos de elementos que se pueden distribuir en varios nodos de cl√∫ster para procesarlos en paralelo) porque aplica optimizaciones autom√°ticas de consultas y memoria.\n",
    "\n",
    "    **Ejemplo:** una telco usa Spark SQL para resumir registros de llamadas usando DataFrames en lugar de RDD, reduciendo el tiempo de procesamiento de horas a minutos.\n",
    "\n",
    "\n",
    "* **Transformaciones vs Acciones**:\n",
    "\n",
    "  * **Transformaciones**: `select`, `filter`, `withColumn` (crean un nuevo plan).\n",
    "  * **Acciones**: `count`, `collect`, `write` (disparan la ejecuci√≥n real).\n",
    "  \n",
    "    **Ejemplo:** una aerol√≠nea transforma datos de reservas para calcular tarifas promedio (`withColumn` y `groupBy`), pero no procesa nada hasta que escribe el resultado (`write`) en una tabla anal√≠tica.\n",
    "\n",
    "\n",
    "* **Dependencias *narrow* vs *wide***:\n",
    "\n",
    "  * **Narrow**: cada partici√≥n de salida depende de una sola partici√≥n de entrada (r√°pido, sin *shuffle*).\n",
    "  * **Wide**: requieren reorganizaci√≥n (*shuffle*) de datos entre nodos (m√°s costoso).\n",
    "    **Ejemplo:** una plataforma de streaming aplica un `map` (narrow) para limpiar registros de usuarios, pero al hacer un `groupBy` por regi√≥n (wide) Spark debe redistribuir datos por toda la red, aumentando el tiempo de ejecuci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso a paso para configurar S3 y Spark**\n",
    "\n",
    "### **Paso 1. Crear y configurar tu cuenta de AWS**\n",
    "\n",
    "1. Ve a [https://aws.amazon.com](https://aws.amazon.com) y crea una cuenta (si no tienes).\n",
    "\n",
    "2. Entra a la **Consola de AWS**.\n",
    "\n",
    "3. Ve al servicio **S3** ‚Üí crea un bucket:\n",
    "\n",
    "   * Nombre: `aseguradora` (debe ser √∫nico en todo AWS).\n",
    "   * Regi√≥n: elige una cercana (ej. `us-east-1`).\n",
    "   * Desactiva ‚ÄúBloquear todo el acceso p√∫blico‚Äù solo si necesitas probar sin restricciones (en producci√≥n, mantenlo seguro).\n",
    "   * Crea el bucket.\n",
    "\n",
    "4. **Sube un archivo Parquet**:\n",
    "\n",
    "   * Dentro del bucket, crea la carpeta `polizas/2025/`.\n",
    "   * Sube un archivo `*.parquet` dentro.\n",
    "\n",
    "---\n",
    "\n",
    "### **Paso 2. Configurar credenciales de AWS en tu PC**\n",
    "\n",
    "1. Instala la CLI de AWS (si no la tienes):\n",
    "\n",
    "   ```bash\n",
    "   pip install awscli\n",
    "   ```\n",
    "2. Configura tus credenciales:\n",
    "\n",
    "   ```bash\n",
    "   aws configure\n",
    "   ```\n",
    "\n",
    "   * AWS Access Key ID ‚Üí (copiar desde IAM)\n",
    "   * AWS Secret Access Key ‚Üí (copiar desde IAM)\n",
    "   * Default region ‚Üí `us-east-1` (o la regi√≥n de tu bucket)\n",
    "   * Default output format ‚Üí `json`\n",
    "\n",
    "> Esto guardar√° credenciales en `~/.aws/credentials`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Paso 3. Instalar PySpark y dependencias para S3**\n",
    "\n",
    "Necesitas PySpark con el conector de Hadoop para S3:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "pip install boto3\n",
    "```\n",
    "\n",
    "> Spark usar√° Hadoop para acceder a S3. Con PySpark >= 3.0 ya incluye soporte para `s3a://` y `s3://`, pero a veces hay que forzarlo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Paso 4. Configurar Spark para usar S3**\n",
    "\n",
    "En tu script o notebook, debes pasar credenciales a Spark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadFromS3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"TU_ACCESS_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"TU_SECRET_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lectura de datos\n",
    "df_policies = spark.read.parquet(\"s3a://aseguradora/polizas/2025/\")\n",
    "df_policies.printSchema()\n",
    "```\n",
    "\n",
    "> **Importante:**\n",
    ">\n",
    "> * Usa `s3a://` en lugar de `s3://` para evitar errores de compatibilidad.\n",
    "> * Nunca dejes las llaves reales en c√≥digo p√∫blico: en producci√≥n, Spark las toma autom√°ticamente de `~/.aws/credentials`.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Verificaci√≥n paso a paso**\n",
    "\n",
    "1. Verifica que puedes listar el bucket:\n",
    "\n",
    "   ```bash\n",
    "   aws s3 ls s3://aseguradora/polizas/2025/\n",
    "   ```\n",
    "\n",
    "   Si ves el archivo ‚Üí tu CLI funciona bien.\n",
    "\n",
    "2. Ejecuta el script PySpark. Si todo est√° bien, Spark debe imprimir el esquema de tu Parquet.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Errores comunes y soluciones**\n",
    "\n",
    "* **`No FileSystem for scheme: s3`** ‚Üí te falta el conector Hadoop S3 (usa PySpark >= 3.0 o agrega `hadoop-aws` jar).\n",
    "* **`Access Denied`** ‚Üí revisa credenciales o permisos IAM (necesitas `s3:GetObject`).\n",
    "* **`File not found`** ‚Üí revisa la ruta exacta del archivo/carpeta.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJERCICIO COMPLETO: Ecosistema y Arquitectura de Spark (PySpark)\n",
    "================================================================\n",
    "\n",
    "## Ecosistema y Arquitectura de Spark\n",
    "-------------------------------------\n",
    "2.1 Piezas del ecosistema (visi√≥n r√°pida)\n",
    "- Almacenamiento: HDFS, S3, Azure Blob, Google Cloud Storage, lagos de datos. Usamos Parquet/ORC (columnar, comprimido).\n",
    "- Capa de tablas: Delta Lake, Apache Iceberg, Apache Hudi (ACID, time travel, evoluci√≥n de esquemas). Aqu√≠ usamos Delta si est√° disponible.\n",
    "- C√≥mputo: Apache Spark (Batch/SQL/ML/Streaming), Flink (event-driven), Trino/Presto (consulta interactiva).\n",
    "- Orquestaci√≥n: Airflow, Dagster (coordinan pipelines). Incluimos un DAG de Airflow de ejemplo al final (comentado).\n",
    "\n",
    "2.2 Arquitectura de Spark\n",
    "- Driver y Executors: el Driver coordina y crea el DAG; los Executors procesan los datos en el cl√∫ster.\n",
    "- Lazy evaluation: transformaciones construyen el plan; las acciones disparan la ejecuci√≥n.\n",
    "- DataFrame API (Catalyst + Tungsten): optimizaci√≥n autom√°tica; preferible a RDD.\n",
    "- Transformaciones vs Acciones: select/filter/withColumn (plan), count/collect/write (ejecuci√≥n).\n",
    "- Dependencias narrow vs wide: narrow (sin shuffle), wide (con shuffle).\n",
    "\n",
    "PASO A PASO S3\n",
    "- Incluye gu√≠a para configurar AWS CLI, credenciales, y c√≥mo leer s3a://...\n",
    "\n",
    "MODO DE USO\n",
    "-----------\n",
    "1) LOCAL (recomendado para probar ya):\n",
    "   - Requisitos: Python 3.9+, Java 8/11, Apache Spark 3.x instalado y en PATH.\n",
    "   - (Opcional) Delta Lake: `pip install delta-spark`\n",
    "   - Ejecuta:\n",
    "       spark-submit ejercicio_spark_completo.py\n",
    "\n",
    "2) S3 (opcional):\n",
    "   - Configura AWS CLI: `pip install awscli && aws configure`\n",
    "   - Crea bucket y sube Parquet como se detalla en el README/teor√≠a.\n",
    "   - En este script, establece USE_S3 = True y ajusta credenciales/paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import date\n",
    "from typing import Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# ======================================================\n",
    "# Par√°metros del ejercicio\n",
    "# ======================================================\n",
    "\n",
    "# Cambiar a True si deseas leer desde S3 (requiere credenciales y datos existentes)\n",
    "USE_S3 = False\n",
    "\n",
    "# Rutas LOCAL\n",
    "BASE_DIR = os.path.abspath(os.path.dirname(__file__))\n",
    "LOCAL_PARQUET_DIR = os.path.join(BASE_DIR, \"data\", \"parquet\", \"polizas\", \"2025\")\n",
    "LOCAL_DELTA_DIR   = os.path.join(BASE_DIR, \"data\", \"delta\", \"polizas\")\n",
    "\n",
    "# Rutas S3 (si USE_S3=True)\n",
    "S3_INPUT = \"s3a://aseguradora/polizas/2025/\"\n",
    "S3_DELTA = \"s3a://aseguradora/delta/polizas/\"  # si deseas escribir Delta en S3\n",
    "\n",
    "# Credenciales S3 (solo si no usas ~/.aws/credentials). No pongas llaves reales en c√≥digo p√∫blico.\n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"TU_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"TU_SECRET_KEY\")\n",
    "AWS_ENDPOINT   = os.environ.get(\"AWS_ENDPOINT\", \"s3.amazonaws.com\")\n",
    "\n",
    "\n",
    "def build_spark() -> Tuple[SparkSession, bool]:\n",
    "    \"\"\"\n",
    "    Crea la SparkSession. Si delta-spark est√° instalado, habilita Delta Lake (capa de tablas ACID + time travel).\n",
    "    Devuelve (spark, delta_enabled).\n",
    "    \"\"\"\n",
    "    delta_enabled = False\n",
    "\n",
    "    # Builder base\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"EjercicioSpark_Completo\")\n",
    "    )\n",
    "\n",
    "    if USE_S3:\n",
    "        # ----------------------------\n",
    "        # ECOSISTEMA ‚Üí Almacenamiento en S3 (HDFS/Blob/GCS alternativos)\n",
    "        # ----------------------------\n",
    "        # Preferir s3a:// para compatibilidad.\n",
    "        builder = (\n",
    "            builder\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT)\n",
    "        )\n",
    "\n",
    "    # Intentar habilitar Delta Lake si est√° disponible\n",
    "    try:\n",
    "        # delta-spark permite configurar f√°cilmente la sesi√≥n con los JARs de Delta\n",
    "        from delta import configure_spark_with_delta_pip\n",
    "        builder = (\n",
    "            builder\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        )\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "        delta_enabled = True\n",
    "    except Exception:\n",
    "        # Si no hay delta-spark, seguimos con Spark \"vanilla\"\n",
    "        spark = builder.getOrCreate()\n",
    "\n",
    "    # Ajustes √∫tiles para el ejercicio\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "    return spark, delta_enabled\n",
    "\n",
    "\n",
    "def ensure_local_dirs():\n",
    "    os.makedirs(LOCAL_PARQUET_DIR, exist_ok=True)\n",
    "    os.makedirs(LOCAL_DELTA_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def create_sample_data(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame de p√≥lizas sint√©tico para LOCAL mode.\n",
    "    (ECOSISTEMA - Almacenamiento ‚Üí Parquet; FORMATO columnar): generaremos y guardaremos en Parquet.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"poliza_id\",   StringType(), False),\n",
    "        StructField(\"cliente_id\",  StringType(), False),\n",
    "        StructField(\"region\",      StringType(), False),\n",
    "        StructField(\"monto\",       DoubleType(), False),\n",
    "        StructField(\"prima\",       DoubleType(), False),\n",
    "        StructField(\"estado\",      StringType(), False),\n",
    "        StructField(\"fecha_alta\",  DateType(),   False),\n",
    "    ])\n",
    "\n",
    "    rows = [\n",
    "        (\"P-0001\", \"C-100\", \"Norte\",   950.0,  95.0,  \"activa\",  date(2025, 1, 10)),\n",
    "        (\"P-0002\", \"C-101\", \"Norte\",  1200.0, 120.0,  \"activa\",  date(2025, 2,  5)),\n",
    "        (\"P-0003\", \"C-102\", \"Sur\",     800.0,  80.0,  \"vencida\", date(2025, 2, 18)),\n",
    "        (\"P-0004\", \"C-103\", \"Centro\", 2200.0, 220.0,  \"activa\",  date(2025, 3,  7)),\n",
    "        (\"P-0005\", \"C-104\", \"Sur\",    1800.0, 180.0,  \"activa\",  date(2025, 3, 15)),\n",
    "        (\"P-0006\", \"C-105\", \"Norte\",  3100.0, 310.0,  \"activa\",  date(2025, 3, 20)),\n",
    "        (\"P-0007\", \"C-106\", \"Centro\",  500.0,  50.0,  \"vencida\", date(2025, 3, 25)),\n",
    "        (\"P-0008\", \"C-107\", \"Sur\",    1300.0, 130.0,  \"activa\",  date(2025, 4,  2)),\n",
    "        (\"P-0009\", \"C-108\", \"Norte\",  2600.0, 260.0,  \"activa\",  date(2025, 4, 12)),\n",
    "        (\"P-0010\", \"C-109\", \"Centro\",  950.0,  95.0,  \"activa\",  date(2025, 4, 22)),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "    # Guardar en Parquet (FORMATO columnar comprimido)\n",
    "    df.write.mode(\"overwrite\").parquet(LOCAL_PARQUET_DIR)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_input(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Lee datos desde Parquet LOCAL o desde S3 (seg√∫n USE_S3).\n",
    "    \"\"\"\n",
    "    if USE_S3:\n",
    "        print(f\"üîπ Leyendo desde S3 (Parquet): {S3_INPUT}\")\n",
    "        return spark.read.parquet(S3_INPUT)\n",
    "    else:\n",
    "        print(f\"üîπ Leyendo desde LOCAL (Parquet): {LOCAL_PARQUET_DIR}\")\n",
    "        return spark.read.parquet(LOCAL_PARQUET_DIR)\n",
    "\n",
    "\n",
    "def run_core_exercise(spark: SparkSession, df):\n",
    "    \"\"\"\n",
    "    Ejecuta el ejercicio central cubriendo 2.2 Arquitectura:\n",
    "    - Lazy evaluation (transformaciones no ejecutan hasta acci√≥n)\n",
    "    - DataFrame API (Catalyst+Tungsten)\n",
    "    - Transformaciones vs Acciones\n",
    "    - Narrow vs Wide (sin y con shuffle)\n",
    "    - SQL, cach√©, particionado\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 2.2 ARQUITECTURA DE SPARK ===\")\n",
    "    print(\"Driver (este programa) construye el DAG; Executors procesan los datos al ejecutar acciones.\")\n",
    "\n",
    "    print(\"\\n-- Esquema de entrada --\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # Lazy evaluation: transformaciones (no ejecuta)\n",
    "    print(\"\\n-- Transformaciones (lazy) --\")\n",
    "    df_trans = (df\n",
    "                .filter(F.col(\"monto\") > 1000)                 # narrow\n",
    "                .withColumnRenamed(\"cliente_id\", \"id_cliente\") # narrow\n",
    "                .withColumn(\"ratio_prima\", F.col(\"prima\")/F.col(\"monto\")))\n",
    "\n",
    "    # Catalyst/Tungsten: el optimizador y motor f√≠sico actuar√°n al ejecutar acciones.\n",
    "    print(\"\\n-- Plan l√≥gico/f√≠sico (explain) --\")\n",
    "    df_trans.explain(mode=\"formatted\")\n",
    "\n",
    "    # Acci√≥n: count() dispara ejecuci√≥n\n",
    "    print(\"\\n-- Acci√≥n: count() --\")\n",
    "    n = df_trans.count()\n",
    "    print(f\"Registros con monto > 1000: {n}\")\n",
    "\n",
    "    # Narrow vs Wide: groupBy ‚Üí shuffle (wide)\n",
    "    print(\"\\n-- Wide dependency: groupBy (shuffle) --\")\n",
    "    df_grp = df_trans.groupBy(\"region\").agg(F.count(\"*\").alias(\"polizas\"),\n",
    "                                            F.avg(\"monto\").alias(\"monto_prom\"),\n",
    "                                            F.avg(\"prima\").alias(\"prima_prom\"))\n",
    "\n",
    "    print(\"\\n-- Acci√≥n: collect() --\")\n",
    "    for row in df_grp.collect():\n",
    "        print(row)\n",
    "\n",
    "    # SQL vs DataFrame API\n",
    "    print(\"\\n-- Spark SQL --\")\n",
    "    df_trans.createOrReplaceTempView(\"polizas_filtradas\")\n",
    "    sql_res = spark.sql(\"\"\"\n",
    "        SELECT region,\n",
    "               COUNT(*)  AS polizas,\n",
    "               ROUND(AVG(monto), 2) AS monto_prom,\n",
    "               ROUND(AVG(prima), 2) AS prima_prom\n",
    "        FROM polizas_filtradas\n",
    "        GROUP BY region\n",
    "        ORDER BY polizas DESC\n",
    "    \"\"\")\n",
    "    sql_res.show(truncate=False)\n",
    "\n",
    "    # Cach√© (√∫til cuando reusaremos el mismo resultado varias veces)\n",
    "    print(\"\\n-- Cache / Persist --\")\n",
    "    df_cached = df_trans.cache()\n",
    "    print(\"Cached count:\", df_cached.count())\n",
    "\n",
    "    # Particionado: coalesce (narrow) vs repartition (wide - shuffle)\n",
    "    print(\"\\n-- Particionado --\")\n",
    "    print(\"Particiones actuales:\", df_cached.rdd.getNumPartitions())\n",
    "    df_repart = df_cached.repartition(4, F.col(\"region\"))  # wide (shuffle por 'region')\n",
    "    print(\"Particiones tras repartition(4, 'region'):\", df_repart.rdd.getNumPartitions())\n",
    "\n",
    "    # Escritura particionada en Parquet (ECOSISTEMA - Almacenamiento)\n",
    "    out_part_dir = os.path.join(os.path.dirname(LOCAL_PARQUET_DIR), \"out_particionado\")\n",
    "    df_repart.write.mode(\"overwrite\").partitionBy(\"region\").parquet(out_part_dir)\n",
    "    print(f\"Escritura particionada en Parquet ‚Üí {out_part_dir}\")\n",
    "\n",
    "    return df_trans\n",
    "\n",
    "\n",
    "def try_delta_table(spark: SparkSession, df_trans, delta_enabled: bool):\n",
    "    \"\"\"\n",
    "    Capa de tablas: Delta Lake (si est√° disponible).\n",
    "    - ACID, Time Travel (versionAsOf), evoluci√≥n de esquema.\n",
    "    - Demostraci√≥n: escribimos Delta, hacemos una actualizaci√≥n (UPDATE), consultamos estado anterior (time travel).\n",
    "    \"\"\"\n",
    "    if not delta_enabled:\n",
    "        print(\"\\n(Delta Lake NO disponible; instala con `pip install delta-spark` para habilitar esta secci√≥n).\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Capa de Tablas: Delta Lake (ACID + Time Travel) ===\")\n",
    "    target_path = LOCAL_DELTA_DIR if not USE_S3 else S3_DELTA\n",
    "\n",
    "    # Guardar como Delta\n",
    "    (df_trans\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(target_path))\n",
    "    print(f\"Delta escrito en: {target_path}\")\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "    dtab = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "    # UPDATE (ACID): subir la prima un 10% a p√≥lizas de regi√≥n 'Norte'\n",
    "    print(\"-- UPDATE en Delta (ACID) --\")\n",
    "    dtab.update(\n",
    "        condition=F.expr(\"region = 'Norte'\"),\n",
    "        set={\"prima\": F.expr(\"prima * 1.10\")}\n",
    "    )\n",
    "\n",
    "    # LEER estado actual\n",
    "    print(\"-- Leer Delta (estado actual) --\")\n",
    "    spark.read.format(\"delta\").load(target_path).groupBy(\"region\").agg(\n",
    "        F.round(F.avg(\"prima\"), 2).alias(\"prima_prom\")\n",
    "    ).show()\n",
    "\n",
    "    # TIME TRAVEL: leer versi√≥n anterior (antes del UPDATE)\n",
    "    print(\"-- Time Travel (versionAsOf=0) --\")\n",
    "    spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(target_path).groupBy(\"region\").agg(\n",
    "        F.round(F.avg(\"prima\"), 2).alias(\"prima_prom_v0\")\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def demo_streaming(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    C√≥mputo en tiempo real (Streaming): usamos la fuente 'rate' para simular eventos.\n",
    "    - Esto ilustra Spark Streaming (en el ecosistema, retailer detecta picos en tiempo real).\n",
    "    - Usamos trigger(once=True) para un solo micro-batch y no quedar en ejecuci√≥n indefinida.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Streaming: Structured Streaming con fuente 'rate' ===\")\n",
    "    rate_df = (spark.readStream\n",
    "                    .format(\"rate\")\n",
    "                    .option(\"rowsPerSecond\", 10)\n",
    "                    .load())\n",
    "\n",
    "    # Transformaciones en streaming (lazy)\n",
    "    stream_trans = (rate_df\n",
    "                    .withColumn(\"region\",\n",
    "                                F.when((F.col(\"value\") % 3) == 0, F.lit(\"Norte\"))\n",
    "                                 .when((F.col(\"value\") % 3) == 1, F.lit(\"Sur\"))\n",
    "                                 .otherwise(F.lit(\"Centro\")))\n",
    "                    .groupBy(\"region\")\n",
    "                    .count())  # wide\n",
    "\n",
    "    # Escribimos a memoria para poder consultar luego con SQL\n",
    "    query = (stream_trans.writeStream\n",
    "             .format(\"memory\")\n",
    "             .queryName(\"demanda_regiones\")\n",
    "             .outputMode(\"complete\")\n",
    "             .trigger(once=True)   # ejecuta un micro-batch y termina\n",
    "             .start())\n",
    "\n",
    "    query.awaitTermination()\n",
    "    print(\"-- Resultados en memoria (SQL sobre streaming) --\")\n",
    "    spark.sql(\"SELECT * FROM demanda_regiones ORDER BY count DESC\").show()\n",
    "\n",
    "\n",
    "def airflow_example():\n",
    "    \"\"\"\n",
    "    ORQUESTACI√ìN (Airflow): DAG de ejemplo (comentado) que ejecuta este script con SparkSubmitOperator.\n",
    "\n",
    "    # Guarda como: dags/pipeline_polizas.py\n",
    "    ----------------------------------------------------\n",
    "    from airflow import DAG\n",
    "    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "    from datetime import datetime\n",
    "\n",
    "    with DAG(\n",
    "        dag_id=\"pipeline_polizas\",\n",
    "        start_date=datetime(2025, 8, 1),\n",
    "        schedule_interval=\"0 6 * * *\",  # todos los d√≠as 06:00\n",
    "        catchup=False,\n",
    "        default_args={\"owner\": \"data-eng\"}\n",
    "    ) as dag:\n",
    "\n",
    "        tarea_spark = SparkSubmitOperator(\n",
    "            task_id=\"procesar_polizas\",\n",
    "            application=\"/path/a/ejercicio_spark_completo.py\",\n",
    "            conn_id=\"spark_default\",\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        tarea_spark\n",
    "    ----------------------------------------------------\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def s3_howto_notes():\n",
    "    \"\"\"\n",
    "    PASO A PASO S3 (resumen pr√°ctico dentro del script):\n",
    "    1) Crea cuenta en AWS y configura CLI:\n",
    "        pip install awscli\n",
    "        aws configure\n",
    "    2) Crea bucket 'aseguradora' y sube Parquet a s3://aseguradora/polizas/2025/\n",
    "    3) En este script, pon USE_S3=True y ajusta claves o usa ~/.aws/credentials\n",
    "    4) Usa rutas s3a://... Ej.: spark.read.parquet(\"s3a://aseguradora/polizas/2025/\")\n",
    "    5) Errores comunes:\n",
    "       - No FileSystem for scheme: s3 ‚Üí falta hadoop-aws/versi√≥n.\n",
    "       - Access Denied ‚Üí permisos IAM (s3:GetObject).\n",
    "       - File not found ‚Üí ruta/bucket incorrectos.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not USE_S3:\n",
    "        ensure_local_dirs()\n",
    "\n",
    "    spark, delta_enabled = build_spark()\n",
    "\n",
    "    try:\n",
    "        # En LOCAL creamos datos de ejemplo ‚Üí guardamos en Parquet\n",
    "        if not USE_S3:\n",
    "            create_sample_data(spark)\n",
    "\n",
    "        # Leemos la \"capa de almacenamiento\": Parquet local o S3\n",
    "        df_in = read_input(spark)\n",
    "\n",
    "        # N√∫cleo del ejercicio (Arquitectura: lazy, acciones, narrow/wide, SQL, cach√©, particionado)\n",
    "        df_trans = run_core_exercise(spark, df_in)\n",
    "\n",
    "        # Capa de tablas: Delta Lake (ACID + Time Travel), si est√° disponible\n",
    "        try_delta_table(spark, df_trans, delta_enabled)\n",
    "\n",
    "        # C√≥mputo en tiempo real: mini demo de Streaming (rate source)\n",
    "        demo_streaming(spark)\n",
    "\n",
    "        print(\"\\n‚úÖ EJERCICIO COMPLETO FINALIZADO CORRECTAMENTE.\")\n",
    "        print(\"   - Ecosistema (almacenamiento, tabla/Delta, c√≥mputo, orquestaci√≥n).\")\n",
    "        print(\"   - Arquitectura (Driver/Executors, lazy, Catalyst/Tungsten, transformaciones vs acciones, narrow/wide).\")\n",
    "        print(\"   - SQL, cach√©, particionado, Streaming, Delta (si disponible).\")\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFqn9evD7wAueE07OVI5/+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
