{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAampK_0VLtL"
   },
   "source": [
    "# Clase 1: Introducción a Big Data y PySpark\n",
    "\n",
    "**Rol del docente:** Profesor de Big Data.\n",
    "\n",
    "**Objetivo general:** Al finalizar la sesión, el estudiante comprende los fundamentos de Big Data, la arquitectura básica de Apache Spark y es capaz de ejecutar un análisis inicial de un *dataset* grande con PySpark aplicando buenas prácticas de ingeniería, calidad de datos y estándares.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8akMMcebVQII"
   },
   "source": [
    "## **1) Fundamentos de Big Data**\n",
    "\n",
    "### **1.1 ¿Qué es Big Data?**\n",
    "\n",
    "Big Data **no se trata únicamente de tener “muchos datos”**, sino de la **capacidad de procesarlos, analizarlos y generar valor a partir de ellos** usando tecnologías avanzadas y equipos humanos especializados. Veamos sus características fundamentales y por qué es un concepto mucho más profundo que simplemente “datos grandes”.\n",
    "\n",
    "---\n",
    "\n",
    "### **Las 5V (y extensiones 7V)**\n",
    "\n",
    "1. **Volumen**\n",
    "\n",
    "   * Se refiere a la **cantidad masiva de datos** que generan las empresas, dispositivos IoT, redes sociales, sensores, transacciones, etc.\n",
    "   * Ejemplo: Facebook procesa más de *4 petabytes* de datos diarios.\n",
    "\n",
    "**Caso real:** Lectura del dataset de “Yellow Taxi Trip Records” de Nueva York (disponible públicamente en Amazon S3), que contiene millones de viajes de taxi por mes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalar libreria\n",
    "\n",
    "```\n",
    "pip install pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEH6Ev8qYE_p"
   },
   "outputs": [],
   "source": [
    "# PySpark: Framework de procesamiento distribuido para grandes volúmenes de datos\n",
    "# (Este script descarga y analiza un parquet público de NYC Taxi de forma local)\n",
    "\n",
    "# Importar SparkSession: punto de entrada para crear DataFrames y ejecutar operaciones Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar requests para descargar archivos vía HTTP(S)\n",
    "import requests\n",
    "\n",
    "# Importar warnings para suprimir advertencias no relevantes en la salida\n",
    "import warnings\n",
    "\n",
    "# Suprimir advertencias de la librería warnings para mantener la salida limpia\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------\n",
    "# 1. Crear la sesión de Spark\n",
    "# -------------------------\n",
    "# Iniciamos el \"builder\" para configurar la SparkSession\n",
    "# .appName() define el nombre que verá en la UI de Spark\n",
    "# .config(...) añade configuraciones específicas (aquí ejemplos de ajustes)\n",
    "# .getOrCreate() crea la sesión si no existe o devuelve la existente\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigData_Volumen\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ajustar el nivel de logs del SparkContext para evitar ver mucho detalle (INFO/DEBUG)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 2. Definir URL y ruta local para descargar dataset\n",
    "# URL pública hacia el archivo Parquet (CloudFront público que sirve datos de NYC taxi)\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "# Nombre del archivo local donde lo guardaremos\n",
    "local_path = \"yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "# 2b. Descargar dataset (manejo de errores básico)\n",
    "try:\n",
    "    # Mensaje informativo para el usuario/alumno\n",
    "    print(\"Descargando dataset...\")\n",
    "    # Realiza la petición HTTP con un timeout para no colgar indefinidamente\n",
    "    response = requests.get(url, timeout=30)\n",
    "    # Abrir el archivo local en modo binario y escribir el contenido descargado\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # Mensaje de éxito\n",
    "    print(\"Descarga completada\")\n",
    "except Exception as e:\n",
    "    # Si ocurre cualquier error de red o IO, lo mostramos (podrías manejarlo más finamente)\n",
    "    print(f\"Error en descarga: {e}\")\n",
    "\n",
    "# 3. Leer el archivo Parquet con Spark\n",
    "# read.parquet() es la forma nativa y más eficiente de leer Parquet en Spark\n",
    "df = spark.read.parquet(local_path)\n",
    "\n",
    "# 4. Inspección básica del dataset\n",
    "print(\"INFORMACIÓN DEL DATASET NYC TAXI\")\n",
    "\n",
    "# Contar el número total de registros (count() es una acción que dispara ejecución)\n",
    "total_records = df.count()\n",
    "# Imprimir el número total con formato de miles\n",
    "print(f\"\\nTOTAL DE REGISTROS: {total_records:,}\")\n",
    "\n",
    "# Mostrar sólo las primeras 3 filas con algunas columnas relevantes, sin truncamiento\n",
    "print(\"\\nPRIMERAS 3 FILAS:\")\n",
    "df.select(\"VendorID\", \"tpep_pickup_datetime\", \"fare_amount\", \"tip_amount\", \"total_amount\") \\\n",
    "  .show(3, truncate=False)\n",
    "\n",
    "# 4b. Mostrar esquema de forma ordenada (campo → tipo)\n",
    "print(\"\\nESQUEMA DEL DATASET:\")\n",
    "# Creamos un diccionario para ordenar/mostrar el esquema más legible\n",
    "schema_dict = {}\n",
    "for field in df.schema.fields:\n",
    "    # field.name es el nombre de la columna; field.dataType es su tipo (p. ej., TimestampType, DoubleType)\n",
    "    schema_dict[field.name] = str(field.dataType)\n",
    "\n",
    "# Imprimir el esquema enumerado y con alineación para que sea fácil de leer en clase\n",
    "for i, (field_name, field_type) in enumerate(schema_dict.items(), 1):\n",
    "    print(f\"  {i:2d}. {field_name:<25} → {field_type}\")\n",
    "\n",
    "# 5. Análisis estadístico básico (tarifas y propinas)\n",
    "print(\"\\nANÁLISIS ESTADÍSTICO BÁSICO\")\n",
    "\n",
    "# Columnas numéricas de interés para el ejemplo\n",
    "numeric_cols = ['fare_amount', 'tip_amount', 'total_amount', 'trip_distance']\n",
    "\n",
    "# Iteramos sobre las columnas numéricas y mostramos estadísticas resumidas\n",
    "for col in numeric_cols:\n",
    "    # Comprobar que la columna exista en el DataFrame (por compatibilidad entre versiones de dataset)\n",
    "    if col in df.columns:\n",
    "        # summary() produce métricas: count, mean, stddev, min, max (entre otras si se pide)\n",
    "        # collect() trae las filas resultantes al driver como lista de Rows\n",
    "        stats = df.select(col).summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\").collect()\n",
    "        # Si stats no está vacío procedemos a extraer los valores\n",
    "        if stats:\n",
    "            # Cada entrada de stats es una fila; accedemos por índice y luego por nombre de columna\n",
    "            count = stats[0][col]    # 'count' viene como string (por eso mostramos tal cual)\n",
    "            mean = stats[1][col]     # 'mean' normalmente es string convertible a float\n",
    "            std = stats[2][col]      # 'stddev' como string\n",
    "            min_val = stats[3][col]  # 'min'\n",
    "            max_val = stats[4][col]  # 'max'\n",
    "            \n",
    "            # Imprimir las estadísticas con formato legible (conversión a float para formateo)\n",
    "            print(f\"\\n  {col.upper()}:\")\n",
    "            print(f\"    • Total: {count}\")\n",
    "            # Guardamos un manejo seguro al convertir a float (siempre que no sea 'null' o cadena vacía)\n",
    "            try:\n",
    "                print(f\"    • Promedio: ${float(mean):.2f}\")\n",
    "            except Exception:\n",
    "                print(f\"    • Promedio: {mean} (no convertible a float)\")\n",
    "            try:\n",
    "                print(f\"    • Desviación: ${float(std):.2f}\")\n",
    "            except Exception:\n",
    "                print(f\"    • Desviación: {std} (no convertible a float)\")\n",
    "            # Min / Max también formateados si son convertibles\n",
    "            try:\n",
    "                print(f\"    • Rango: ${float(min_val):.2f} - ${float(max_val):.2f}\")\n",
    "            except Exception:\n",
    "                print(f\"    • Rango: {min_val} - {max_val}\")\n",
    "\n",
    "# 6. Cerrar la sesión de Spark (liberar recursos)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4Il5PxoYFsZ"
   },
   "source": [
    "2. **Velocidad**\n",
    "\n",
    "   * Describe la **rapidez con la que los datos se generan y deben procesarse**.\n",
    "   * Ejemplo: Transacciones de tarjetas de crédito que deben validarse en milisegundos para evitar fraude.\n",
    "\n",
    "\n",
    "**Caso empresarial:** detección de fraude en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do6TqFs4ZaRP"
   },
   "source": [
    "3. **Variedad**\n",
    "\n",
    "   * Son los **diferentes formatos y fuentes**: datos estructurados (bases de datos), semiestructurados (JSON, XML) y no estructurados (imágenes, videos, texto libre).\n",
    "   * Ejemplo: En salud, los datos provienen de historiales clínicos, estudios de imagen, sensores portátiles, etc.\n",
    "\n",
    "**Caso empresarial:** combinar ventas (estructuradas) con clics web (semiestructurados) para análisis omnicanal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR7VqKfxZapY"
   },
   "outputs": [],
   "source": [
    "# Importar SparkSession: punto de entrada para crear DataFrames y usar Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar requests: librería para realizar peticiones HTTP (descargar archivos desde internet)\n",
    "import requests\n",
    "\n",
    "# Importar os: utilidades del sistema de archivos (rutas, creación de carpetas, comprobaciones)\n",
    "import os\n",
    "\n",
    "# Crear sesión de Spark\n",
    "# builder() arma la configuración; appName() nombra la aplicación (aparece en la UI de Spark);\n",
    "# getOrCreate() crea la sesión si no existe o devuelve la existente.\n",
    "spark = SparkSession.builder.appName(\"Streaming_Clase\").getOrCreate()\n",
    "\n",
    "print(\"=== DESCARGANDO DATASETS DESDE INTERNET ===\")\n",
    "\n",
    "# Definición de la URL del dataset 1 (aeropuertos). Es un CSV público alojado en GitHub.\n",
    "airports_url = \"https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv\"\n",
    "\n",
    "# Ruta local donde se pretende guardar el CSV de aeropuertos\n",
    "airports_file = \"airports_from_internet.csv\"\n",
    "\n",
    "# Mostrar en consola de dónde se va a descargar el archivo (útil para depuración y explicación)\n",
    "print(f\"Descargando aeropuertos desde: {airports_url}\")\n",
    "\n",
    "# En producción conviene crear la carpeta con os.makedirs(output_dir, exist_ok=True) antes de escribir.\n",
    "try:\n",
    "    # Realizar la petición HTTP GET hacia la URL del CSV (puede tardar según la red)\n",
    "    response = requests.get(airports_url, timeout=30)\n",
    "    # Abrir (o crear) el archivo local en modo texto con codificación UTF-8 y escribir el contenido\n",
    "    with open(airports_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    # Indicar que la descarga fue exitosa\n",
    "    print(\"Dataset de aeropuertos descargado exitosamente\")\n",
    "except Exception as e:\n",
    "    # Si ocurre cualquier error (red, permiso, path inválido), lo imprimimos para diagnóstico\n",
    "    print(f\"Error descargando aeropuertos: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset 2: Países (CSV real)\n",
    "# -------------------------\n",
    "\n",
    "# URL pública del dataset de códigos de país (CSV en GitHub)\n",
    "countries_url = \"https://raw.githubusercontent.com/datasets/country-codes/master/data/country-codes.csv\"\n",
    "\n",
    "# Ruta local donde se guardará el CSV de países\n",
    "countries_file = \"countries_from_internet.csv\"\n",
    "\n",
    "# Mensaje en consola indicando la URL de descarga del segundo dataset\n",
    "print(f\"Descargando países desde: {countries_url}\")\n",
    "\n",
    "try:\n",
    "    # Petición HTTP GET para descargar el CSV de países\n",
    "    response = requests.get(countries_url, timeout=30)\n",
    "    # Escribir el contenido descargado en el archivo local con codificación UTF-8\n",
    "    with open(countries_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    # Mensaje de éxito\n",
    "    print(\"Dataset de países descargado exitosamente\")\n",
    "except Exception as e:\n",
    "    # Mensaje de error para depuración si la descarga falla\n",
    "    print(f\"Error descargando países: {e}\")\n",
    "\n",
    "# Línea en blanco para separar secciones en la salida por consola\n",
    "print(\"\\n=== LEYENDO DATASETS DESCARGADOS ===\")\n",
    "\n",
    "# Leer el CSV de aeropuertos con Spark:\n",
    "# - .read crea un lector de DataFrame\n",
    "# - .option(\"header\", True) indica que la primera fila contiene nombres de columnas\n",
    "# - .csv(path) lee el CSV desde la ruta local proporcionada\n",
    "df_airports = spark.read.option(\"header\", True).csv(airports_file)\n",
    "\n",
    "# Leer el CSV de países con Spark (mismas opciones)\n",
    "df_countries = spark.read.option(\"header\", True).csv(countries_file)\n",
    "\n",
    "# Mensaje indicando que vamos a mostrar el esquema (estructura de columnas) del dataset de aeropuertos\n",
    "print(\"=== SCHEMA DE AEROPUERTOS (DESCARGADO) ===\")\n",
    "\n",
    "# printSchema() imprime en consola los nombres de columnas y sus tipos inferidos por Spark\n",
    "df_airports.printSchema()\n",
    "\n",
    "# Mensaje indicando que ahora mostraremos el esquema del dataset de países\n",
    "print(\"\\n=== SCHEMA DE PAÍSES (DESCARGADO) ===\")\n",
    "\n",
    "# Imprimir el esquema de países\n",
    "df_countries.printSchema()\n",
    "\n",
    "# Mensaje que indica que vamos a mostrar una muestra de los datos de aeropuertos\n",
    "print(\"\\n=== MUESTRA DE AEROPUERTOS ===\")\n",
    "\n",
    "# Seleccionar columnas relevantes del DataFrame de aeropuertos y mostrar 5 filas sin truncar\n",
    "# - select(...) elige columnas\n",
    "# - show(5, truncate=False) muestra 5 registros y no trunca texto largo\n",
    "df_airports.select(\"ident\", \"name\", \"iso_country\", \"type\").show(5, truncate=False)\n",
    "\n",
    "# Mensaje que indica que vamos a mostrar una muestra del dataset de países\n",
    "print(\"\\n=== MUESTRA DE PAÍSES ===\")\n",
    "\n",
    "# Seleccionar columnas clave del DataFrame de países y mostrar 5 filas sin truncar\n",
    "# Observa que el nombre de la columna \"ISO3166-1-Alpha-2\" incluye guiones; Spark lo acepta tal cual\n",
    "df_countries.select(\"ISO3166-1-Alpha-2\", \"official_name_en\", \"Continent\").show(5, truncate=False)\n",
    "\n",
    "# Finalmente, cerrar la sesión de Spark para liberar recursos del cluster/local\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSWuPgj9ZbqP"
   },
   "source": [
    "4. **Veracidad**\n",
    "\n",
    "   * Garantizar que los datos sean **fiables y precisos**, evitando errores o duplicidades.\n",
    "   * Ejemplo: Si un sensor de temperatura falla y envía lecturas incorrectas, podría afectar un sistema industrial automatizado.\n",
    "\n",
    "**Caso empresarial:** asegurarse de que no entren datos corruptos al pipeline financiero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3vnNlwXZjXw"
   },
   "outputs": [],
   "source": [
    "# Importar SparkSession: punto de entrada para trabajar con DataFrames en PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar funciones de conveniencia de pyspark.sql.functions\n",
    "# - col: referencia columnas en expresiones\n",
    "# - when, count, length, regexp_replace, upper, trim: funciones útiles para limpieza y validación\n",
    "from pyspark.sql.functions import col, when, count, length, regexp_replace, upper, trim\n",
    "\n",
    "# Importar tipos (no usado explícitamente en el script, pero disponible si necesitas esquemas)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Importar requests para descargar archivos desde una URL\n",
    "import requests\n",
    "\n",
    "# -------------------------\n",
    "# Crear sesión de Spark\n",
    "# -------------------------\n",
    "# .builder: inicia la configuración de la SparkSession\n",
    "# .appName(): nombre de la aplicación (aparece en UI)\n",
    "# .getOrCreate(): crea la sesión o devuelve la existente\n",
    "spark = SparkSession.builder.appName(\"Veracidad_Datos\").getOrCreate()\n",
    "\n",
    "# Mensaje informativo para la consola / presentación\n",
    "print(\"=== SISTEMA DE VERACIDAD DE DATOS ===\")\n",
    "print(\"Garantizando datos fiables, precisos y sin errores\")\n",
    "\n",
    "# -------------------------\n",
    "# Descargar dataset desde internet\n",
    "# -------------------------\n",
    "print(\"\\n=== DESCARGANDO DATASET DESDE INTERNET ===\")\n",
    "\n",
    "# URL pública del dataset de aeropuertos (CSV alojado en GitHub)\n",
    "url = \"https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv\"\n",
    "\n",
    "# Nombre de archivo local donde guardaremos el CSV descargado\n",
    "archivo = \"aeropuertos.csv\"\n",
    "\n",
    "# Mensaje indicando la URL que se va a descargar (útil para depuración)\n",
    "print(f\"Descargando dataset desde: {url}\")\n",
    "\n",
    "# Intento de descarga con manejo básico de excepciones\n",
    "try:\n",
    "    # Realiza la petición HTTP GET (sin timeout en este ejemplo; en producción añade timeout)\n",
    "    response = requests.get(url, timeout=30)\n",
    "    # Escribe el contenido de la respuesta en un archivo local (modo texto con encoding UTF-8)\n",
    "    with open(archivo, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    # Mensaje de éxito\n",
    "    print(\"Dataset descargado exitosamente\")\n",
    "except Exception as e:\n",
    "    # Si ocurre cualquier problema (red, permisos, path), lo mostramos en consola\n",
    "    print(f\"Error descargando: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# 1. DETECCIÓN DE DATOS CORRUPTOS\n",
    "# -------------------------\n",
    "print(\"\\n=== 1. DETECCIÓN DE DATOS CORRUPTOS ===\")\n",
    "\n",
    "# Leer el CSV con Spark indicando que la primera fila es cabecera\n",
    "df = spark.read.option(\"header\", True).csv(archivo)\n",
    "\n",
    "# Mostrar estado inicial: número de registros (count() fuerza ejecución distribuida)\n",
    "print(\"ESTADO INICIAL DEL DATASET:\")\n",
    "print(f\"Total de registros: {df.count()}\")\n",
    "print(f\"Total de columnas: {len(df.columns)}\")\n",
    "\n",
    "# Detectar valores faltantes por columna (isNull detecta solo NULL, no strings vacíos)\n",
    "print(\"\\nVALORES FALTANTES POR COLUMNA:\")\n",
    "for col_name in df.columns:\n",
    "    # Cada filter.count() ejecuta una acción; en datasets grandes puede ser costoso\n",
    "    missing_count = df.filter(col(col_name).isNull()).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"  {col_name}: {missing_count} valores faltantes\")\n",
    "\n",
    "# Detectar duplicados comparando todas las columnas (groupBy(df.columns).count())\n",
    "# Nota: agrupar por todas las columnas puede ser muy costoso si el dataset es grande\n",
    "duplicates = df.groupBy(df.columns).count().filter(col(\"count\") > 1)\n",
    "duplicate_count = duplicates.count()\n",
    "print(f\"\\nDUPLICADOS ENCONTRADOS: {duplicate_count}\")\n",
    "\n",
    "# Si hay duplicados, mostramos ejemplos\n",
    "if duplicate_count > 0:\n",
    "    print(\"Ejemplos de duplicados:\")\n",
    "    duplicates.show(5, truncate=False)\n",
    "\n",
    "# -------------------------\n",
    "# 2. LIMPIEZA Y VALIDACIÓN DE DATOS\n",
    "# -------------------------\n",
    "print(\"\\n=== 2. LIMPIEZA Y VALIDACIÓN DE DATOS ===\")\n",
    "\n",
    "# Eliminar filas que no tienen 'ident' o 'name' (clave mínima para identificar aeropuerto)\n",
    "# .na.drop() elimina filas con NULLs en las columnas indicadas\n",
    "df_clean = df.na.drop(subset=[\"ident\", \"name\"])\n",
    "\n",
    "# Validar formato de 'iso_country': filtramos solo códigos de longitud 2 (ej: 'US', 'CO')\n",
    "df_clean = df_clean.filter(length(col(\"iso_country\")) == 2)\n",
    "\n",
    "# Limpiar el nombre: quitar espacios exteriores y convertir a mayúsculas => columna 'name_clean'\n",
    "df_clean = df_clean.withColumn(\"name_clean\", trim(upper(col(\"name\"))))\n",
    "\n",
    "# Validar coordenadas: mantener solo filas cuya columna 'coordinates' contiene una coma (lon,lat)\n",
    "# Esto evita valores sin formato esperado\n",
    "df_clean = df_clean.filter(col(\"coordinates\").contains(\",\"))\n",
    "\n",
    "# Mostrar número de registros tras la limpieza\n",
    "# (recalcular count -> acción costosa; en produccion considere cache() antes)\n",
    "print(f\"Registros después de limpieza: {df_clean.count()}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. CONTROL DE CALIDAD (reglas)\n",
    "# -------------------------\n",
    "print(\"\\n=== 3. CONTROL DE CALIDAD ===\")\n",
    "\n",
    "# Preparar lista para almacenar resultados de cada regla de validación\n",
    "validation_results = []\n",
    "\n",
    "# Regla 1: Todos los registros limpios deben tener 'ident' no nulo\n",
    "ident_count = df_clean.filter(col(\"ident\").isNotNull()).count()\n",
    "total_count = df_clean.count()\n",
    "validation_results.append((\"Ident único\", ident_count == total_count, f\"{ident_count}/{total_count}\"))\n",
    "\n",
    "# Regla 2: Códigos de país válidos (longitud == 2)\n",
    "valid_countries = df_clean.filter(length(col(\"iso_country\")) == 2).count()\n",
    "validation_results.append((\"Códigos país válidos\", valid_countries == total_count, f\"{valid_countries}/{total_count}\"))\n",
    "\n",
    "# Regla 3: 'type' debe estar dentro de tipos esperados\n",
    "valid_types = [\"large_airport\", \"medium_airport\", \"small_airport\", \"heliport\", \"seaplane_base\"]\n",
    "valid_type_count = df_clean.filter(col(\"type\").isin(valid_types)).count()\n",
    "validation_results.append((\"Tipos válidos\", valid_type_count == total_count, f\"{valid_type_count}/{total_count}\"))\n",
    "\n",
    "# Imprimir los resultados de las validaciones con estado claro\n",
    "print(\"📋 RESULTADOS DE VALIDACIÓN:\")\n",
    "for rule, passed, details in validation_results:\n",
    "    status = \"PASÓ\" if passed else \"FALLÓ\"\n",
    "    print(f\"  {rule}: {status} ({details})\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. DETECCIÓN DE ANOMALÍAS\n",
    "# -------------------------\n",
    "print(\"\\n=== 4. DETECCIÓN DE ANOMALÍAS ===\")\n",
    "\n",
    "# Anomalía 1: contar aeropuertos sin coordenadas (NULL o cadena vacía)\n",
    "sin_coordenadas = df_clean.filter(col(\"coordinates\").isNull() | (col(\"coordinates\") == \"\")).count()\n",
    "print(f\"Aeropuertos sin coordenadas: {sin_coordenadas}\")\n",
    "\n",
    "# Anomalía 2: nombres muy cortos (menos de 3 caracteres) — posible dato corrupto o abreviatura\n",
    "nombres_cortos = df_clean.filter(length(col(\"name\")) < 3).count()\n",
    "print(f\"Nombres muy cortos (<3 chars): {nombres_cortos}\")\n",
    "\n",
    "# Anomalía 3: tipos desconocidos (no están en la lista 'valid_types')\n",
    "tipos_raros = df_clean.filter(~col(\"type\").isin(valid_types)).count()\n",
    "print(f\"Tipos de aeropuerto desconocidos: {tipos_raros}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. REPORTE FINAL DE CALIDAD\n",
    "# -------------------------\n",
    "print(\"\\n=== 5. REPORTE FINAL DE CALIDAD ===\")\n",
    "\n",
    "# Calcular puntaje simple de calidad: porcentaje de reglas que pasaron\n",
    "quality_score = sum(1 for _, passed, _ in validation_results if passed) / len(validation_results) * 100\n",
    "print(f\"PUNTAJE DE CALIDAD: {quality_score:.1f}%\")\n",
    "\n",
    "# Interpretación textual del puntaje\n",
    "if quality_score >= 90:\n",
    "    print(\"EXCELENTE: Los datos son altamente confiables\")\n",
    "elif quality_score >= 70:\n",
    "    print(\"BUENO: Los datos son confiables con algunas excepciones\")\n",
    "else:\n",
    "    print(\"ADVERTENCIA: Los datos requieren revisión manual\")\n",
    "\n",
    "# Resumen con números absolutos\n",
    "print(f\"\\nRESUMEN FINAL:\")\n",
    "print(f\"  - Registros originales: {df.count()}\")\n",
    "print(f\"  - Registros después de limpieza: {df_clean.count()}\")\n",
    "print(f\"  - Registros perdidos: {df.count() - df_clean.count()}\")\n",
    "print(f\"  - Porcentaje de retención: {(df_clean.count() / df.count() * 100):.1f}%\")\n",
    "\n",
    "# Mostrar una muestra de 5 registros limpios y validados para inspección\n",
    "print(\"\\n=== MUESTRA DE DATOS VALIDADOS ===\")\n",
    "df_clean.select(\"ident\", \"name_clean\", \"iso_country\", \"type\", \"coordinates\").show(5, truncate=False)\n",
    "\n",
    "# Cerrar la sesión de Spark para liberar recursos\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jRSqNriZjwv"
   },
   "source": [
    "5. **Valor**\n",
    "\n",
    "   * No basta con almacenar datos: deben **traducirse en conocimiento útil** o en ventajas competitivas.\n",
    "   * Ejemplo: Amazon usa datos de compras para recomendar productos relevantes y aumentar ventas.\n",
    "\n",
    "**Caso empresarial:** usar los datos para priorizar productos más rentables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWTuj7TWaH84"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, desc, when\n",
    "import requests\n",
    "\n",
    "# === 1. INICIO DE SESIÓN SPARK ===\n",
    "spark = SparkSession.builder.appName(\"Valor_Datos\").getOrCreate()\n",
    "\n",
    "print(\"=== EXTRACCIÓN DE VALOR DE DATOS ===\")\n",
    "print(\"Transformando datos en conocimiento útil\")\n",
    "\n",
    "# === 2. DESCARGA DEL DATASET REAL ===\n",
    "url = \"https://raw.githubusercontent.com/datasets/airport-codes/master/data/airport-codes.csv\"\n",
    "archivo = \"aeropuertos.csv\"\n",
    "\n",
    "print(f\"Descargando desde: {url}\")\n",
    "response = requests.get(url)\n",
    "with open(archivo, 'w', encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "print(\"Dataset descargado\")\n",
    "\n",
    "# === 3. LECTURA DEL DATASET CON SPARK ===\n",
    "df = spark.read.option(\"header\", True).csv(archivo)\n",
    "print(f\"Total aeropuertos: {df.count()}\")   # Contar filas totales\n",
    "\n",
    "# === 4. ANÁLISIS ESTRATÉGICO ===\n",
    "print(\"\\n=== 1. TOP 5 PAÍSES CON MÁS AEROPUERTOS ===\")\n",
    "top_countries = (\n",
    "    df.groupBy(\"iso_country\")                # Agrupar por país\n",
    "      .agg(count(\"*\").alias(\"total\"))        # Contar aeropuertos por país\n",
    "      .orderBy(desc(\"total\"))                # Ordenar de mayor a menor\n",
    "      .limit(5)                              # Quedarse con top 5\n",
    ")\n",
    "top_countries.show(truncate=False)           # Mostrar resultado sin truncar\n",
    "\n",
    "# === 5. SEGMENTACIÓN DE MERCADO ===\n",
    "print(\"\\n=== 2. DISTRIBUCIÓN POR TIPO ===\")\n",
    "market_segments = (\n",
    "    df.groupBy(\"type\")                       # Agrupar por tipo de aeropuerto\n",
    "      .agg(count(\"*\").alias(\"cantidad\"))     # Contar por tipo\n",
    "      .orderBy(desc(\"cantidad\"))             # Ordenar descendente\n",
    ")\n",
    "market_segments.show(truncate=False)\n",
    "\n",
    "# === 6. OPORTUNIDADES DE INVERSIÓN ===\n",
    "print(\"\\n=== 3. PAÍSES CON SOLO AEROPUERTOS PEQUEÑOS ===\")\n",
    "opportunities = (\n",
    "    df.groupBy(\"iso_country\")                                         # Agrupar por país\n",
    "      .agg(\n",
    "          count(\"*\").alias(\"total\"),                                  # Total de aeropuertos\n",
    "          count(when(col(\"type\") == \"small_airport\", True)).alias(\"pequeños\"),  # Contar solo pequeños\n",
    "          count(when(col(\"type\") == \"large_airport\", True)).alias(\"grandes\")   # Contar grandes\n",
    "      )\n",
    "      .filter((col(\"grandes\") == 0) & (col(\"total\") > 10))            # Países sin aeropuertos grandes pero con >10 aeropuertos\n",
    "      .orderBy(desc(\"total\"))                                         # Ordenar descendente\n",
    "      .limit(5)                                                       # Limitar a 5\n",
    ")\n",
    "opportunities.show(truncate=False)\n",
    "\n",
    "# === 7. REPORTE FINAL ===\n",
    "print(\"\\n=== REPORTE FINAL ===\")\n",
    "total_airports = df.count()\n",
    "large_airports = df.filter(col(\"type\") == \"large_airport\").count()\n",
    "small_airports = df.filter(col(\"type\") == \"small_airport\").count()\n",
    "\n",
    "print(\"RESUMEN:\")\n",
    "print(f\"  - Aeropuertos grandes: {large_airports} ({large_airports/total_airports*100:.1f}%)\")\n",
    "print(f\"  - Aeropuertos pequeños: {small_airports} ({small_airports/total_airports*100:.1f}%)\")\n",
    "\n",
    "# Cálculo simple de valor estratégico ponderado\n",
    "strategic_value = (large_airports * 0.4 + small_airports * 0.1) / total_airports * 100\n",
    "print(f\"VALOR ESTRATÉGICO: {strategic_value:.1f}%\")\n",
    "\n",
    "# Evaluación de resultados\n",
    "if strategic_value >= 20:\n",
    "    print(\"BUENO: Oportunidades claras identificadas\")\n",
    "else:\n",
    "    print(\"MODERADO: Requiere análisis adicional\")\n",
    "\n",
    "print(\"\\nAPLICACIONES:\")\n",
    "print(\"  - Identificar mercados objetivo\")\n",
    "print(\"  - Oportunidades de inversión\")\n",
    "print(\"  - Estrategias de expansión\")\n",
    "\n",
    "# === 8. CIERRE DE SESIÓN ===\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZvbdDM_aIU3"
   },
   "source": [
    "---\n",
    "\n",
    "### **Extensiones frecuentes: 7V**\n",
    "\n",
    "* **Variabilidad:** los datos no siempre tienen un comportamiento constante, pueden cambiar su estructura o significado con el tiempo.\n",
    "* **Visibilidad:** la capacidad de entender qué datos son importantes y cuáles no.\n",
    "\n",
    "---\n",
    "\n",
    "### **Big Data ≠ Datos grandes**\n",
    "\n",
    "**Big Data implica tres elementos clave:**\n",
    "\n",
    "1. **Procesos:** cómo se capturan, almacenan, procesan y analizan los datos.\n",
    "2. **Tecnología:** infraestructura necesaria (Hadoop, Spark, NoSQL, cloud computing).\n",
    "3. **Personas:** especialistas capaces de transformar datos en decisiones (científicos de datos, ingenieros de datos, analistas).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtLSlV31VwxT"
   },
   "source": [
    "### 1.2 Casos de uso (ejemplos reales)\n",
    "\n",
    "* **Retail**: recomendaciones personalizadas y *forecasting* de demanda.\n",
    "* **Salud**: analítica de cohortes y detección temprana de eventos adversos.\n",
    "* **Servicios financieros**: *fraud detection* en tiempo casi real.\n",
    "* **Transporte urbano**: análisis de flujos, tiempos de viaje, congestión y optimización de rutas.\n",
    "* **Industria/IoT**: mantenimiento predictivo (sensores, *streams* de eventos).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I8_gB7jVzcq"
   },
   "source": [
    "### 1.3 Patrones de procesamiento\n",
    "\n",
    "* **Batch** (lotes): grandes volúmenes, latencia alta; *data lakes*, ETL/ELT.\n",
    "* **Streaming** (tiempo real o casi real): baja latencia; Kafka + Spark Structured Streaming.\n",
    "* **Híbridos**: Lambda/Kappa architectures.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gakub1osW9yp"
   },
   "source": [
    "## 2) **Ecosistema y Arquitectura de Spark**\n",
    "\n",
    "### 2.1 **Piezas del ecosistema (visión rápida)**\n",
    "\n",
    "* **Almacenamiento**:\n",
    "\n",
    "  * **HDFS, S3, Azure Blob, Google Cloud Storage, lagos de datos**: repositorios donde residen datos estructurados y no estructurados.\n",
    "  * **Formatos Parquet/ORC (columnar, comprimido)**: (del inglés Apache Parquet) son un formato de archivo de almacenamiento de datos de código abierto, optimizado para el almacenamiento eficiente y el análisis de grandes volúmenes de datos en el ecosistema de big data.\n",
    "\n",
    "\n",
    "    **Ejemplo:** una aseguradora almacena historiales de pólizas y reclamaciones en Parquet sobre S3 para reducir costos de almacenamiento y acelerar consultas que analizan el riesgo de clientes.\n",
    "\n",
    "\n",
    "* **Capa de tablas**:\n",
    "\n",
    "  * **Delta Lake, Apache Iceberg, Apache Hudi**: permiten transacciones ACID (Atomicidad, Consistencia, Aislamiento y Durabilidad), control de versiones (*time travel*) y evolución de esquemas.\n",
    "\n",
    "\n",
    "    **Ejemplo:** un banco usa Delta Lake para auditar cambios en transacciones históricas: si hay una corrección contable, pueden “viajar en el tiempo” y recuperar el estado de la tabla antes del ajuste.\n",
    "\n",
    "\n",
    "* **Cómputo**:\n",
    "\n",
    "  * **Apache Spark (Batch/SQL/ML/Streaming)**: procesamiento masivo en lotes o en tiempo real.\n",
    "  * **Flink (event-driven)**: orientado a flujos continuos y baja latencia.\n",
    "  * **Trino/Presto (consulta interactiva)**: consultas ad-hoc rápidas sobre diferentes fuentes.\n",
    "\n",
    "\n",
    "    **Ejemplo:** un retailer global usa Spark Streaming para detectar picos de demanda en tiempo real y ajustar inventario; mientras su equipo de BI usa Trino para consultas rápidas sin tener que mover datos.\n",
    "\n",
    "\n",
    "* **Orquestación**:\n",
    "\n",
    "  * **Airflow, Dagster**: coordinan y programan pipelines complejos.\n",
    "\n",
    "  \n",
    "    **Ejemplo:** una empresa de logística usa Airflow para automatizar procesos diarios: extraer datos de sensores de camiones, procesarlos con Spark y generar dashboards de eficiencia de rutas antes de las 7 am.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llNOmCCpXNJK"
   },
   "source": [
    "### 2.2 **Arquitectura de Spark**\n",
    "\n",
    "* **Driver y Executors**:\n",
    "\n",
    "  * El **Driver** coordina, planifica y crea el DAG de ejecución ((Gráfico Acíclico Dirigido) es un gráfico que representa un flujo de trabajo o pipeline de datos de manera visual); los **Executors** procesan los datos.\n",
    "\n",
    "    **Ejemplo:** en una empresa de e-commerce, el driver envía tareas de limpieza y agregación de datos de usuarios hacia múltiples executors que corren en un clúster Kubernetes para calcular métricas de conversión.\n",
    "\n",
    "\n",
    "* **Lazy evaluation**:\n",
    "\n",
    "  * Las transformaciones no ejecutan de inmediato; Spark construye un DAG y solo realiza el cómputo cuando ocurre una **acción**.\n",
    "\n",
    "    **Ejemplo:** una fintech aplica transformaciones (`filter`, `join`) sobre millones de transacciones fraudulentas potenciales, pero el cálculo real solo ocurre cuando llaman `count()` para saber cuántos casos sospechosos hay.\n",
    "\n",
    "\n",
    "* **DataFrame API (Catalyst + Tungsten)**:\n",
    "\n",
    "  * Preferida sobre RDD (conjunto de datos distribuidos resilientes (RDD): Colección inmutable y tolerante a fallos de elementos que se pueden distribuir en varios nodos de clúster para procesarlos en paralelo) porque aplica optimizaciones automáticas de consultas y memoria.\n",
    "\n",
    "    **Ejemplo:** una telco usa Spark SQL para resumir registros de llamadas usando DataFrames en lugar de RDD, reduciendo el tiempo de procesamiento de horas a minutos.\n",
    "\n",
    "\n",
    "* **Transformaciones vs Acciones**:\n",
    "\n",
    "  * **Transformaciones**: `select`, `filter`, `withColumn` (crean un nuevo plan).\n",
    "  * **Acciones**: `count`, `collect`, `write` (disparan la ejecución real).\n",
    "  \n",
    "    **Ejemplo:** una aerolínea transforma datos de reservas para calcular tarifas promedio (`withColumn` y `groupBy`), pero no procesa nada hasta que escribe el resultado (`write`) en una tabla analítica.\n",
    "\n",
    "\n",
    "* **Dependencias *narrow* vs *wide***:\n",
    "\n",
    "  * **Narrow**: cada partición de salida depende de una sola partición de entrada (rápido, sin *shuffle*).\n",
    "  * **Wide**: requieren reorganización (*shuffle*) de datos entre nodos (más costoso).\n",
    "    **Ejemplo:** una plataforma de streaming aplica un `map` (narrow) para limpiar registros de usuarios, pero al hacer un `groupBy` por región (wide) Spark debe redistribuir datos por toda la red, aumentando el tiempo de ejecución.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paso a paso para configurar S3 y Spark**\n",
    "\n",
    "### **Paso 1. Crear y configurar tu cuenta de AWS**\n",
    "\n",
    "1. Ve a [https://aws.amazon.com](https://aws.amazon.com) y crea una cuenta (si no tienes).\n",
    "\n",
    "2. Entra a la **Consola de AWS**.\n",
    "\n",
    "3. Ve al servicio **S3** → crea un bucket:\n",
    "\n",
    "   * Nombre: `aseguradora` (debe ser único en todo AWS).\n",
    "   * Región: elige una cercana (ej. `us-east-1`).\n",
    "   * Desactiva “Bloquear todo el acceso público” solo si necesitas probar sin restricciones (en producción, mantenlo seguro).\n",
    "   * Crea el bucket.\n",
    "\n",
    "4. **Sube un archivo Parquet**:\n",
    "\n",
    "   * Dentro del bucket, crea la carpeta `polizas/2025/`.\n",
    "   * Sube un archivo `*.parquet` dentro.\n",
    "\n",
    "---\n",
    "\n",
    "### **Paso 2. Configurar credenciales de AWS en tu PC**\n",
    "\n",
    "1. Instala la CLI de AWS (si no la tienes):\n",
    "\n",
    "   ```bash\n",
    "   pip install awscli\n",
    "   ```\n",
    "2. Configura tus credenciales:\n",
    "\n",
    "   ```bash\n",
    "   aws configure\n",
    "   ```\n",
    "\n",
    "   * AWS Access Key ID → (copiar desde IAM)\n",
    "   * AWS Secret Access Key → (copiar desde IAM)\n",
    "   * Default region → `us-east-1` (o la región de tu bucket)\n",
    "   * Default output format → `json`\n",
    "\n",
    "> Esto guardará credenciales en `~/.aws/credentials`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Paso 3. Instalar PySpark y dependencias para S3**\n",
    "\n",
    "Necesitas PySpark con el conector de Hadoop para S3:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "pip install boto3\n",
    "```\n",
    "\n",
    "> Spark usará Hadoop para acceder a S3. Con PySpark >= 3.0 ya incluye soporte para `s3a://` y `s3://`, pero a veces hay que forzarlo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Paso 4. Configurar Spark para usar S3**\n",
    "\n",
    "En tu script o notebook, debes pasar credenciales a Spark:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadFromS3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"TU_ACCESS_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"TU_SECRET_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lectura de datos\n",
    "df_policies = spark.read.parquet(\"s3a://aseguradora/polizas/2025/\")\n",
    "df_policies.printSchema()\n",
    "```\n",
    "\n",
    "> **Importante:**\n",
    ">\n",
    "> * Usa `s3a://` en lugar de `s3://` para evitar errores de compatibilidad.\n",
    "> * Nunca dejes las llaves reales en código público: en producción, Spark las toma automáticamente de `~/.aws/credentials`.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Verificación paso a paso**\n",
    "\n",
    "1. Verifica que puedes listar el bucket:\n",
    "\n",
    "   ```bash\n",
    "   aws s3 ls s3://aseguradora/polizas/2025/\n",
    "   ```\n",
    "\n",
    "   Si ves el archivo → tu CLI funciona bien.\n",
    "\n",
    "2. Ejecuta el script PySpark. Si todo está bien, Spark debe imprimir el esquema de tu Parquet.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Errores comunes y soluciones**\n",
    "\n",
    "* **`No FileSystem for scheme: s3`** → te falta el conector Hadoop S3 (usa PySpark >= 3.0 o agrega `hadoop-aws` jar).\n",
    "* **`Access Denied`** → revisa credenciales o permisos IAM (necesitas `s3:GetObject`).\n",
    "* **`File not found`** → revisa la ruta exacta del archivo/carpeta.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EJERCICIO COMPLETO: Ecosistema y Arquitectura de Spark (PySpark)\n",
    "================================================================\n",
    "\n",
    "## Ecosistema y Arquitectura de Spark\n",
    "-------------------------------------\n",
    "2.1 Piezas del ecosistema (visión rápida)\n",
    "- Almacenamiento: HDFS, S3, Azure Blob, Google Cloud Storage, lagos de datos. Usamos Parquet/ORC (columnar, comprimido).\n",
    "- Capa de tablas: Delta Lake, Apache Iceberg, Apache Hudi (ACID, time travel, evolución de esquemas). Aquí usamos Delta si está disponible.\n",
    "- Cómputo: Apache Spark (Batch/SQL/ML/Streaming), Flink (event-driven), Trino/Presto (consulta interactiva).\n",
    "- Orquestación: Airflow, Dagster (coordinan pipelines). Incluimos un DAG de Airflow de ejemplo al final (comentado).\n",
    "\n",
    "2.2 Arquitectura de Spark\n",
    "- Driver y Executors: el Driver coordina y crea el DAG; los Executors procesan los datos en el clúster.\n",
    "- Lazy evaluation: transformaciones construyen el plan; las acciones disparan la ejecución.\n",
    "- DataFrame API (Catalyst + Tungsten): optimización automática; preferible a RDD.\n",
    "- Transformaciones vs Acciones: select/filter/withColumn (plan), count/collect/write (ejecución).\n",
    "- Dependencias narrow vs wide: narrow (sin shuffle), wide (con shuffle).\n",
    "\n",
    "PASO A PASO S3\n",
    "- Incluye guía para configurar AWS CLI, credenciales, y cómo leer s3a://...\n",
    "\n",
    "MODO DE USO\n",
    "-----------\n",
    "1) LOCAL (recomendado para probar ya):\n",
    "   - Requisitos: Python 3.9+, Java 8/11, Apache Spark 3.x instalado y en PATH.\n",
    "   - (Opcional) Delta Lake: `pip install delta-spark`\n",
    "   - Ejecuta:\n",
    "       spark-submit ejercicio_spark_completo.py\n",
    "\n",
    "2) S3 (opcional):\n",
    "   - Configura AWS CLI: `pip install awscli && aws configure`\n",
    "   - Crea bucket y sube Parquet como se detalla en el README/teoría.\n",
    "   - En este script, establece USE_S3 = True y ajusta credenciales/paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import date\n",
    "from typing import Tuple\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# ======================================================\n",
    "# Parámetros del ejercicio\n",
    "# ======================================================\n",
    "\n",
    "# Cambiar a True si deseas leer desde S3 (requiere credenciales y datos existentes)\n",
    "USE_S3 = False\n",
    "\n",
    "# Rutas LOCAL\n",
    "BASE_DIR = os.path.abspath(os.path.dirname(__file__))\n",
    "LOCAL_PARQUET_DIR = os.path.join(BASE_DIR, \"data\", \"parquet\", \"polizas\", \"2025\")\n",
    "LOCAL_DELTA_DIR   = os.path.join(BASE_DIR, \"data\", \"delta\", \"polizas\")\n",
    "\n",
    "# Rutas S3 (si USE_S3=True)\n",
    "S3_INPUT = \"s3a://aseguradora/polizas/2025/\"\n",
    "S3_DELTA = \"s3a://aseguradora/delta/polizas/\"  # si deseas escribir Delta en S3\n",
    "\n",
    "# Credenciales S3 (solo si no usas ~/.aws/credentials). No pongas llaves reales en código público.\n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"TU_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"TU_SECRET_KEY\")\n",
    "AWS_ENDPOINT   = os.environ.get(\"AWS_ENDPOINT\", \"s3.amazonaws.com\")\n",
    "\n",
    "\n",
    "def build_spark() -> Tuple[SparkSession, bool]:\n",
    "    \"\"\"\n",
    "    Crea la SparkSession. Si delta-spark está instalado, habilita Delta Lake (capa de tablas ACID + time travel).\n",
    "    Devuelve (spark, delta_enabled).\n",
    "    \"\"\"\n",
    "    delta_enabled = False\n",
    "\n",
    "    # Builder base\n",
    "    builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"EjercicioSpark_Completo\")\n",
    "    )\n",
    "\n",
    "    if USE_S3:\n",
    "        # ----------------------------\n",
    "        # ECOSISTEMA → Almacenamiento en S3 (HDFS/Blob/GCS alternativos)\n",
    "        # ----------------------------\n",
    "        # Preferir s3a:// para compatibilidad.\n",
    "        builder = (\n",
    "            builder\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT)\n",
    "        )\n",
    "\n",
    "    # Intentar habilitar Delta Lake si está disponible\n",
    "    try:\n",
    "        # delta-spark permite configurar fácilmente la sesión con los JARs de Delta\n",
    "        from delta import configure_spark_with_delta_pip\n",
    "        builder = (\n",
    "            builder\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        )\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "        delta_enabled = True\n",
    "    except Exception:\n",
    "        # Si no hay delta-spark, seguimos con Spark \"vanilla\"\n",
    "        spark = builder.getOrCreate()\n",
    "\n",
    "    # Ajustes útiles para el ejercicio\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "    return spark, delta_enabled\n",
    "\n",
    "\n",
    "def ensure_local_dirs():\n",
    "    os.makedirs(LOCAL_PARQUET_DIR, exist_ok=True)\n",
    "    os.makedirs(LOCAL_DELTA_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def create_sample_data(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame de pólizas sintético para LOCAL mode.\n",
    "    (ECOSISTEMA - Almacenamiento → Parquet; FORMATO columnar): generaremos y guardaremos en Parquet.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"poliza_id\",   StringType(), False),\n",
    "        StructField(\"cliente_id\",  StringType(), False),\n",
    "        StructField(\"region\",      StringType(), False),\n",
    "        StructField(\"monto\",       DoubleType(), False),\n",
    "        StructField(\"prima\",       DoubleType(), False),\n",
    "        StructField(\"estado\",      StringType(), False),\n",
    "        StructField(\"fecha_alta\",  DateType(),   False),\n",
    "    ])\n",
    "\n",
    "    rows = [\n",
    "        (\"P-0001\", \"C-100\", \"Norte\",   950.0,  95.0,  \"activa\",  date(2025, 1, 10)),\n",
    "        (\"P-0002\", \"C-101\", \"Norte\",  1200.0, 120.0,  \"activa\",  date(2025, 2,  5)),\n",
    "        (\"P-0003\", \"C-102\", \"Sur\",     800.0,  80.0,  \"vencida\", date(2025, 2, 18)),\n",
    "        (\"P-0004\", \"C-103\", \"Centro\", 2200.0, 220.0,  \"activa\",  date(2025, 3,  7)),\n",
    "        (\"P-0005\", \"C-104\", \"Sur\",    1800.0, 180.0,  \"activa\",  date(2025, 3, 15)),\n",
    "        (\"P-0006\", \"C-105\", \"Norte\",  3100.0, 310.0,  \"activa\",  date(2025, 3, 20)),\n",
    "        (\"P-0007\", \"C-106\", \"Centro\",  500.0,  50.0,  \"vencida\", date(2025, 3, 25)),\n",
    "        (\"P-0008\", \"C-107\", \"Sur\",    1300.0, 130.0,  \"activa\",  date(2025, 4,  2)),\n",
    "        (\"P-0009\", \"C-108\", \"Norte\",  2600.0, 260.0,  \"activa\",  date(2025, 4, 12)),\n",
    "        (\"P-0010\", \"C-109\", \"Centro\",  950.0,  95.0,  \"activa\",  date(2025, 4, 22)),\n",
    "    ]\n",
    "\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "    # Guardar en Parquet (FORMATO columnar comprimido)\n",
    "    df.write.mode(\"overwrite\").parquet(LOCAL_PARQUET_DIR)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_input(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Lee datos desde Parquet LOCAL o desde S3 (según USE_S3).\n",
    "    \"\"\"\n",
    "    if USE_S3:\n",
    "        print(f\"🔹 Leyendo desde S3 (Parquet): {S3_INPUT}\")\n",
    "        return spark.read.parquet(S3_INPUT)\n",
    "    else:\n",
    "        print(f\"🔹 Leyendo desde LOCAL (Parquet): {LOCAL_PARQUET_DIR}\")\n",
    "        return spark.read.parquet(LOCAL_PARQUET_DIR)\n",
    "\n",
    "\n",
    "def run_core_exercise(spark: SparkSession, df):\n",
    "    \"\"\"\n",
    "    Ejecuta el ejercicio central cubriendo 2.2 Arquitectura:\n",
    "    - Lazy evaluation (transformaciones no ejecutan hasta acción)\n",
    "    - DataFrame API (Catalyst+Tungsten)\n",
    "    - Transformaciones vs Acciones\n",
    "    - Narrow vs Wide (sin y con shuffle)\n",
    "    - SQL, caché, particionado\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 2.2 ARQUITECTURA DE SPARK ===\")\n",
    "    print(\"Driver (este programa) construye el DAG; Executors procesan los datos al ejecutar acciones.\")\n",
    "\n",
    "    print(\"\\n-- Esquema de entrada --\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # Lazy evaluation: transformaciones (no ejecuta)\n",
    "    print(\"\\n-- Transformaciones (lazy) --\")\n",
    "    df_trans = (df\n",
    "                .filter(F.col(\"monto\") > 1000)                 # narrow\n",
    "                .withColumnRenamed(\"cliente_id\", \"id_cliente\") # narrow\n",
    "                .withColumn(\"ratio_prima\", F.col(\"prima\")/F.col(\"monto\")))\n",
    "\n",
    "    # Catalyst/Tungsten: el optimizador y motor físico actuarán al ejecutar acciones.\n",
    "    print(\"\\n-- Plan lógico/físico (explain) --\")\n",
    "    df_trans.explain(mode=\"formatted\")\n",
    "\n",
    "    # Acción: count() dispara ejecución\n",
    "    print(\"\\n-- Acción: count() --\")\n",
    "    n = df_trans.count()\n",
    "    print(f\"Registros con monto > 1000: {n}\")\n",
    "\n",
    "    # Narrow vs Wide: groupBy → shuffle (wide)\n",
    "    print(\"\\n-- Wide dependency: groupBy (shuffle) --\")\n",
    "    df_grp = df_trans.groupBy(\"region\").agg(F.count(\"*\").alias(\"polizas\"),\n",
    "                                            F.avg(\"monto\").alias(\"monto_prom\"),\n",
    "                                            F.avg(\"prima\").alias(\"prima_prom\"))\n",
    "\n",
    "    print(\"\\n-- Acción: collect() --\")\n",
    "    for row in df_grp.collect():\n",
    "        print(row)\n",
    "\n",
    "    # SQL vs DataFrame API\n",
    "    print(\"\\n-- Spark SQL --\")\n",
    "    df_trans.createOrReplaceTempView(\"polizas_filtradas\")\n",
    "    sql_res = spark.sql(\"\"\"\n",
    "        SELECT region,\n",
    "               COUNT(*)  AS polizas,\n",
    "               ROUND(AVG(monto), 2) AS monto_prom,\n",
    "               ROUND(AVG(prima), 2) AS prima_prom\n",
    "        FROM polizas_filtradas\n",
    "        GROUP BY region\n",
    "        ORDER BY polizas DESC\n",
    "    \"\"\")\n",
    "    sql_res.show(truncate=False)\n",
    "\n",
    "    # Caché (útil cuando reusaremos el mismo resultado varias veces)\n",
    "    print(\"\\n-- Cache / Persist --\")\n",
    "    df_cached = df_trans.cache()\n",
    "    print(\"Cached count:\", df_cached.count())\n",
    "\n",
    "    # Particionado: coalesce (narrow) vs repartition (wide - shuffle)\n",
    "    print(\"\\n-- Particionado --\")\n",
    "    print(\"Particiones actuales:\", df_cached.rdd.getNumPartitions())\n",
    "    df_repart = df_cached.repartition(4, F.col(\"region\"))  # wide (shuffle por 'region')\n",
    "    print(\"Particiones tras repartition(4, 'region'):\", df_repart.rdd.getNumPartitions())\n",
    "\n",
    "    # Escritura particionada en Parquet (ECOSISTEMA - Almacenamiento)\n",
    "    out_part_dir = os.path.join(os.path.dirname(LOCAL_PARQUET_DIR), \"out_particionado\")\n",
    "    df_repart.write.mode(\"overwrite\").partitionBy(\"region\").parquet(out_part_dir)\n",
    "    print(f\"Escritura particionada en Parquet → {out_part_dir}\")\n",
    "\n",
    "    return df_trans\n",
    "\n",
    "\n",
    "def try_delta_table(spark: SparkSession, df_trans, delta_enabled: bool):\n",
    "    \"\"\"\n",
    "    Capa de tablas: Delta Lake (si está disponible).\n",
    "    - ACID, Time Travel (versionAsOf), evolución de esquema.\n",
    "    - Demostración: escribimos Delta, hacemos una actualización (UPDATE), consultamos estado anterior (time travel).\n",
    "    \"\"\"\n",
    "    if not delta_enabled:\n",
    "        print(\"\\n(Delta Lake NO disponible; instala con `pip install delta-spark` para habilitar esta sección).\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Capa de Tablas: Delta Lake (ACID + Time Travel) ===\")\n",
    "    target_path = LOCAL_DELTA_DIR if not USE_S3 else S3_DELTA\n",
    "\n",
    "    # Guardar como Delta\n",
    "    (df_trans\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(target_path))\n",
    "    print(f\"Delta escrito en: {target_path}\")\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "    dtab = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "    # UPDATE (ACID): subir la prima un 10% a pólizas de región 'Norte'\n",
    "    print(\"-- UPDATE en Delta (ACID) --\")\n",
    "    dtab.update(\n",
    "        condition=F.expr(\"region = 'Norte'\"),\n",
    "        set={\"prima\": F.expr(\"prima * 1.10\")}\n",
    "    )\n",
    "\n",
    "    # LEER estado actual\n",
    "    print(\"-- Leer Delta (estado actual) --\")\n",
    "    spark.read.format(\"delta\").load(target_path).groupBy(\"region\").agg(\n",
    "        F.round(F.avg(\"prima\"), 2).alias(\"prima_prom\")\n",
    "    ).show()\n",
    "\n",
    "    # TIME TRAVEL: leer versión anterior (antes del UPDATE)\n",
    "    print(\"-- Time Travel (versionAsOf=0) --\")\n",
    "    spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(target_path).groupBy(\"region\").agg(\n",
    "        F.round(F.avg(\"prima\"), 2).alias(\"prima_prom_v0\")\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def demo_streaming(spark: SparkSession):\n",
    "    \"\"\"\n",
    "    Cómputo en tiempo real (Streaming): usamos la fuente 'rate' para simular eventos.\n",
    "    - Esto ilustra Spark Streaming (en el ecosistema, retailer detecta picos en tiempo real).\n",
    "    - Usamos trigger(once=True) para un solo micro-batch y no quedar en ejecución indefinida.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Streaming: Structured Streaming con fuente 'rate' ===\")\n",
    "    rate_df = (spark.readStream\n",
    "                    .format(\"rate\")\n",
    "                    .option(\"rowsPerSecond\", 10)\n",
    "                    .load())\n",
    "\n",
    "    # Transformaciones en streaming (lazy)\n",
    "    stream_trans = (rate_df\n",
    "                    .withColumn(\"region\",\n",
    "                                F.when((F.col(\"value\") % 3) == 0, F.lit(\"Norte\"))\n",
    "                                 .when((F.col(\"value\") % 3) == 1, F.lit(\"Sur\"))\n",
    "                                 .otherwise(F.lit(\"Centro\")))\n",
    "                    .groupBy(\"region\")\n",
    "                    .count())  # wide\n",
    "\n",
    "    # Escribimos a memoria para poder consultar luego con SQL\n",
    "    query = (stream_trans.writeStream\n",
    "             .format(\"memory\")\n",
    "             .queryName(\"demanda_regiones\")\n",
    "             .outputMode(\"complete\")\n",
    "             .trigger(once=True)   # ejecuta un micro-batch y termina\n",
    "             .start())\n",
    "\n",
    "    query.awaitTermination()\n",
    "    print(\"-- Resultados en memoria (SQL sobre streaming) --\")\n",
    "    spark.sql(\"SELECT * FROM demanda_regiones ORDER BY count DESC\").show()\n",
    "\n",
    "\n",
    "def airflow_example():\n",
    "    \"\"\"\n",
    "    ORQUESTACIÓN (Airflow): DAG de ejemplo (comentado) que ejecuta este script con SparkSubmitOperator.\n",
    "\n",
    "    # Guarda como: dags/pipeline_polizas.py\n",
    "    ----------------------------------------------------\n",
    "    from airflow import DAG\n",
    "    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "    from datetime import datetime\n",
    "\n",
    "    with DAG(\n",
    "        dag_id=\"pipeline_polizas\",\n",
    "        start_date=datetime(2025, 8, 1),\n",
    "        schedule_interval=\"0 6 * * *\",  # todos los días 06:00\n",
    "        catchup=False,\n",
    "        default_args={\"owner\": \"data-eng\"}\n",
    "    ) as dag:\n",
    "\n",
    "        tarea_spark = SparkSubmitOperator(\n",
    "            task_id=\"procesar_polizas\",\n",
    "            application=\"/path/a/ejercicio_spark_completo.py\",\n",
    "            conn_id=\"spark_default\",\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        tarea_spark\n",
    "    ----------------------------------------------------\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def s3_howto_notes():\n",
    "    \"\"\"\n",
    "    PASO A PASO S3 (resumen práctico dentro del script):\n",
    "    1) Crea cuenta en AWS y configura CLI:\n",
    "        pip install awscli\n",
    "        aws configure\n",
    "    2) Crea bucket 'aseguradora' y sube Parquet a s3://aseguradora/polizas/2025/\n",
    "    3) En este script, pon USE_S3=True y ajusta claves o usa ~/.aws/credentials\n",
    "    4) Usa rutas s3a://... Ej.: spark.read.parquet(\"s3a://aseguradora/polizas/2025/\")\n",
    "    5) Errores comunes:\n",
    "       - No FileSystem for scheme: s3 → falta hadoop-aws/versión.\n",
    "       - Access Denied → permisos IAM (s3:GetObject).\n",
    "       - File not found → ruta/bucket incorrectos.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not USE_S3:\n",
    "        ensure_local_dirs()\n",
    "\n",
    "    spark, delta_enabled = build_spark()\n",
    "\n",
    "    try:\n",
    "        # En LOCAL creamos datos de ejemplo → guardamos en Parquet\n",
    "        if not USE_S3:\n",
    "            create_sample_data(spark)\n",
    "\n",
    "        # Leemos la \"capa de almacenamiento\": Parquet local o S3\n",
    "        df_in = read_input(spark)\n",
    "\n",
    "        # Núcleo del ejercicio (Arquitectura: lazy, acciones, narrow/wide, SQL, caché, particionado)\n",
    "        df_trans = run_core_exercise(spark, df_in)\n",
    "\n",
    "        # Capa de tablas: Delta Lake (ACID + Time Travel), si está disponible\n",
    "        try_delta_table(spark, df_trans, delta_enabled)\n",
    "\n",
    "        # Cómputo en tiempo real: mini demo de Streaming (rate source)\n",
    "        demo_streaming(spark)\n",
    "\n",
    "        print(\"\\n✅ EJERCICIO COMPLETO FINALIZADO CORRECTAMENTE.\")\n",
    "        print(\"   - Ecosistema (almacenamiento, tabla/Delta, cómputo, orquestación).\")\n",
    "        print(\"   - Arquitectura (Driver/Executors, lazy, Catalyst/Tungsten, transformaciones vs acciones, narrow/wide).\")\n",
    "        print(\"   - SQL, caché, particionado, Streaming, Delta (si disponible).\")\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFqn9evD7wAueE07OVI5/+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
