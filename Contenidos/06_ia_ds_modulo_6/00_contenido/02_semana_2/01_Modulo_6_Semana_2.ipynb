{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f83c0b8",
   "metadata": {},
   "source": [
    "# Clase 2 — Técnicas avanzadas en Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "# Resumen ejecutivo y objetivos de la sesión\n",
    "\n",
    "Duración: **180 minutos** (3 horas)\n",
    "\n",
    "Objetivos de aprendizaje:\n",
    "\n",
    "1. Comprender matemáticamente y conceptualmente PCA y t-SNE; conocer sus ventajas, limitaciones y pautas prácticas.\n",
    "2. Aplicar PCA y t-SNE a datos reales (scikit-learn `digits` / `iris`) siguiendo buenas prácticas de ingeniería reproducible.\n",
    "3. Crear visualizaciones interactivas con **Plotly** y un pequeño **Dash app** para explorar embeddings y parámetros en tiempo real.\n",
    "4. Integrar normas de estilo y calidad de código (PEP-8, docstrings, typing, pipelines reproducibles).\n",
    "\n",
    "Público: estudiantes con conocimientos previos de Python, pandas y scikit-learn básico.\n",
    "\n",
    "Requisitos previos para los alumnos:\n",
    "\n",
    "* Python 3.9+ instalado.\n",
    "* Paquetes: `numpy, pandas, scikit-learn, plotly, dash` (lista más abajo con versiones sugeridas).\n",
    "\n",
    "---\n",
    "\n",
    "# Materiales y setup (comandos rápidos)\n",
    "\n",
    "requirements.txt:\n",
    "\n",
    "```\n",
    "numpy>=1.23\n",
    "pandas>=1.5\n",
    "scikit-learn>=1.2\n",
    "plotly>=5.15\n",
    "dash>=2.9\n",
    "umap-learn>=0.5  # opcional, alternativa a t-SNE\n",
    "```\n",
    "---\n",
    "\n",
    "# CONTENIDO TEÓRICO \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3cb35",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "\n",
    "## 1. Definición general\n",
    "\n",
    "**PCA (Análisis de Componentes Principales)** es una técnica estadística de **reducción de dimensionalidad lineal** que busca transformar un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables **no correlacionadas**, llamadas **componentes principales**.\n",
    "\n",
    "Cada componente principal es una **combinación lineal de las variables originales**, y están ordenados de tal manera que:\n",
    "\n",
    "* El primer componente captura la **máxima varianza posible** de los datos.\n",
    "* El segundo componente captura la **máxima varianza posible no explicada por el primero**, y así sucesivamente.\n",
    "* Todos los componentes son **ortogonales** entre sí.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Objetivos principales\n",
    "\n",
    "* **Reducir la dimensionalidad**: Mantener la mayor parte de la información (varianza) eliminando redundancia.\n",
    "* **Eliminar multicolinealidad**: Los componentes principales no están correlacionados.\n",
    "* **Mejorar eficiencia**: Modelos de Machine Learning entrenan más rápido en espacios reducidos.\n",
    "* **Facilitar visualización**: Transformar datos de alta dimensión a 2D o 3D para graficarlos.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Fundamentos matemáticos\n",
    "\n",
    "Dado un dataset con $n$ observaciones y $p$ variables:\n",
    "\n",
    "1. **Estandarización**\n",
    "   Se recomienda escalar las variables ($z$-score) para que todas contribuyan en la misma magnitud:\n",
    "\n",
    "   $$\n",
    "   z_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j}\n",
    "   $$\n",
    "\n",
    "   donde $\\bar{x}_j$ es la media y $s_j$ la desviación estándar de la variable $j$.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Matriz de covarianza o correlación**\n",
    "   Se calcula la matriz de covarianza $\\Sigma$:\n",
    "\n",
    "   $$\n",
    "   \\Sigma = \\frac{1}{n-1} Z^T Z\n",
    "   $$\n",
    "\n",
    "   donde $Z$ es la matriz de datos estandarizados.\n",
    "\n",
    "   Esta matriz refleja cómo varían conjuntamente las variables.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Descomposición espectral (Eigen decomposition)**\n",
    "   Se encuentran **autovalores** ($\\lambda$) y **autovectores** ($v$) de $\\Sigma$:\n",
    "\n",
    "   $$\n",
    "   \\Sigma v = \\lambda v\n",
    "   $$\n",
    "\n",
    "   * Los **autovalores** indican la varianza explicada por cada componente.\n",
    "   * Los **autovectores** son las direcciones de los componentes principales.\n",
    "\n",
    "   **Ordenar** los autovalores de mayor a menor → así se definen los componentes principales.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Varianza explicada**\n",
    "   La proporción de varianza capturada por el $k$-ésimo componente es:\n",
    "\n",
    "   $$\n",
    "   \\text{Varianza explicada}_k = \\frac{\\lambda_k}{\\sum_{i=1}^p \\lambda_i}\n",
    "   $$\n",
    "\n",
    "   Generalmente se seleccionan los primeros componentes que acumulen, por ejemplo, el **95% de la varianza**.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Transformación de los datos**\n",
    "   La proyección de los datos originales sobre los nuevos ejes:\n",
    "\n",
    "   $$\n",
    "   Z_{\\text{PCA}} = Z \\cdot V_k\n",
    "   $$\n",
    "\n",
    "   donde $V_k$ son los primeros $k$ autovectores.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Interpretación práctica\n",
    "\n",
    "* **Primer componente (PC1):** Dirección que maximiza la varianza.\n",
    "* **Segundo componente (PC2):** Ortogonal a PC1, maximiza varianza restante.\n",
    "* **Carga (loading):** Coeficiente de cada variable en la combinación lineal; indica qué variables influyen más en el componente.\n",
    "* **Score:** Coordenadas de cada observación en el nuevo espacio de componentes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Ventajas\n",
    "\n",
    "* Reduce ruido y redundancia en los datos.\n",
    "* Facilita visualización de datasets complejos en 2D o 3D.\n",
    "* Ayuda a mejorar el rendimiento de modelos en datasets muy grandes.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Limitaciones\n",
    "\n",
    "* Es un método **lineal** — no captura relaciones no lineales.\n",
    "* Los componentes no siempre son interpretables fácilmente (son combinaciones lineales abstractas).\n",
    "* Sensible a escalado: si no se estandarizan las variables, aquellas con valores más grandes dominan el análisis.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Ejemplo ilustrativo (caso intuitivo)\n",
    "\n",
    "Imagina un dataset con dos variables altamente correlacionadas (ejemplo: **altura** y **peso** en una población).\n",
    "\n",
    "* PCA encuentra una **nueva variable** (componente principal) que resume esa correlación (dirección de máxima varianza).\n",
    "* En lugar de trabajar con 2 variables redundantes, podemos proyectar los datos en ese componente principal y **resumir la información en una sola variable**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bcc6e",
   "metadata": {},
   "source": [
    "# t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "## 1. Definición general\n",
    "\n",
    "El **t-SNE** es una técnica de **reducción de dimensionalidad no lineal** diseñada principalmente para **visualización** de datos de alta dimensión en 2D o 3D.\n",
    "Fue propuesta por Laurens van der Maaten y Geoffrey Hinton en 2008 como una mejora sobre **SNE (Stochastic Neighbor Embedding)**.\n",
    "\n",
    "* Su objetivo: **preservar la estructura local de los datos** (vecindades) en un espacio de baja dimensión.\n",
    "\n",
    "A diferencia de **PCA**, que es lineal y global, t-SNE busca que **puntos similares en alta dimensión se mantengan cercanos en la proyección**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Intuición\n",
    "\n",
    "* Imagina un dataset en 100 dimensiones.\n",
    "* t-SNE \"aprende\" una representación en 2D donde **puntos similares** (según distancia en alta dimensión) siguen siendo vecinos.\n",
    "* Se enfoca en la **estructura local**, no en mantener las distancias globales.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "* Si tienes imágenes de dígitos (0-9), t-SNE agrupa los dígitos iguales cerca entre sí, aunque la distancia entre los diferentes dígitos no sea perfectamente proporcional a la original.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Parámetros clave\n",
    "\n",
    "* **Perplexity**: controla el balance entre estructura local y global (típicamente entre 5 y 50). Relacionado con el número efectivo de vecinos.\n",
    "* **Learning rate**: afecta la estabilidad de la optimización; valores muy bajos → agrupamiento artificial, muy altos → dispersión.\n",
    "* **n\\_iter**: número de iteraciones; demasiadas pocas → no converge, demasiadas → sobreajuste.\n",
    "* **init**: inicialización, comúnmente con PCA para mayor estabilidad.\n",
    "* **random\\_state**: controla la reproducibilidad.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Ventajas\n",
    "\n",
    "* Excelente para visualizar datos complejos en 2D o 3D.\n",
    "* Captura relaciones locales muy bien.\n",
    "* Agrupa naturalmente clusters.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Limitaciones\n",
    "\n",
    "* **No es determinista** (resultados varían salvo que fijes `random_state`).\n",
    "* **No preserva relaciones globales**: la distancia entre clusters no siempre es significativa.\n",
    "* **Computacionalmente costoso** en datasets grandes.\n",
    "* **No es transformador general**: no puedes aplicar el mismo embedding a nuevos datos sin recalcular.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Buenas prácticas\n",
    "\n",
    "* Aplicar **PCA previo** para reducir dimensiones a 30-50 antes de t-SNE → mejora velocidad y estabilidad.\n",
    "* Probar varios valores de `perplexity` y comparar resultados.\n",
    "* Usar t-SNE solo como herramienta exploratoria/visual, no como método para modelado predictivo.\n",
    "* Para datasets grandes (>50k muestras), considerar **UMAP** como alternativa más rápida y escalable.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Ejemplo intuitivo\n",
    "\n",
    "Si tienes un dataset de imágenes de ropa (camisas, zapatos, bolsos), t-SNE puede proyectar los datos en 2D de forma que cada tipo de prenda quede agrupada en clusters separados, **sin que tú hayas dado las etiquetas previamente**.\n",
    "\n",
    "Esto lo convierte en una herramienta poderosa para **descubrimiento de patrones** y **detección de estructuras ocultas**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1312e",
   "metadata": {},
   "source": [
    "## Alternativa: UMAP\n",
    "\n",
    "* UMAP es otra técnica no lineal, más rápida y que suele preservar mejor la estructura global; buena alternativa a t-SNE. ([arXiv][5])\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c730e3a",
   "metadata": {},
   "source": [
    "# Enunciado del Ejercicio: Reducción de Dimensionalidad con PCA y t-SNE\n",
    "\n",
    "## Contexto\n",
    "\n",
    "Las organizaciones actuales generan grandes volúmenes de información de diversa naturaleza: transacciones financieras, registros médicos, datos de sensores, imágenes, textos, etc. Sin embargo, estos datos suelen tener **alta dimensionalidad** (muchas variables o características), lo cual dificulta su análisis, interpretación y visualización.\n",
    "\n",
    "En el campo de la **Ciencia de Datos y Big Data**, las técnicas de **reducción de dimensionalidad** permiten transformar datasets complejos en representaciones más compactas y fáciles de interpretar, sin perder demasiada información. Entre estas técnicas se encuentran:\n",
    "\n",
    "* **PCA (Principal Component Analysis):** Método lineal que transforma las variables originales en nuevas combinaciones (componentes principales) maximizando la varianza explicada.\n",
    "* **t-SNE (t-distributed Stochastic Neighbor Embedding):** Método no lineal que preserva relaciones de vecindad, muy útil para la visualización de datos complejos en 2D o 3D.\n",
    "\n",
    "Este ejercicio busca que los estudiantes apliquen estas dos técnicas sobre un dataset real y comparen los resultados, entendiendo sus ventajas y limitaciones.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo del Ejercicio\n",
    "\n",
    "1. Comprender la importancia de la reducción de dimensionalidad en ciencia de datos.\n",
    "2. Aplicar **PCA** y **t-SNE** a un dataset real de dígitos manuscritos.\n",
    "3. Evaluar y comparar cómo ambas técnicas representan los datos en un espacio de dos dimensiones.\n",
    "4. Visualizar los resultados mediante gráficos interactivos que faciliten la interpretación.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Se trabajará con el dataset **Digits** de `sklearn`, que contiene:\n",
    "\n",
    "* **1797 imágenes de dígitos manuscritos (0 a 9).**\n",
    "* Cada imagen es de **8x8 píxeles**, lo que genera **64 variables**.\n",
    "* La etiqueta asociada a cada observación indica el número real escrito.\n",
    "\n",
    "---\n",
    "\n",
    "## Actividades a realizar\n",
    "\n",
    "### **Parte 1: Preparación y preprocesamiento**\n",
    "\n",
    "1. Cargar el dataset `Digits` desde `sklearn.datasets`.\n",
    "2. Revisar la forma y las características de los datos.\n",
    "3. Normalizar las variables utilizando `StandardScaler` para garantizar que todas las características tengan la misma importancia.\n",
    "\n",
    "---\n",
    "\n",
    "### **Parte 2: Análisis con PCA**\n",
    "\n",
    "1. Reducir las 64 dimensiones a 2 componentes principales con **PCA**.\n",
    "2. Calcular e interpretar la **varianza explicada** por cada componente.\n",
    "3. Generar un gráfico de dispersión en 2D donde cada punto represente un dígito, coloreado según su etiqueta (0–9).\n",
    "4. Responder:\n",
    "\n",
    "   * ¿Qué porcentaje de información se conserva en los 2 primeros componentes?\n",
    "   * ¿Se observan grupos definidos en el gráfico?\n",
    "\n",
    "---\n",
    "\n",
    "### **Parte 3: Análisis con t-SNE**\n",
    "\n",
    "1. Aplicar **t-SNE** para reducir las dimensiones a 2D.\n",
    "2. Configurar los parámetros clave:\n",
    "\n",
    "   * `perplexity = 30`\n",
    "   * `learning_rate = 200`\n",
    "   * `n_iter = 1000`\n",
    "3. Visualizar los resultados en un gráfico de dispersión coloreado por etiquetas.\n",
    "4. Responder:\n",
    "\n",
    "   * ¿Qué diferencias se observan frente al PCA?\n",
    "   * ¿Qué ventajas ofrece t-SNE en este caso?\n",
    "\n",
    "---\n",
    "\n",
    "### **Parte 4: Comparación y Conclusiones**\n",
    "\n",
    "1. Comparar visualmente los resultados de **PCA** y **t-SNE**.\n",
    "2. Discutir en grupo:\n",
    "\n",
    "   * ¿Cuál de las dos técnicas genera una separación más clara de los dígitos?\n",
    "   * ¿En qué contextos empresariales podría aplicarse cada técnica (ejemplo: segmentación de clientes, análisis de sensores, reconocimiento de imágenes, etc.)?\n",
    "3. Elaborar un breve informe (máximo 1 página) con conclusiones sobre:\n",
    "\n",
    "   * La utilidad de PCA.\n",
    "   * La utilidad de t-SNE.\n",
    "   * Sus principales diferencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Productos esperados\n",
    "\n",
    "* Código en Python bien estructurado y comentado.\n",
    "* Gráficos de dispersión comparativos de PCA y t-SNE.\n",
    "* Respuestas a las preguntas planteadas.\n",
    "* Informe de conclusiones.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ejemplo práctico: PCA y t-SNE en Ciencia de Datos\n",
    "-------------------------------------------------\n",
    "Este script aplica reducción de dimensionalidad utilizando:\n",
    "1. PCA (Principal Component Analysis).\n",
    "2. t-SNE (t-distributed Stochastic Neighbor Embedding).\n",
    "\n",
    "Dataset: dígitos manuscritos de sklearn (8x8 píxeles, 64 variables).\n",
    "Objetivo: visualizar cómo se distribuyen los datos en espacios reducidos.\n",
    "\n",
    "Buenas prácticas aplicadas:\n",
    "- Código comentado siguiendo PEP 8.\n",
    "- Uso de funciones para modularizar el análisis.\n",
    "- Reproducibilidad asegurada con random_state.\n",
    "- Manejo de errores y validación de entrada.\n",
    "- Parámetros configurables y métricas de rendimiento.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# Importación de librerías\n",
    "# =========================\n",
    "# Librerías estándar de Python\n",
    "import time                    # Para medir tiempo de ejecución\n",
    "import warnings               # Para mostrar advertencias\n",
    "from typing import Tuple, Optional  # Para anotaciones de tipo\n",
    "\n",
    "# Librerías de terceros para análisis de datos y visualización\n",
    "import matplotlib.pyplot as plt    # Para crear gráficos\n",
    "import numpy as np                 # Para operaciones numéricas\n",
    "import seaborn as sns             # Para gráficos estadísticos más bonitos\n",
    "from sklearn.datasets import load_digits      # Dataset de dígitos manuscritos\n",
    "from sklearn.decomposition import PCA         # Algoritmo PCA\n",
    "from sklearn.manifold import TSNE            # Algoritmo t-SNE\n",
    "from sklearn.preprocessing import StandardScaler  # Para normalizar datos\n",
    "\n",
    "# =========================\n",
    "# Constantes de configuración\n",
    "# =========================\n",
    "RANDOM_STATE = 42              # Semilla para reproducibilidad\n",
    "DEFAULT_PERPLEXITY = 30        # Perplejidad por defecto para t-SNE\n",
    "DEFAULT_LEARNING_RATE = 200    # Tasa de aprendizaje por defecto para t-SNE\n",
    "DEFAULT_N_ITER = 1000          # Número de iteraciones por defecto para t-SNE\n",
    "DEFAULT_FIGURE_SIZE = (8, 6)   # Tamaño por defecto de las figuras (ancho, alto)\n",
    "\n",
    "# =========================\n",
    "# Funciones auxiliares\n",
    "# =========================\n",
    "def validar_datos(X, y=None):\n",
    "    \"\"\"\n",
    "    Valida los datos de entrada para las operaciones de reducción de dimensionalidad.\n",
    "    :param X: Datos de entrada (matriz de características).\n",
    "    :param y: Etiquetas opcionales.\n",
    "    :return: True si los datos son válidos.\n",
    "    \"\"\"\n",
    "    # Verificar que los datos no estén vacíos\n",
    "    if X is None or len(X) == 0:\n",
    "        raise ValueError(\"Los datos de entrada no pueden estar vacíos.\")\n",
    "    \n",
    "    # Convertir a array de numpy si no lo es\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    \n",
    "    # Verificar que sea una matriz 2D (muestras x características)\n",
    "    if len(X.shape) != 2:\n",
    "        raise ValueError(\"Los datos deben ser una matriz 2D.\")\n",
    "    \n",
    "    # Verificar que las etiquetas coincidan con el número de muestras\n",
    "    if y is not None and len(y) != len(X):\n",
    "        raise ValueError(\"El número de etiquetas debe coincidir con el número de muestras.\")\n",
    "    \n",
    "    return True  # Si llegamos aquí, los datos son válidos\n",
    "\n",
    "\n",
    "def aplicar_pca(X, n_componentes=2):\n",
    "    \"\"\"\n",
    "    Aplica PCA para reducir dimensionalidad.\n",
    "    :param X: Datos de entrada (matriz de características).\n",
    "    :param n_componentes: Número de componentes principales.\n",
    "    :return: Tupla con (datos transformados, tiempo de ejecución).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validar que los datos de entrada sean correctos\n",
    "        validar_datos(X)\n",
    "        \n",
    "        # Verificar que no se pidan más componentes que características disponibles\n",
    "        if n_componentes > X.shape[1]:\n",
    "            warnings.warn(f\"n_componentes ({n_componentes}) mayor que el número de características ({X.shape[1]}). \"\n",
    "                         f\"Se ajustará a {X.shape[1]}.\")\n",
    "            n_componentes = X.shape[1]  # Ajustar al máximo posible\n",
    "        \n",
    "        # Iniciar medición de tiempo\n",
    "        inicio = time.time()\n",
    "        \n",
    "        # Crear objeto PCA con el número de componentes especificado\n",
    "        pca = PCA(n_components=n_componentes, random_state=RANDOM_STATE)\n",
    "        \n",
    "        # Aplicar PCA: ajustar el modelo y transformar los datos\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        # Calcular tiempo de ejecución\n",
    "        tiempo_ejecucion = time.time() - inicio\n",
    "        \n",
    "        # Mostrar información sobre la varianza explicada\n",
    "        print(f\"Varianza explicada por cada componente: {pca.explained_variance_ratio_}\")\n",
    "        print(f\"Varianza total explicada: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "        print(f\"Tiempo de ejecución PCA: {tiempo_ejecucion:.2f} segundos\")\n",
    "        \n",
    "        # Retornar datos transformados y tiempo de ejecución\n",
    "        return X_pca, tiempo_ejecucion\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Si hay algún error, mostrarlo y re-lanzarlo\n",
    "        print(f\"Error en PCA: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def aplicar_tsne(X, n_componentes=2, perplejidad=DEFAULT_PERPLEXITY, \n",
    "                learning_rate=DEFAULT_LEARNING_RATE, n_iter=DEFAULT_N_ITER):\n",
    "    \"\"\"\n",
    "    Aplica t-SNE para reducción de dimensionalidad.\n",
    "    :param X: Datos de entrada (matriz de características).\n",
    "    :param n_componentes: Dimensiones objetivo.\n",
    "    :param perplejidad: Balance entre densidad local/global.\n",
    "    :param learning_rate: Velocidad de aprendizaje.\n",
    "    :param n_iter: Número de iteraciones.\n",
    "    :return: Tupla con (datos transformados, tiempo de ejecución).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validar que los datos de entrada sean correctos\n",
    "        validar_datos(X)\n",
    "        \n",
    "        # Validar perplejidad: debe ser menor que el número de muestras\n",
    "        if perplejidad >= len(X):\n",
    "            perplejidad = min(perplejidad, len(X) - 1)  # Ajustar al máximo permitido\n",
    "            warnings.warn(f\"Perplejidad ajustada a {perplejidad} (máximo permitido: {len(X) - 1})\")\n",
    "        \n",
    "        # Iniciar medición de tiempo\n",
    "        inicio = time.time()\n",
    "        \n",
    "        # Crear objeto t-SNE con los parámetros especificados\n",
    "        tsne = TSNE(\n",
    "            n_components=n_componentes,      # Número de dimensiones de salida\n",
    "            perplexity=perplejidad,          # Controla el balance local/global\n",
    "            learning_rate=learning_rate,     # Velocidad de optimización\n",
    "            random_state=RANDOM_STATE,       # Para reproducibilidad\n",
    "            n_iter=n_iter                    # Número de iteraciones de optimización\n",
    "        )\n",
    "        \n",
    "        # Aplicar t-SNE: ajustar el modelo y transformar los datos\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "        \n",
    "        # Calcular tiempo de ejecución\n",
    "        tiempo_ejecucion = time.time() - inicio\n",
    "        \n",
    "        # Mostrar tiempo de ejecución\n",
    "        print(f\"Tiempo de ejecución t-SNE: {tiempo_ejecucion:.2f} segundos\")\n",
    "        \n",
    "        # Retornar datos transformados y tiempo de ejecución\n",
    "        return X_tsne, tiempo_ejecucion\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Si hay algún error, mostrarlo y re-lanzarlo\n",
    "        print(f\"Error en t-SNE: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def graficar_resultados(X_transformado, y, titulo, figura_size=DEFAULT_FIGURE_SIZE):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de dispersión coloreado por etiqueta.\n",
    "    :param X_transformado: Datos reducidos.\n",
    "    :param y: Etiquetas de clase.\n",
    "    :param titulo: Título del gráfico.\n",
    "    :param figura_size: Tamaño de la figura (ancho, alto).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validar que los datos de entrada sean correctos\n",
    "        validar_datos(X_transformado, y)\n",
    "        \n",
    "        # Obtener el número de componentes (dimensiones) de los datos transformados\n",
    "        n_componentes = X_transformado.shape[1]\n",
    "        \n",
    "        if n_componentes == 2:\n",
    "            # Visualización 2D: gráfico de dispersión simple\n",
    "            plt.figure(figsize=figura_size)  # Crear nueva figura con tamaño especificado\n",
    "            sns.scatterplot(\n",
    "                x=X_transformado[:, 0],      # Eje X: primera componente\n",
    "                y=X_transformado[:, 1],      # Eje Y: segunda componente\n",
    "                hue=y,                       # Colorear por etiqueta de clase\n",
    "                palette=\"tab10\",             # Paleta de colores para las clases\n",
    "                legend=\"full\",               # Mostrar leyenda completa\n",
    "                alpha=0.7                    # Transparencia de los puntos\n",
    "            )\n",
    "            plt.title(titulo, fontsize=14)   # Título del gráfico\n",
    "            plt.xlabel(\"Componente 1\")       # Etiqueta del eje X\n",
    "            plt.ylabel(\"Componente 2\")       # Etiqueta del eje Y\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Posición de la leyenda\n",
    "            \n",
    "        elif n_componentes == 3:\n",
    "            # Visualización 3D: gráfico de dispersión tridimensional\n",
    "            fig = plt.figure(figsize=figura_size)  # Crear figura\n",
    "            ax = fig.add_subplot(111, projection='3d')  # Agregar subplot 3D\n",
    "            \n",
    "            # Crear gráfico de dispersión 3D\n",
    "            scatter = ax.scatter(\n",
    "                X_transformado[:, 0],        # Coordenada X\n",
    "                X_transformado[:, 1],        # Coordenada Y\n",
    "                X_transformado[:, 2],        # Coordenada Z\n",
    "                c=y,                         # Colorear por etiqueta\n",
    "                cmap='tab10',                # Mapa de colores\n",
    "                alpha=0.7                    # Transparencia\n",
    "            )\n",
    "            ax.set_title(titulo, fontsize=14)  # Título\n",
    "            ax.set_xlabel(\"Componente 1\")      # Etiqueta eje X\n",
    "            ax.set_ylabel(\"Componente 2\")      # Etiqueta eje Y\n",
    "            ax.set_zlabel(\"Componente 3\")      # Etiqueta eje Z\n",
    "            \n",
    "        else:\n",
    "            # Para más de 3 dimensiones, mostrar solo las primeras 2\n",
    "            plt.figure(figsize=figura_size)\n",
    "            sns.scatterplot(\n",
    "                x=X_transformado[:, 0],      # Primera componente\n",
    "                y=X_transformado[:, 1],      # Segunda componente\n",
    "                hue=y,                       # Colorear por clase\n",
    "                palette=\"tab10\",\n",
    "                legend=\"full\",\n",
    "                alpha=0.7\n",
    "            )\n",
    "            # Título indicando que solo se muestran 2 de N componentes\n",
    "            plt.title(f\"{titulo} (primeras 2 componentes de {n_componentes})\", fontsize=14)\n",
    "            plt.xlabel(\"Componente 1\")\n",
    "            plt.ylabel(\"Componente 2\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()  # Ajustar layout para evitar solapamientos\n",
    "        plt.show()          # Mostrar el gráfico\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Si hay algún error, mostrarlo y re-lanzarlo\n",
    "        print(f\"Error en visualización: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Flujo principal del script\n",
    "# =========================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta el análisis de reducción de dimensionalidad.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Mostrar encabezado del análisis\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ANÁLISIS DE REDUCCIÓN DE DIMENSIONALIDAD\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Cargar dataset de dígitos manuscritos\n",
    "        print(\"\\n1. Cargando dataset de dígitos manuscritos...\")\n",
    "        digits = load_digits()        # Cargar dataset de sklearn\n",
    "        X = digits.data               # Matriz de características (1797 x 64)\n",
    "        y = digits.target             # Vector de etiquetas (1797,)\n",
    "\n",
    "        # Mostrar información básica del dataset\n",
    "        print(f\"Dimensiones originales: {X.shape}\")  # 1797 muestras, 64 variables\n",
    "        print(f\"Número de clases: {len(np.unique(y))}\")  # 10 clases (dígitos 0-9)\n",
    "\n",
    "        # 2. Escalado de datos (normalización estándar)\n",
    "        print(\"\\n2. Escalando datos...\")\n",
    "        scaler = StandardScaler()     # Crear objeto escalador\n",
    "        X_scaled = scaler.fit_transform(X)  # Ajustar y transformar datos\n",
    "        print(\"Escalado completado.\")\n",
    "\n",
    "        # 3. Aplicar PCA (Análisis de Componentes Principales)\n",
    "        print(\"\\n3. Aplicando PCA...\")\n",
    "        X_pca, tiempo_pca = aplicar_pca(X_scaled, n_componentes=2)  # Reducir a 2D\n",
    "        graficar_resultados(X_pca, y, \"Visualización con PCA\")      # Mostrar gráfico\n",
    "\n",
    "        # 4. Aplicar t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "        print(\"\\n4. Aplicando t-SNE...\")\n",
    "        X_tsne, tiempo_tsne = aplicar_tsne(X_scaled, n_componentes=2, \n",
    "                                          perplejidad=DEFAULT_PERPLEXITY, \n",
    "                                          learning_rate=DEFAULT_LEARNING_RATE)\n",
    "        graficar_resultados(X_tsne, y, \"Visualización con t-SNE\")   # Mostrar gráfico\n",
    "\n",
    "        # 5. Mostrar resumen de rendimiento\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"RESUMEN DE RENDIMIENTO\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Tiempo total PCA: {tiempo_pca:.2f} segundos\")\n",
    "        print(f\"Tiempo total t-SNE: {tiempo_tsne:.2f} segundos\")\n",
    "        print(f\"Tiempo total: {tiempo_pca + tiempo_tsne:.2f} segundos\")\n",
    "        print(\"\\nAnálisis completado exitosamente.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Si hay algún error, mostrarlo y re-lanzarlo\n",
    "        print(f\"Error en el análisis principal: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Punto de entrada del script\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Ejecutar función principal solo si se ejecuta directamente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf3e1f",
   "metadata": {},
   "source": [
    "# Enunciado del ejercicio\n",
    "\n",
    "## Tema: Reducción de Dimensionalidad Avanzada con PCA y t-SNE\n",
    "\n",
    "En esta práctica los estudiantes explorarán dos técnicas clave de reducción de dimensionalidad en Ciencia de Datos:\n",
    "\n",
    "1. **PCA (Principal Component Analysis)** → Técnica lineal que transforma los datos en componentes ortogonales maximizando la varianza explicada.\n",
    "2. **t-SNE (t-distributed Stochastic Neighbor Embedding)** → Técnica no lineal que preserva relaciones de vecindad y es muy útil para la visualización en 2D/3D de datos complejos.\n",
    "\n",
    "Se trabajará con datasets disponibles en **scikit-learn**:\n",
    "\n",
    "* **Digits** (1797 imágenes de dígitos manuscritos, 64 variables).\n",
    "* **Iris** (150 muestras, 4 variables, dataset pequeño para comparación).\n",
    "\n",
    "El objetivo es:\n",
    "\n",
    "* Entender cómo se reducen datos de alta dimensionalidad.\n",
    "* Comparar PCA vs t-SNE en visualización y agrupamiento.\n",
    "* Generar **gráficos interactivos** con **Plotly**.\n",
    "* Implementar un **Dashboard en Dash** para experimentar con parámetros.\n",
    "\n",
    "---\n",
    "\n",
    "# Estructura de carpetas recomendada\n",
    "\n",
    "```bash\n",
    "dimred_project/\n",
    "├── README.md                 # Descripción del proyecto y objetivos\n",
    "├── requirements.txt          # Librerías necesarias\n",
    "├── example_dimred.py         # Script principal (PCA y t-SNE con Plotly)\n",
    "├── dash_dimred_app.py        # Dashboard interactivo con Dash\n",
    "└── notebooks/\n",
    "    └── demo_dimred.ipynb     # Notebook para clase (explicación paso a paso)\n",
    "```\n",
    "\n",
    "* Esto da claridad:\n",
    "\n",
    "* `example_dimred.py` → código profesional reproducible.\n",
    "* `dash_dimred_app.py` → aplicación web simple para exploración interactiva.\n",
    "* `notebooks/` → versión educativa y explicada para los estudiantes.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475a8d0",
   "metadata": {},
   "source": [
    "# Archivo `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dash==2.17.1\n",
    "plotly==5.24.1\n",
    "scikit-learn==1.5.2\n",
    "pandas==2.2.3\n",
    "numpy==2.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48dff5",
   "metadata": {},
   "source": [
    "# Código principal: `example_dimred.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aaf496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ejemplo: PCA y t-SNE con visualizaciones Plotly\n",
    "------------------------------------------------\n",
    "Este script aplica reducción de dimensionalidad usando PCA y t-SNE\n",
    "en datasets de sklearn (Digits, Iris). Incluye:\n",
    "- Buenas prácticas (PEP8, typing, docstrings).\n",
    "- Logging para trazabilidad.\n",
    "- Gráficos interactivos con Plotly.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits, load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import plotly.express as px\n",
    "\n",
    "# =====================\n",
    "# Configuración logging\n",
    "# =====================\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Definición de Dataset\n",
    "# =====================\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    X: np.ndarray\n",
    "    y: np.ndarray\n",
    "    feature_names: list[str]\n",
    "    target_names: list[str]\n",
    "\n",
    "\n",
    "def load_dataset(name: str = \"digits\") -> Dataset:\n",
    "    \"\"\"\n",
    "    Carga dataset de sklearn: 'digits' o 'iris'.\n",
    "    \"\"\"\n",
    "    if name == \"digits\":\n",
    "        data = load_digits()\n",
    "    elif name == \"iris\":\n",
    "        data = load_iris()\n",
    "    else:\n",
    "        raise ValueError(\"Dataset no soportado. Usa 'digits' o 'iris'.\")\n",
    "\n",
    "    X = data[\"data\"]\n",
    "    y = data[\"target\"]\n",
    "    feature_names = list(data.get(\"feature_names\", [f\"f{i}\" for i in range(X.shape[1])]))\n",
    "    target_names = list(map(str, data.get(\"target_names\", np.unique(y).astype(str))))\n",
    "    logger.info(\"Dataset %s cargado: X=%s, y=%s\", name, X.shape, y.shape)\n",
    "    return Dataset(X=X, y=y, feature_names=feature_names, target_names=target_names)\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Funciones PCA y t-SNE\n",
    "# =====================\n",
    "def compute_pca(X: np.ndarray,\n",
    "                n_components: int = 2,\n",
    "                random_state: int | None = 42) -> Tuple[np.ndarray, PCA]:\n",
    "    \"\"\"\n",
    "    Escala datos y aplica PCA. Retorna embedding y el objeto PCA.\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=n_components, random_state=random_state))\n",
    "    ])\n",
    "    X_trans = pipeline.fit_transform(X)\n",
    "    pca_obj: PCA = pipeline.named_steps[\"pca\"]\n",
    "    logger.info(\"PCA varianza explicada: %s\", pca_obj.explained_variance_ratio_[:5])\n",
    "    return X_trans, pca_obj\n",
    "\n",
    "\n",
    "def compute_tsne(X: np.ndarray,\n",
    "                 n_components: int = 2,\n",
    "                 perplexity: float = 30.0,\n",
    "                 learning_rate: float = 200.0,\n",
    "                 random_state: int | None = 42,\n",
    "                 init_shape: str = \"pca\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Aplica t-SNE (recomendado escalar y usar init='pca').\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=n_components,\n",
    "        perplexity=perplexity,\n",
    "        learning_rate=learning_rate,\n",
    "        init=init_shape,\n",
    "        random_state=random_state,\n",
    "        n_iter=1000\n",
    "    )\n",
    "    embedding = tsne.fit_transform(Xs)\n",
    "    logger.info(\"t-SNE terminado: embedding %s\", embedding.shape)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Funciones de gráficos\n",
    "# =====================\n",
    "def plot_embedding(df: pd.DataFrame, title: str = \"Embedding 2D\") -> None:\n",
    "    \"\"\"\n",
    "    Gráfico scatter interactivo con Plotly Express.\n",
    "    \"\"\"\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\",\n",
    "                     hover_data=df.columns.tolist(), title=title)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Flujo demo\n",
    "# =====================\n",
    "def demo_flow():\n",
    "    \"\"\"Flujo demo para ejecutar en clase.\"\"\"\n",
    "    ds = load_dataset(\"digits\")\n",
    "    X, y = ds.X, ds.y\n",
    "\n",
    "    # PCA 2D\n",
    "    X_pca2, pca2 = compute_pca(X, n_components=2)\n",
    "    df_pca = pd.DataFrame({\"x\": X_pca2[:, 0], \"y\": X_pca2[:, 1], \"label\": y})\n",
    "    plot_embedding(df_pca, title=\"Digits - PCA (2D)\")\n",
    "\n",
    "    # Varianza explicada\n",
    "    X_pca_full, pca_full = compute_pca(X, n_components=min(30, X.shape[1]))\n",
    "    explained = pca_full.explained_variance_ratio_\n",
    "    df_ev = pd.DataFrame({\"component\": range(1, len(explained) + 1),\n",
    "                          \"explained_variance\": explained})\n",
    "    px.bar(df_ev, x=\"component\", y=\"explained_variance\",\n",
    "           title=\"Explained variance per component\").show()\n",
    "\n",
    "    # t-SNE\n",
    "    emb_tsne = compute_tsne(X, n_components=2, perplexity=30, learning_rate=200, init_shape=\"pca\")\n",
    "    df_tsne = pd.DataFrame({\"x\": emb_tsne[:, 0], \"y\": emb_tsne[:, 1], \"label\": y})\n",
    "    plot_embedding(df_tsne, title=\"Digits - t-SNE (pca init)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_flow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102e3a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dashboard interactivo: `dash_dimred_app.py`\n",
    "\n",
    "Este archivo permite jugar con parámetros de PCA y t-SNE en un entorno web. Cómo correrlo: `python dash_dimred_app.py`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dash_dimred_app.py\n",
    "Aplicación Dash para reducción de dimensionalidad con PCA y t-SNE\n",
    "usando el dataset Digits de sklearn.\n",
    "\n",
    "Autor: [Tu Nombre]\n",
    "Curso: Técnicas avanzadas en Ciencia de Datos\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# Importación de librerías\n",
    "# ===============================\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# Carga del dataset\n",
    "# ===============================\n",
    "digits = load_digits()\n",
    "X = digits.data  # variables (64 dimensiones)\n",
    "y = digits.target  # etiquetas (0–9)\n",
    "\n",
    "# ===============================\n",
    "# Inicialización de la aplicación\n",
    "# ===============================\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server  # para despliegue en producción (ej. Heroku)\n",
    "\n",
    "# ===============================\n",
    "# Layout de la app\n",
    "# ===============================\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Reducción de Dimensionalidad: PCA vs t-SNE\", \n",
    "            style={\"textAlign\": \"center\"}),\n",
    "\n",
    "    html.Div([\n",
    "        html.Label(\"Seleccione técnica de reducción:\"),\n",
    "        dcc.Dropdown(\n",
    "            id=\"method-dropdown\",\n",
    "            options=[\n",
    "                {\"label\": \"PCA\", \"value\": \"PCA\"},\n",
    "                {\"label\": \"t-SNE\", \"value\": \"TSNE\"}\n",
    "            ],\n",
    "            value=\"PCA\",\n",
    "            clearable=False\n",
    "        )\n",
    "    ], style={\"width\": \"40%\", \"margin\": \"auto\"}),\n",
    "\n",
    "    dcc.Graph(id=\"scatter-plot\", style={\"height\": \"80vh\"})\n",
    "])\n",
    "\n",
    "# ===============================\n",
    "# Callbacks\n",
    "# ===============================\n",
    "@app.callback(\n",
    "    Output(\"scatter-plot\", \"figure\"),\n",
    "    [Input(\"method-dropdown\", \"value\")]\n",
    ")\n",
    "def update_graph(method):\n",
    "    \"\"\"\n",
    "    Callback para actualizar el gráfico de dispersión según\n",
    "    la técnica seleccionada (PCA o t-SNE).\n",
    "    \"\"\"\n",
    "\n",
    "    if method == \"PCA\":\n",
    "        reducer = PCA(n_components=2)\n",
    "        reduced_data = reducer.fit_transform(X)\n",
    "    else:\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        reduced_data = reducer.fit_transform(X)\n",
    "\n",
    "    # Crear DataFrame para Plotly\n",
    "    df = pd.DataFrame({\n",
    "        \"Component 1\": reduced_data[:, 0],\n",
    "        \"Component 2\": reduced_data[:, 1],\n",
    "        \"Digit\": y\n",
    "    })\n",
    "\n",
    "    # Gráfico interactivo\n",
    "    fig = px.scatter(\n",
    "        df, x=\"Component 1\", y=\"Component 2\",\n",
    "        color=df[\"Digit\"].astype(str),\n",
    "        title=f\"Visualización con {method}\",\n",
    "        labels={\"Digit\": \"Etiqueta del dígito\"},\n",
    "        opacity=0.7\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Main\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62fd7c",
   "metadata": {},
   "source": [
    "### Cómo usarlo\n",
    "\n",
    "1. Guarda el archivo como `dash_dimred_app.py`.\n",
    "2. Instala dependencias si no las tienes:\n",
    "\n",
    "   ```bash\n",
    "   pip install dash plotly scikit-learn pandas\n",
    "   ```\n",
    "3. Ejecuta:\n",
    "\n",
    "   ```bash\n",
    "   python dash_dimred_app.py\n",
    "   ```\n",
    "4. Abre en el navegador `http://127.0.0.1:8050`\n",
    "\n",
    "Tendrás una aplicación donde puedes elegir **PCA** o **t-SNE** en un dropdown y ver el resultado de la reducción de dimensionalidad en un gráfico interactivo.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
