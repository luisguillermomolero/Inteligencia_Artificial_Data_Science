{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61f8dcd",
   "metadata": {},
   "source": [
    "# Clase 3 — Desarrollo de aplicaciones con Visión por Computador (3 horas)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar la sesión, los estudiantes serán capaces de:\n",
    "\n",
    "1. Diseñar una API REST segura y eficiente para servir modelos de visión por computador.\n",
    "2. Implementar endpoints para clasificación y detección que sigan buenas prácticas (validación, manejo de errores, tipado).\n",
    "3. Exportar un modelo PyTorch a TorchScript u ONNX y explicar las ventajas de cada formato para producción.\n",
    "4. Contenerizar la aplicación con Docker y describir consideraciones para despliegue en producción.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76496ba4",
   "metadata": {},
   "source": [
    "## Requisitos previos y recursos\n",
    "\n",
    "* Python 3.9+\n",
    "* PyTorch + torchvision\n",
    "* FastAPI, Uvicorn\n",
    "* Docker (para la parte de contenerización)\n",
    "\n",
    "Ejemplo mínimo de `requirements.txt`:\n",
    "\n",
    "```\n",
    "fastapi>=0.95\n",
    "uvicorn[standard]>=0.20\n",
    "torch>=1.12\n",
    "torchvision>=0.13\n",
    "pydantic>=1.10\n",
    "numpy>=1.23\n",
    "python-multipart>=0.0.5\n",
    "pytest>=7.0\n",
    "requests>=2.28\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ede39",
   "metadata": {},
   "source": [
    "## Parte teórica\n",
    "\n",
    "### 1) ¿Por qué separar modelo y aplicación?\n",
    "\n",
    "* **Separación de responsabilidades**: modelo (inferencia) vs API (validación, seguridad, logging).\n",
    "* **Escalabilidad**: independizar el servicio permite escalar el modelo por separado (más réplicas o GPU nodes).\n",
    "* **Observabilidad**: el servicio web permite instrumentación (métricas, logs, traces).\n",
    "\n",
    "### 2) Opciones de serving\n",
    "\n",
    "* **FastAPI**: rápido de desarrollar, documentación automática (OpenAPI), ideal para PoC y microservicios. Permite endpoints sin complicaciones.\n",
    "* **TorchServe**: diseñado para modelos PyTorch en producción; soporta batching, métricas y gestion de endpoints con modelos múltiples.\n",
    "* **TensorFlow Serving / TF-Serving**: equivalente para modelos TensorFlow.\n",
    "* **ONNX Runtime**: para ejecutar modelos exportados a ONNX con alto rendimiento en CPU y aceleradores.\n",
    "\n",
    "**Regla**: para prototipos y clases use FastAPI; para producción con necesidades de escalado/batching considere TorchServe/ONNX.\n",
    "\n",
    "### 3) Requisitos de diseño de API\n",
    "\n",
    "* Endpoints claros: `/health`, `/predict/classify`, `/predict/detect`.\n",
    "* Validación de entrada con Pydantic (tamaños máximos, tipos, formatos MIME permitidos).\n",
    "* Soporte para subida de archivos (`multipart/form-data`) y payloads base64.\n",
    "* Respuestas JSON estandarizadas (clave `status`, `predictions`, `error`, `meta`).\n",
    "\n",
    "### 4) Seguridad y privacidad mínima\n",
    "\n",
    "* Limitar tamaño de subida (`max_content_length`), evitar DoS por archivos grandes.\n",
    "* Autenticación: JWT / API keys para acceso a la API en producción.\n",
    "* Si se procesan imágenes con datos personales: anonimización, políticas de retención y cumplimiento (GDPR/HIPAA según correspondencia).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c388ff",
   "metadata": {},
   "source": [
    "## Laboratorio práctico — Integración de una CNN en un servicio web\n",
    "\n",
    "### Objetivo práctico\n",
    "\n",
    "Construir un servicio web (FastAPI) que:\n",
    "\n",
    "1. Cargue un modelo PyTorch (preentrenado o fine-tuned).\n",
    "2. Exponga un endpoint `/predict/classify` que acepte una imagen y devuelva la clase y probabilidades.\n",
    "3. (Opcional) Exponga `/predict/detect` que devuelva bounding boxes y scores usando un detector preentrenado.\n",
    "\n",
    "### Estructura de archivos sugerida\n",
    "\n",
    "```\n",
    "cv-app/\n",
    "├── app/\n",
    "│   ├── main.py            # FastAPI app\n",
    "│   ├── model.py           # carga, preproc y postproc del modelo\n",
    "│   └── schemas.py         # Pydantic models para requests/responses\n",
    "├── requirements.txt\n",
    "```\n",
    "\n",
    "> Recomendación: mantenga la lógica de preprocesado y postprocesado en `model.py` para facilitar tests y posible exportación.\n",
    "\n",
    "### Código: `app/model.py` (ejemplo para clasificación con ResNet18)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/model.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "\n",
    "logger = logging.getLogger(\"cv_app.model\")\n",
    "\n",
    "class ImageClassifier:\n",
    "    def __init__(self, device: str = \"cpu\") -> None:\n",
    "        self.device = device\n",
    "        self.model = self._load_model()\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.labels = self._load_imagenet_labels()\n",
    "        # Imagenet mean/std para pre-trained models (optimizado para MobileNet)\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(224),  # Reducido de 256 a 224 directamente\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def _load_model(self) -> torch.nn.Module:\n",
    "        logger.info(\"Descargando modelo MobileNet (ligero)...\")\n",
    "        model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "        logger.info(\"Modelo MobileNet descargado y cargado\")\n",
    "        # si tiene que cambiar la cabeza para num_classes != 1000, hacerlo aquí\n",
    "        return model\n",
    "\n",
    "    def _load_imagenet_labels(self) -> List[str]:\n",
    "        \"\"\"Carga las etiquetas de ImageNet de forma simple\"\"\"\n",
    "        try:\n",
    "            # Usar las etiquetas que vienen con el modelo (más confiable)\n",
    "            from torchvision.models import get_model_weights\n",
    "            weights = get_model_weights('mobilenet_v3_small')\n",
    "            labels = weights.IMAGENET1K_V1.meta['categories']\n",
    "            logger.info(f\"Cargadas {len(labels)} etiquetas desde weights\")\n",
    "            return labels\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"No se pudieron cargar las etiquetas desde weights: {e}\")\n",
    "            # Fallback: etiquetas genéricas más descriptivas\n",
    "            return [f\"objeto_{i}\" for i in range(1000)]\n",
    "\n",
    "    def preprocess(self, image_bytes: bytes) -> torch.Tensor:\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        x = self.transform(image).unsqueeze(0)  # batch dimension\n",
    "        return x.to(self.device)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, image_bytes: bytes, topk: int = 5) -> List[Tuple[str, float]]:\n",
    "        x = self.preprocess(image_bytes)\n",
    "        logits = self.model(x)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        top_probs, top_idxs = probs.topk(topk, dim=1)\n",
    "        top_probs = top_probs.cpu().numpy().flatten().tolist()\n",
    "        top_idxs = top_idxs.cpu().numpy().flatten().tolist()\n",
    "        # Mapear índices a etiquetas reales de ImageNet\n",
    "        labels = [self.labels[idx] for idx in top_idxs]\n",
    "        return list(zip(labels, top_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675a3a1",
   "metadata": {},
   "source": [
    "### Código: `app/schemas.py` (Pydantic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb04f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/schemas.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Tuple\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    label: str\n",
    "    score: float\n",
    "\n",
    "class ClassifyResponse(BaseModel):\n",
    "    status: str = Field(\"ok\")\n",
    "    predictions: List[Prediction]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50920479",
   "metadata": {},
   "source": [
    "### Código: `app/main.py` (FastAPI app)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app/main.py\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import logging\n",
    "\n",
    "from model.model import ImageClassifier\n",
    "from schemas.schema import ClassifyResponse, Prediction\n",
    "\n",
    "logger = logging.getLogger(\"cv_app\")\n",
    "app = FastAPI(title=\"CV Model API\", version=\"0.1\")\n",
    "\n",
    "# cargamos el modelo de forma lazy (cuando se necesite)\n",
    "classifier: ImageClassifier | None = None\n",
    "\n",
    "def get_classifier():\n",
    "    global classifier\n",
    "    if classifier is None:\n",
    "        logger.info(\"Cargando modelo por primera vez...\")\n",
    "        classifier = ImageClassifier(device=\"cpu\")\n",
    "        logger.info(\"Modelo cargado exitosamente\")\n",
    "    return classifier\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    \"\"\"Endpoint raíz con información de la API\"\"\"\n",
    "    return {\n",
    "        \"message\": \"API de Clasificación de Imágenes con IA\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"description\": \"Clasifica imágenes usando MobileNet pre-entrenado en ImageNet\",\n",
    "        \"endpoints\": {\n",
    "            \"health\": \"/health - Estado del servidor\",\n",
    "            \"classify\": \"/predict/classify - Clasificar una imagen\",\n",
    "            \"test_labels\": \"/test-labels - Ver etiquetas del modelo\",\n",
    "            \"docs\": \"/docs - Documentación interactiva\"\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"method\": \"POST\",\n",
    "            \"endpoint\": \"/predict/classify\",\n",
    "            \"content_type\": \"multipart/form-data\",\n",
    "            \"parameter\": \"file (imagen)\"\n",
    "        },\n",
    "        \"example\": \"Sube una imagen a /predict/classify para obtener las 3 predicciones más probables\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.get(\"/test-labels\")\n",
    "def test_labels():\n",
    "    \"\"\"Endpoint para probar las etiquetas del modelo\"\"\"\n",
    "    classifier = get_classifier()\n",
    "    # Mostrar las primeras 10 etiquetas\n",
    "    sample_labels = classifier.labels[:10]\n",
    "    return {\"labels_sample\": sample_labels, \"total_labels\": len(classifier.labels)}\n",
    "\n",
    "@app.post(\"/predict/classify\", response_model=ClassifyResponse)\n",
    "async def predict_classify(file: UploadFile = File(...)):\n",
    "    if not file.content_type.startswith(\"image/\"):\n",
    "        raise HTTPException(status_code=400, detail=\"Archivo no es una imagen\")\n",
    "    image_bytes = await file.read()\n",
    "    if len(image_bytes) == 0:\n",
    "        raise HTTPException(status_code=400, detail=\"Archivo vacío\")\n",
    "    classifier = get_classifier()  # Carga el modelo solo cuando se necesita\n",
    "    preds = classifier.predict(image_bytes, topk=3)  # Reducido de 5 a 3 predicciones\n",
    "    response = ClassifyResponse(predictions=[Prediction(label=p[0], score=p[1]) for p in preds])\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81dd72",
   "metadata": {},
   "source": [
    "# Ahora...\n",
    "\n",
    "## Crear la carpeta /img y guardar las imagenes para probar el modelo\n",
    "\n",
    "## Ejecutar la aplicación\n",
    "```\n",
    "uvicorn main:app --reload --host 127.0.0.1 --port 8000\n",
    "```\n",
    "### **2. En el navegador web:**\n",
    "- **Documentación**: `http://127.0.0.1:8000/docs`\n",
    "- **API principal**: `http://127.0.0.1:8000`\n",
    "- **Health check**: `http://127.0.0.1:8000/health`\n",
    "\n",
    "### **3. Para probar la clasificación de imágenes:**\n",
    "\n",
    "1. **Abre tu navegador**\n",
    "2. **Ve a**: `http://127.0.0.1:8000/docs`\n",
    "3. **Verás la interfaz de Swagger** con los endpoints\n",
    "4. **Haz clic en** `/predict/classify`\n",
    "5. **Haz clic en** \"Try it out\"\n",
    "6. **Sube una imagen** y haz clic en \"Execute\"\n",
    "\n",
    "### **4. La consola mostrará:**\n",
    "- Cuando alguien haga una petición\n",
    "- Los logs de carga del modelo (primera vez)\n",
    "- Los logs de procesamiento de imágenes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa0804",
   "metadata": {},
   "source": [
    "## Lecturas y recursos\n",
    "\n",
    "* FastAPI docs (Auto-generated OpenAPI).\n",
    "* PyTorch/TorchVision model zoo.\n",
    "* TorchServe docs y ejemplos para serving en producción.\n",
    "* ONNX Runtime docs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
