{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61f8dcd",
   "metadata": {},
   "source": [
    "# Clase 3 — Desarrollo de aplicaciones con Visión por Computador\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar la sesión, los estudiantes serán capaces de:\n",
    "\n",
    "1. Diseñar una API REST segura y eficiente para servir modelos de visión por computador.\n",
    "2. Implementar endpoints para clasificación y detección que sigan buenas prácticas (validación, manejo de errores, tipado).\n",
    "3. Exportar un modelo PyTorch a TorchScript u ONNX y explicar las ventajas de cada formato para producción.\n",
    "4. Contenerizar la aplicación con Docker y describir consideraciones para despliegue en producción.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76496ba4",
   "metadata": {},
   "source": [
    "## Requisitos previos y recursos\n",
    "\n",
    "* Python 3.9+\n",
    "* PyTorch + torchvision\n",
    "* FastAPI, Uvicorn\n",
    "\n",
    "Ejemplo mínimo de \n",
    "\n",
    "`requirements.txt`:\n",
    "\n",
    "```\n",
    "fastapi>=0.95\n",
    "uvicorn[standard]>=0.20\n",
    "torch>=1.12\n",
    "torchvision>=0.13\n",
    "pydantic>=1.10\n",
    "numpy>=1.23\n",
    "python-multipart>=0.0.5\n",
    "pytest>=7.0\n",
    "requests>=2.28\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ede39",
   "metadata": {},
   "source": [
    "## Parte teórica\n",
    "\n",
    "### 1) ¿Por qué separar modelo y aplicación?\n",
    "\n",
    "* **Separación de responsabilidades**: modelo (inferencia) vs API (validación, seguridad, logging).\n",
    "* **Escalabilidad**: independizar el servicio permite escalar el modelo por separado (más réplicas o GPU nodes).\n",
    "* **Observabilidad**: el servicio web permite instrumentación (métricas, logs, traces).\n",
    "\n",
    "### 2) Opciones de serving\n",
    "\n",
    "* **FastAPI**: rápido de desarrollar, documentación automática (OpenAPI), ideal para PoC y microservicios. Permite endpoints sin complicaciones.\n",
    "* **TorchServe**: diseñado para modelos PyTorch en producción; soporta batching, métricas y gestion de endpoints con modelos múltiples.\n",
    "* **TensorFlow Serving / TF-Serving**: equivalente para modelos TensorFlow.\n",
    "* **ONNX Runtime**: para ejecutar modelos exportados a ONNX con alto rendimiento en CPU y aceleradores.\n",
    "\n",
    "**Regla**: para prototipos y clases use FastAPI; para producción con necesidades de escalado/batching considere TorchServe/ONNX.\n",
    "\n",
    "### 3) Requisitos de diseño de API\n",
    "\n",
    "* Endpoints claros: `/health`, `/predict/classify`, `/predict/detect`.\n",
    "* Validación de entrada con Pydantic (tamaños máximos, tipos, formatos MIME permitidos).\n",
    "* Soporte para subida de archivos (`multipart/form-data`) y payloads base64.\n",
    "* Respuestas JSON estandarizadas (clave `status`, `predictions`, `error`, `meta`).\n",
    "\n",
    "### 4) Seguridad y privacidad mínima\n",
    "\n",
    "* Limitar tamaño de subida (`max_content_length`), evitar DoS por archivos grandes.\n",
    "* Autenticación: JWT / API keys para acceso a la API en producción.\n",
    "* Si se procesan imágenes con datos personales: anonimización, políticas de retención y cumplimiento (GDPR/HIPAA según correspondencia).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c388ff",
   "metadata": {},
   "source": [
    "## Laboratorio práctico — Integración de una CNN en un servicio web\n",
    "\n",
    "### Objetivo práctico\n",
    "\n",
    "Construir un servicio web (FastAPI) que:\n",
    "\n",
    "1. Cargue un modelo PyTorch (preentrenado o fine-tuned).\n",
    "2. Exponga un endpoint `/predict/classify` que acepte una imagen y devuelva la clase y probabilidades.\n",
    "3. (Opcional) Exponga `/predict/detect` que devuelva bounding boxes y scores usando un detector preentrenado.\n",
    "\n",
    "### Estructura de archivos sugerida\n",
    "\n",
    "```\n",
    "cv-app/\n",
    "├── app/\n",
    "│   ├── main.py            # FastAPI app\n",
    "│   ├── model.py           # carga, preproc y postproc del modelo\n",
    "│   └── schemas.py         # Pydantic models para requests/responses\n",
    "├── requirements.txt\n",
    "```\n",
    "\n",
    "> Recomendación: mantenga la lógica de preprocesado y postprocesado en `model.py` para facilitar tests y posible exportación.\n",
    "\n",
    "### Código: `app/model.py` (ejemplo para clasificación con ResNet18)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# app/model.py — Módulo del modelo de clasificación de imágenes\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Permite compatibilidad futura con anotaciones de tipo (Python 3.7+)\n",
    "from __future__ import annotations\n",
    "\n",
    "# Importaciones estándar\n",
    "import io          # Para manipular datos binarios (como imágenes en bytes)\n",
    "import logging     # Para registrar mensajes informativos, advertencias y errores\n",
    "from typing import List, Tuple  # Tipos usados para mayor claridad en el código\n",
    "import json        # Librería estándar para manejar datos en formato JSON\n",
    "\n",
    "# Librerías de procesamiento científico y deep learning\n",
    "import numpy as np                  # Para operaciones numéricas y manejo de arrays\n",
    "import torch                        # Framework de aprendizaje profundo PyTorch\n",
    "import torchvision.transforms as T  # Módulo de transformaciones de imágenes\n",
    "from PIL import Image               # PIL (Pillow): para manipulación de imágenes\n",
    "from torchvision import models       # Modelos preentrenados y arquitecturas CNN\n",
    "\n",
    "# Crear un logger específico para este módulo\n",
    "logger = logging.getLogger(\"cv_app.model\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Clase principal: ImageClassifier\n",
    "# ------------------------------------------------------------\n",
    "class ImageClassifier:\n",
    "    \"\"\"\n",
    "    Clase que encapsula la lógica de un modelo de clasificación de imágenes.\n",
    "    Utiliza MobileNetV3 preentrenado en ImageNet para clasificar imágenes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device: str = \"cpu\") -> None:\n",
    "        \"\"\"\n",
    "        Constructor del clasificador.\n",
    "        Carga el modelo, las etiquetas y define las transformaciones.\n",
    "        \"\"\"\n",
    "        self.device = device  # CPU o GPU según disponibilidad\n",
    "\n",
    "        # Cargar el modelo preentrenado\n",
    "        self.model = self._load_model()\n",
    "\n",
    "        # Mover el modelo al dispositivo correspondiente (CPU o GPU)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Poner el modelo en modo evaluación (no entrenamiento)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Cargar las etiquetas de las clases de ImageNet\n",
    "        self.labels = self._load_imagenet_labels()\n",
    "\n",
    "        # Definir las transformaciones de preprocesamiento de imágenes\n",
    "        # Estas son las normalizaciones estándar usadas por ImageNet\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(224),  # Redimensiona la imagen al tamaño 224x224\n",
    "            T.ToTensor(),   # Convierte la imagen a tensor de PyTorch (valores [0,1])\n",
    "            T.Normalize(    # Normaliza los valores RGB con la media y desviación típicas\n",
    "                mean=[0.485, 0.456, 0.406],   # Medias por canal RGB\n",
    "                std=[0.229, 0.224, 0.225]     # Desviaciones estándar por canal\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Función interna para cargar el modelo\n",
    "    # --------------------------------------------------------\n",
    "    def _load_model(self) -> torch.nn.Module:\n",
    "        \"\"\"\n",
    "        Descarga y carga el modelo preentrenado MobileNetV3-Small.\n",
    "        Es un modelo liviano y eficiente para clasificación de imágenes.\n",
    "        \"\"\"\n",
    "        logger.info(\"Descargando modelo MobileNet (ligero)...\")\n",
    "\n",
    "        # Se descarga y carga el modelo con pesos preentrenados en ImageNet\n",
    "        model = models.mobilenet_v3_small(\n",
    "            weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "\n",
    "        logger.info(\"Modelo MobileNet descargado y cargado correctamente\")\n",
    "\n",
    "        # Si se quisiera adaptar el modelo a otro número de clases,\n",
    "        # aquí se modificaría la capa de salida (classifier).\n",
    "        return model\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Cargar etiquetas de ImageNet\n",
    "    # --------------------------------------------------------\n",
    "    def _load_imagenet_labels(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Carga las etiquetas (nombres de clases) del dataset ImageNet.\n",
    "        Estas etiquetas están incluidas dentro de los pesos del modelo.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Se obtienen los pesos del modelo MobileNetV3\n",
    "            from torchvision.models import get_model_weights\n",
    "\n",
    "            # Cargar las etiquetas desde los metadatos de los pesos\n",
    "            weights = get_model_weights('mobilenet_v3_small')\n",
    "            labels = weights.IMAGENET1K_V1.meta['categories']\n",
    "\n",
    "            logger.info(f\"Cargadas {len(labels)} etiquetas desde weights preentrenados\")\n",
    "            return labels\n",
    "\n",
    "        except Exception as e:\n",
    "            # Si ocurre un error (por ejemplo, sin conexión a internet)\n",
    "            logger.warning(f\"No se pudieron cargar las etiquetas desde weights: {e}\")\n",
    "\n",
    "            # Se devuelven etiquetas genéricas de respaldo (objeto_0, objeto_1, …)\n",
    "            return [f\"objeto_{i}\" for i in range(1000)]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Preprocesamiento de imágenes\n",
    "    # --------------------------------------------------------\n",
    "    def preprocess(self, image_bytes: bytes) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convierte los bytes de una imagen en un tensor listo para el modelo.\n",
    "        - Decodifica los bytes a imagen RGB\n",
    "        - Aplica transformaciones (resize, normalización, tensor)\n",
    "        - Añade una dimensión batch (1 x 3 x 224 x 224)\n",
    "        \"\"\"\n",
    "        # Abre la imagen desde bytes usando PIL y la convierte a RGB\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "        # Aplica las transformaciones definidas (resize, normalize, etc.)\n",
    "        x = self.transform(image).unsqueeze(0)  # unsqueeze añade la dimensión batch\n",
    "\n",
    "        # Devuelve el tensor en el dispositivo correcto (CPU o GPU)\n",
    "        return x.to(self.device)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Predicción\n",
    "    # --------------------------------------------------------\n",
    "    @torch.inference_mode()  # Desactiva gradientes para optimizar inferencia\n",
    "    def predict(self, image_bytes: bytes, topk: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Realiza la predicción de clase sobre una imagen.\n",
    "        Devuelve las top-K clases más probables con sus probabilidades.\n",
    "        \"\"\"\n",
    "        # Preprocesar la imagen (de bytes a tensor normalizado)\n",
    "        x = self.preprocess(image_bytes)\n",
    "\n",
    "        # Pasar la imagen por el modelo (forward pass)\n",
    "        logits = self.model(x)\n",
    "\n",
    "        # Aplicar softmax para convertir logits en probabilidades\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        # Seleccionar las K clases con mayor probabilidad\n",
    "        top_probs, top_idxs = probs.topk(topk, dim=1)\n",
    "\n",
    "        # Convertir resultados a listas de Python para facilidad de uso\n",
    "        top_probs = top_probs.cpu().numpy().flatten().tolist()\n",
    "        top_idxs = top_idxs.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        # Mapear los índices a las etiquetas reales de ImageNet\n",
    "        labels = [self.labels[idx] for idx in top_idxs]\n",
    "\n",
    "        # Devolver pares (etiqueta, probabilidad)\n",
    "        return list(zip(labels, top_probs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675a3a1",
   "metadata": {},
   "source": [
    "### Código: `app/schemas.py` (Pydantic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb04f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# app/schemas.py — Definición de esquemas de datos con Pydantic\n",
    "# ------------------------------------------------------------\n",
    "# En este módulo se definen las estructuras de datos (modelos)\n",
    "# que FastAPI usa para validar, documentar y serializar la entrada/salida\n",
    "# de la API. Los esquemas aseguran que los datos sigan un formato consistente.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Importaciones necesarias\n",
    "# ------------------------------------------------------------\n",
    "from pydantic import BaseModel, Field  # Base para crear modelos de validación\n",
    "from typing import List, Tuple         # Tipos de datos para listas y tuplas\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Modelo Prediction\n",
    "# ------------------------------------------------------------\n",
    "class Prediction(BaseModel):\n",
    "    \"\"\"\n",
    "    Representa una predicción individual del modelo de clasificación.\n",
    "    Cada predicción tiene:\n",
    "    - label: nombre de la clase predicha (por ejemplo, 'perro', 'auto')\n",
    "    - score: probabilidad asociada a esa predicción (float entre 0 y 1)\n",
    "    \"\"\"\n",
    "    label: str    # Nombre de la clase predicha\n",
    "    score: float  # Probabilidad asociada a la predicción\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Modelo ClassifyResponse\n",
    "# ------------------------------------------------------------\n",
    "class ClassifyResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Define la estructura de la respuesta JSON del endpoint de clasificación.\n",
    "    Incluye:\n",
    "    - status: texto que indica el estado de la petición (por defecto \"ok\")\n",
    "    - predictions: lista de objetos Prediction (las predicciones del modelo)\n",
    "    \"\"\"\n",
    "    status: str = Field(\"ok\")              # Campo con valor por defecto \"ok\"\n",
    "    predictions: List[Prediction]          # Lista de predicciones generadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50920479",
   "metadata": {},
   "source": [
    "### Código: `app/main.py` (FastAPI app)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# app/main.py — API REST para Clasificación de Imágenes\n",
    "# ------------------------------------------------------------\n",
    "# Este módulo implementa una aplicación web usando FastAPI.\n",
    "# Permite cargar imágenes y clasificarlas mediante un modelo\n",
    "# de Deep Learning (MobileNet preentrenado en ImageNet).\n",
    "# Se siguen principios de:\n",
    "# - PEP 8 (estilo de código en Python)\n",
    "# - RESTful API Design\n",
    "# - Clean Code y separación de responsabilidades\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Importaciones necesarias\n",
    "# ------------------------------------------------------------\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException  # Framework web moderno y rápido\n",
    "from fastapi.responses import JSONResponse                    # Permite devolver respuestas JSON personalizadas\n",
    "import logging                                                 # Módulo estándar de logging (registro de eventos)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Importaciones de módulos locales (de la carpeta app/)\n",
    "# ------------------------------------------------------------\n",
    "from model.model import ImageClassifier                       # Clase para manejar el modelo de IA\n",
    "from schemas.schema import ClassifyResponse, Prediction        # Modelos Pydantic para la estructura de respuesta\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Configuración inicial del logger\n",
    "# ------------------------------------------------------------\n",
    "logger = logging.getLogger(\"cv_app\")  # Crea un logger llamado \"cv_app\" para registrar mensajes en consola o archivo\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Inicialización de la aplicación FastAPI\n",
    "# ------------------------------------------------------------\n",
    "app = FastAPI(\n",
    "    title=\"CV Model API\",   # Título visible en la documentación automática (/docs)\n",
    "    version=\"0.1\"           # Versión de la API\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Carga del modelo de manera diferida (lazy loading)\n",
    "# ------------------------------------------------------------\n",
    "# La idea es no cargar el modelo de IA al iniciar la API,\n",
    "# sino solo cuando se necesita por primera vez (optimización de recursos).\n",
    "classifier: ImageClassifier | None = None  # Variable global para almacenar el modelo\n",
    "\n",
    "def get_classifier():\n",
    "    \"\"\"\n",
    "    Carga el modelo de clasificación de imágenes solo una vez.\n",
    "    Si ya está cargado, lo reutiliza.\n",
    "    Esta técnica evita que el modelo se cargue múltiples veces,\n",
    "    lo que mejora la eficiencia y reduce el uso de memoria.\n",
    "    \"\"\"\n",
    "    global classifier  # Se usa global para acceder/modificar la variable definida arriba\n",
    "\n",
    "    if classifier is None:  # Si el modelo no está cargado aún\n",
    "        logger.info(\"Cargando modelo por primera vez...\")  # Mensaje informativo\n",
    "        classifier = ImageClassifier(device=\"cpu\")         # Inicializa el modelo en CPU\n",
    "        logger.info(\"Modelo cargado exitosamente\")         # Confirma carga exitosa\n",
    "\n",
    "    return classifier  # Devuelve la instancia del modelo (ya cargada)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Endpoint raíz: información general de la API\n",
    "# ------------------------------------------------------------\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    \"\"\"Endpoint raíz con descripción general de la API.\"\"\"\n",
    "    return {\n",
    "        \"message\": \"API de Clasificación de Imágenes con IA\",  # Mensaje principal\n",
    "        \"version\": \"1.0.0\",                                    # Versión informativa\n",
    "        \"description\": \"Clasifica imágenes usando MobileNet pre-entrenado en ImageNet\",\n",
    "        # Información de los endpoints disponibles\n",
    "        \"endpoints\": {\n",
    "            \"health\": \"/health - Estado del servidor\",\n",
    "            \"classify\": \"/predict/classify - Clasificar una imagen\",\n",
    "            \"test_labels\": \"/test-labels - Ver etiquetas del modelo\",\n",
    "            \"docs\": \"/docs - Documentación interactiva generada por FastAPI\"\n",
    "        },\n",
    "        # Ejemplo de uso para usuarios nuevos\n",
    "        \"usage\": {\n",
    "            \"method\": \"POST\",\n",
    "            \"endpoint\": \"/predict/classify\",\n",
    "            \"content_type\": \"multipart/form-data\",\n",
    "            \"parameter\": \"file (imagen)\"\n",
    "        },\n",
    "        # Ejemplo práctico de cómo consumir el servicio\n",
    "        \"example\": \"Sube una imagen a /predict/classify para obtener las 3 predicciones más probables\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Endpoint de salud: verificación del estado del servidor\n",
    "# ------------------------------------------------------------\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    \"\"\"\n",
    "    Devuelve el estado básico del servidor.\n",
    "    Permite saber si la API está corriendo correctamente.\n",
    "    \"\"\"\n",
    "    return {\"status\": \"ok\"}  # Respuesta simple tipo JSON\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Endpoint de prueba de etiquetas del modelo\n",
    "# ------------------------------------------------------------\n",
    "@app.get(\"/test-labels\")\n",
    "def test_labels():\n",
    "    \"\"\"\n",
    "    Devuelve una muestra de etiquetas del modelo preentrenado.\n",
    "    Sirve para verificar que el modelo se cargó correctamente\n",
    "    y que las clases están disponibles.\n",
    "    \"\"\"\n",
    "    classifier = get_classifier()                # Asegura que el modelo esté cargado\n",
    "    sample_labels = classifier.labels[:10]       # Obtiene las primeras 10 etiquetas\n",
    "    return {\n",
    "        \"labels_sample\": sample_labels,          # Ejemplo de etiquetas\n",
    "        \"total_labels\": len(classifier.labels)   # Número total de etiquetas del modelo\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Endpoint principal: clasificación de imágenes\n",
    "# ------------------------------------------------------------\n",
    "@app.post(\"/predict/classify\", response_model=ClassifyResponse)\n",
    "async def predict_classify(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Endpoint para clasificar una imagen.\n",
    "    Recibe una imagen en formato multipart/form-data,\n",
    "    la procesa con el modelo y devuelve las predicciones más probables.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    file : UploadFile\n",
    "        Imagen enviada por el usuario.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    ClassifyResponse\n",
    "        Estructura JSON con el estado y las predicciones (label y score).\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Validación del tipo de archivo\n",
    "    # --------------------------------------------------------\n",
    "    if not file.content_type.startswith(\"image/\"):\n",
    "        # Si el archivo no es una imagen, se lanza un error HTTP 400 (Bad Request)\n",
    "        raise HTTPException(status_code=400, detail=\"Archivo no es una imagen\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Lectura de bytes del archivo\n",
    "    # --------------------------------------------------------\n",
    "    image_bytes = await file.read()  # Lee el contenido binario del archivo (asíncrono)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Validación de contenido vacío\n",
    "    # --------------------------------------------------------\n",
    "    if len(image_bytes) == 0:\n",
    "        raise HTTPException(status_code=400, detail=\"Archivo vacío\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Carga del modelo solo cuando se necesita\n",
    "    # --------------------------------------------------------\n",
    "    classifier = get_classifier()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Generación de predicciones\n",
    "    # --------------------------------------------------------\n",
    "    preds = classifier.predict(image_bytes, topk=3)  # topk=3: se devuelven las 3 clases más probables\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Construcción de la respuesta usando Pydantic\n",
    "    # --------------------------------------------------------\n",
    "    response = ClassifyResponse(\n",
    "        predictions=[Prediction(label=p[0], score=p[1]) for p in preds]\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Devolución de la respuesta estructurada\n",
    "    # --------------------------------------------------------\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81dd72",
   "metadata": {},
   "source": [
    "# Ahora...\n",
    "\n",
    "## Crear la carpeta /img y guardar las imagenes para probar el modelo\n",
    "\n",
    "## Ejecutar la aplicación\n",
    "```\n",
    "uvicorn main:app --reload --host 127.0.0.1 --port 8000\n",
    "```\n",
    "### **2. En el navegador web:**\n",
    "- **Documentación**: `http://127.0.0.1:8000/docs`\n",
    "- **API principal**: `http://127.0.0.1:8000`\n",
    "- **Health check**: `http://127.0.0.1:8000/health`\n",
    "\n",
    "### **3. Para probar la clasificación de imágenes:**\n",
    "\n",
    "1. **Abre tu navegador**\n",
    "2. **Ve a**: `http://127.0.0.1:8000/docs`\n",
    "3. **Verás la interfaz de Swagger** con los endpoints\n",
    "4. **Haz clic en** `/predict/classify`\n",
    "5. **Haz clic en** \"Try it out\"\n",
    "6. **Sube una imagen** y haz clic en \"Execute\"\n",
    "\n",
    "### **4. La consola mostrará:**\n",
    "- Cuando alguien haga una petición\n",
    "- Los logs de carga del modelo (primera vez)\n",
    "- Los logs de procesamiento de imágenes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa0804",
   "metadata": {},
   "source": [
    "## Lecturas y recursos\n",
    "\n",
    "* FastAPI docs (Auto-generated OpenAPI).\n",
    "* PyTorch/TorchVision model zoo.\n",
    "* TorchServe docs y ejemplos para serving en producción.\n",
    "* ONNX Runtime docs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
